This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
config/
  config_manager.py
  legacy_constants.py
  tuned_params.py
core/
  create_global_processor.py
  dataset.py
  shared_utils.py
federated/
  client_utils.py
  client.py
  server.py
  sim.py
  utils.py
models/
  use_saved_model.py
  use_tuned_params.py
tuning/
  ray_tune_xgboost.py
utils/
  enhanced_logging.py
  visualization.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/config_manager.py">
"""
Configuration Management System for FL-CML-Pipeline
This module provides a centralized, type-safe configuration management system
using Hydra for loading YAML configurations with experiment overrides.
"""
from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Union
from pathlib import Path
import logging
from omegaconf import DictConfig, OmegaConf
from hydra import compose, initialize_config_dir
from hydra.core.global_hydra import GlobalHydra
@dataclass
class DataConfig:  # pylint: disable=too-many-instance-attributes
    """Data-related configuration."""
    path: str
    filename: str
    train_test_split: float
    stratified: bool
    temporal_window_size: int
    seed: int
@dataclass
class ModelParamsConfig:  # pylint: disable=too-many-instance-attributes
    """XGBoost model parameters configuration."""
    objective: str
    num_class: int
    eta: float
    max_depth: int
    min_child_weight: int
    gamma: float
    subsample: float
    colsample_bytree: float
    colsample_bylevel: float
    nthread: int
    tree_method: str
    eval_metric: List[str]
    max_delta_step: int
    reg_alpha: float
    reg_lambda: float
    base_score: float
    scale_pos_weight: float
    grow_policy: str
    normalize_type: str
    random_state: int
    # Optional parameters for specific experiments
    num_boost_round: Optional[int] = None
@dataclass
class ModelConfig:
    """Model configuration."""
    type: str
    num_local_rounds: int
    params: ModelParamsConfig
@dataclass
class FederatedConfig:  # pylint: disable=too-many-instance-attributes
    """Federated learning configuration."""
    train_method: str
    pool_size: int
    num_rounds: int
    num_clients_per_round: int
    num_evaluate_clients: int
    centralised_eval: bool
    num_partitions: int
    partitioner_type: str
    test_fraction: float
    scaled_lr: bool
    num_cpus_per_client: int
    # Optional fields for experiment overrides
    fraction_fit: Optional[float] = None
    fraction_evaluate: Optional[float] = None
@dataclass
class SchedulerConfig:
    """Ray Tune scheduler configuration."""
    type: str
    max_t: int
    grace_period: int
    reduction_factor: int
@dataclass
class TuningConfig:  # pylint: disable=too-many-instance-attributes
    """Hyperparameter tuning configuration."""
    enabled: bool
    num_samples: int
    cpus_per_trial: int
    max_concurrent_trials: int
    output_dir: str
    scheduler: SchedulerConfig
@dataclass
class GlobalProcessorConfig:
    """Global processor configuration."""
    force_recreate: bool
    output_dir: str
@dataclass
class PreprocessingConfig:
    """Preprocessing configuration."""
    consistent_across_phases: bool
    global_processor_path: Optional[str] = None
@dataclass
class PipelineConfig:
    """Pipeline execution configuration."""
    steps: List[str]
    global_processor: GlobalProcessorConfig
    preprocessing: PreprocessingConfig
@dataclass
class LoggingConfig:
    """Logging configuration."""
    level: str
    format: str
    file: str
@dataclass
class OutputsConfig:  # pylint: disable=too-many-instance-attributes
    """Output configuration."""
    base_dir: str
    create_timestamped_dirs: bool
    save_results_pickle: bool
    save_model: bool
    generate_visualizations: bool
    experiment_name: Optional[str] = None
@dataclass
class EarlyStoppingConfig:
    """Early stopping configuration."""
    enabled: bool
    patience: int
    min_delta: float
@dataclass
class FlConfig:  # pylint: disable=too-many-instance-attributes
    """Main configuration container."""
    data: DataConfig
    model: ModelConfig
    federated: FederatedConfig
    tuning: TuningConfig
    pipeline: PipelineConfig
    logging: LoggingConfig
    outputs: OutputsConfig
    early_stopping: EarlyStoppingConfig
class ConfigManager:
    """
    Centralized configuration manager using Hydra.
    Provides type-safe access to configuration with experiment overrides.
    """
    def __init__(self, config_dir: Optional[str] = None):
        """
        Initialize ConfigManager.
        Args:
            config_dir: Directory containing configuration files.
                       Defaults to 'configs' in project root.
        """
        self._config: Optional[FlConfig] = None
        self._raw_config: Optional[DictConfig] = None
        self._config_dir = config_dir
        self._logger = logging.getLogger(__name__)
    def load_config(self, config_name: str = "base", 
                   experiment: Optional[str] = None,
                   overrides: Optional[List[str]] = None) -> FlConfig:
        """
        Load configuration using Hydra.
        Args:
            config_name: Base configuration name (default: "base")
            experiment: Experiment name for overrides (e.g., "bagging", "cyclic")
            overrides: Additional configuration overrides
        Returns:
            Loaded and validated FlConfig instance
        Example:
            # Load base config
            config = manager.load_config()
            # Load bagging experiment config
            config = manager.load_config(experiment="bagging")
            # Load with custom overrides
            config = manager.load_config(
                experiment="dev",
                overrides=["tuning.enabled=true", "federated.num_rounds=10"]
            )
        """
        try:
            # Clear any existing Hydra instance
            if GlobalHydra().is_initialized():
                GlobalHydra.instance().clear()
            # Determine config directory
            if self._config_dir is None:
                # Default to configs directory in project root
                project_root = Path(__file__).parent.parent.parent
                self._config_dir = str(project_root / "configs")
            # Initialize Hydra with config directory
            with initialize_config_dir(config_dir=self._config_dir, version_base=None):
                # Build config overrides
                config_overrides = []
                if experiment:
                    # Use +experiment= syntax to append experiment to defaults
                    config_overrides.append(f"+experiment={experiment}")
                if overrides:
                    config_overrides.extend(overrides)
                # Load configuration
                self._raw_config = compose(config_name=config_name, overrides=config_overrides)
                # Convert to structured config
                self._config = self._convert_to_structured_config(self._raw_config)
                self._logger.info("Configuration loaded successfully: %s", config_name)
                if experiment:
                    self._logger.info("Experiment override applied: %s", experiment)
                if overrides:
                    self._logger.info("Custom overrides applied: %s", overrides)
                return self._config
        except Exception as e:
            self._logger.error("Failed to load configuration: %s", e)
            raise
    def _convert_to_structured_config(self, cfg: DictConfig) -> FlConfig:
        """Convert DictConfig to structured dataclass configuration."""
        try:
            # Convert nested configurations
            data_config = DataConfig(**cfg.data)
            model_params = ModelParamsConfig(**cfg.model.params)
            model_config = ModelConfig(
                type=cfg.model.type,
                num_local_rounds=cfg.model.num_local_rounds,
                params=model_params
            )
            federated_config = FederatedConfig(**cfg.federated)
            scheduler_config = SchedulerConfig(**cfg.tuning.scheduler)
            tuning_config = TuningConfig(
                enabled=cfg.tuning.enabled,
                num_samples=cfg.tuning.num_samples,
                cpus_per_trial=cfg.tuning.cpus_per_trial,
                max_concurrent_trials=cfg.tuning.max_concurrent_trials,
                output_dir=cfg.tuning.output_dir,
                scheduler=scheduler_config
            )
            global_processor_config = GlobalProcessorConfig(**cfg.pipeline.global_processor)
            preprocessing_config = PreprocessingConfig(**cfg.pipeline.preprocessing)
            pipeline_config = PipelineConfig(
                steps=cfg.pipeline.steps,
                global_processor=global_processor_config,
                preprocessing=preprocessing_config
            )
            logging_config = LoggingConfig(**cfg.logging)
            outputs_config = OutputsConfig(**cfg.outputs)
            early_stopping_config = EarlyStoppingConfig(**cfg.early_stopping)
            return FlConfig(
                data=data_config,
                model=model_config,
                federated=federated_config,
                tuning=tuning_config,
                pipeline=pipeline_config,
                logging=logging_config,
                outputs=outputs_config,
                early_stopping=early_stopping_config
            )
        except Exception as e:
            self._logger.error("Failed to convert configuration to structured format: %s", e)
            raise
    @property
    def config(self) -> FlConfig:
        """Get the current configuration."""
        if self._config is None:
            raise RuntimeError("Configuration not loaded. Call load_config() first.")
        return self._config
    @property
    def raw_config(self) -> DictConfig:
        """Get the raw Hydra DictConfig."""
        if self._raw_config is None:
            raise RuntimeError("Configuration not loaded. Call load_config() first.")
        return self._raw_config
    def get_model_params_dict(self) -> Dict[str, Any]:
        """Get XGBoost model parameters as dictionary."""
        model_params = self.config.model.params
        params_dict = {
            'objective': model_params.objective,
            'num_class': model_params.num_class,
            'eta': model_params.eta,
            'max_depth': model_params.max_depth,
            'min_child_weight': model_params.min_child_weight,
            'gamma': model_params.gamma,
            'subsample': model_params.subsample,
            'colsample_bytree': model_params.colsample_bytree,
            'colsample_bylevel': model_params.colsample_bylevel,
            'nthread': model_params.nthread,
            'tree_method': model_params.tree_method,
            'eval_metric': 'mlogloss',
            'max_delta_step': model_params.max_delta_step,
            'reg_alpha': model_params.reg_alpha,
            'reg_lambda': model_params.reg_lambda,
            'base_score': model_params.base_score,
            'scale_pos_weight': model_params.scale_pos_weight,
            'grow_policy': model_params.grow_policy,
            'normalize_type': model_params.normalize_type,
            'random_state': model_params.random_state
        }
        # Add optional parameters if present
        if model_params.num_boost_round is not None:
            params_dict['num_boost_round'] = model_params.num_boost_round
        return params_dict
    def get_data_path(self) -> Path:
        """Get the full data file path."""
        return Path(self.config.data.path) / self.config.data.filename
    def is_tuning_enabled(self) -> bool:
        """Check if hyperparameter tuning is enabled."""
        return self.config.tuning.enabled
    def get_experiment_name(self) -> str:
        """Get experiment name for output organization."""
        if hasattr(self.config.outputs, 'experiment_name') and self.config.outputs.experiment_name:
            return self.config.outputs.experiment_name
        return f"{self.config.federated.train_method}_experiment"
    def should_create_timestamped_dirs(self) -> bool:
        """Check if timestamped output directories should be created."""
        return self.config.outputs.create_timestamped_dirs
    def update_config_value(self, key_path: str, value: Any) -> None:
        """
        Update a configuration value dynamically.
        Args:
            key_path: Dot-notation path to the config value (e.g., "tuning.enabled")
            value: New value to set
        """
        if self._raw_config is None:
            raise RuntimeError("Configuration not loaded. Call load_config() first.")
        OmegaConf.set(self._raw_config, key_path, value)
        # Reload structured config
        self._config = self._convert_to_structured_config(self._raw_config)
        self._logger.info("Updated configuration: %s = %s", key_path, value)
    def print_config(self) -> None:
        """Print current configuration in a readable format."""
        if self._raw_config is None:
            raise RuntimeError("Configuration not loaded. Call load_config() first.")
        print("\n" + "="*60)
        print("FL-CML-Pipeline Configuration")
        print("="*60)
        print(OmegaConf.to_yaml(self._raw_config))
        print("="*60 + "\n")
    def save_config(self, output_path: Union[str, Path]) -> None:
        """Save current configuration to a YAML file."""
        if self._raw_config is None:
            raise RuntimeError("Configuration not loaded. Call load_config() first.")
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            OmegaConf.save(self._raw_config, f)
        self._logger.info("Configuration saved to: %s", output_path)
# Global configuration manager instance
_config_manager = None  # pylint: disable=global-statement
def get_config_manager() -> ConfigManager:
    """Get the global ConfigManager instance."""
    global _config_manager  # pylint: disable=global-statement
    if _config_manager is None:
        _config_manager = ConfigManager()
    return _config_manager
def load_config(config_name: str = "base", 
               experiment: Optional[str] = None,
               overrides: Optional[List[str]] = None) -> FlConfig:
    """Convenience function to load configuration."""
    manager = get_config_manager()
    return manager.load_config(config_name, experiment, overrides)
</file>

<file path="config/legacy_constants.py">
"""
Legacy constants for the FL-CML-Pipeline.
Note: This file previously contained hardcoded constants and argument parsers
that have been migrated to the ConfigManager system. It is kept for backward
compatibility but should not be used in new code.
For configuration management, use:
    from src.config.config_manager import ConfigManager
    config_manager = ConfigManager()
    model_params = config_manager.get_model_params_dict()
"""
# This file has been cleaned up as part of Step 4: Legacy Code Cleanup
# All constants and argument parsers have been migrated to ConfigManager
# See configs/base.yaml for the current configuration structure
</file>

<file path="config/tuned_params.py">
# This file is generated automatically by use_tuned_params.py
# It contains optimized XGBoost parameters found by Ray Tune
NUM_LOCAL_ROUND = 15  # Reduced from 82 for faster federated learning
TUNED_PARAMS = {
    'objective': 'multi:softprob',
    'tree_method': 'hist',
    'eval_metric': ['mlogloss', 'merror'],
    'num_class': 11,
    'random_state': 42,
    'nthread': 16,
    'max_depth': 6,  # Reduced from 8 for faster training
    'min_child_weight': 5,
    'eta': 0.1,  # Increased from 0.05 for faster convergence
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'num_boost_round': 15,  # Reduced from 82 for faster federated learning
}
</file>

<file path="core/create_global_processor.py">
#!/usr/bin/env python3
"""
create_global_processor.py
This script creates a global feature processor that can be used across all clients
in the federated learning setup to ensure consistent data preprocessing.
"""
import argparse
import os
import sys
# Add project root directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # Go up two levels to project root
sys.path.insert(0, project_root)
# Import local modules
from src.core.dataset import create_global_feature_processor, load_global_feature_processor
from flwr.common.logger import log
from logging import INFO
def main():
    """Create global feature processor for consistent preprocessing."""
    parser = argparse.ArgumentParser(
        description="Create global feature processor for consistent preprocessing"
    )
    parser.add_argument(
        "--data-file",
        type=str,
        default="data/received/final_dataset.csv",
        help="Path to the dataset file (default: data/received/final_dataset.csv)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs",
        help="Output directory for the processor (default: outputs)"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force recreation of processor even if it already exists"
    )
    args = parser.parse_args()
    # Check if data file exists
    if not os.path.exists(args.data_file):
        log(INFO, "Error: Data file not found: %s", args.data_file)
        sys.exit(1)
    # Check if processor already exists
    processor_path = os.path.join(args.output_dir, "global_feature_processor.pkl")
    if os.path.exists(processor_path) and not args.force:
        log(INFO, "Global feature processor already exists at: %s", processor_path)
        log(INFO, "Use --force to recreate it")
        # Load and display info about existing processor
        try:
            processor = load_global_feature_processor(processor_path)
            log(INFO, "Existing processor details:")
            log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
            log(INFO, "  Categorical features: %d", len(processor.categorical_features))
            log(INFO, "  Numerical features: %d", len(processor.numerical_features))
            log(INFO, "  Is fitted: %s", processor.is_fitted)
        except (FileNotFoundError, ImportError, AttributeError) as e:
            log(INFO, "Error loading existing processor: %s", str(e))
            log(INFO, "Consider using --force to recreate it")
        sys.exit(0)
    # Create the global processor
    log(INFO, "Creating global feature processor...")
    try:
        processor_path = create_global_feature_processor(args.data_file, args.output_dir)
        log(INFO, "Successfully created global feature processor at: %s", processor_path)
        # Verify the processor
        processor = load_global_feature_processor(processor_path)
        log(INFO, "Verification successful!")
        log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
        log(INFO, "  Categorical features: %d", len(processor.categorical_features))
        log(INFO, "  Numerical features: %d", len(processor.numerical_features))
        log(INFO, "  Is fitted: %s", processor.is_fitted)
        if hasattr(processor, 'unique_labels'):
            log(INFO, "  Unique labels: %s", processor.unique_labels)
    except (FileNotFoundError, ImportError, AttributeError, ValueError) as e:
        log(INFO, "Error creating global feature processor: %s", str(e))
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="core/dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING, ERROR
import pickle
import os
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self, dataset_type="unsw_nb15"):
        """
        Initialize the feature processor.
        Args:
            dataset_type (str): Type of dataset to process.
                Options: "unsw_nb15" (original) or "engineered" (new dataset)
        """
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        self.label_encoder = LabelEncoder()
        self.dataset_type = dataset_type
        # Define feature groups based on dataset type
        if dataset_type == "unsw_nb15":
            # Original UNSW_NB15 dataset features
            self.categorical_features = [
                'proto', 'service', 'state', 'is_ftp_login', 'is_sm_ips_ports'
            ]
            self.numerical_features = [
                'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 
                'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 
                'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 
                'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 
                'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 
                'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst'
            ]
        elif dataset_type == "engineered":
            # New engineered dataset features - all are numerical (pre-normalized)
            self.categorical_features = []
            self.numerical_features = [
                'dur', 'sbytes', 'dbytes', 'Sload', 'swin', 'smeansz', 'Sjit', 'Stime',
                'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_dst_src_ltm',
                'duration', 'jit_ratio', 'inter_pkt_ratio', 'tcp_setup_ratio',
                'byte_pkt_interaction_dst', 'load_jit_interaction_dst', 'tcp_seq_diff'
            ]
        else:
            raise ValueError(f"Unknown dataset type: {dataset_type}")
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # === LABEL COLUMN STANDARDIZATION DURING FIT ===
        # Ensure consistent label column naming during fitting
        df_copy = df.copy()
        if 'attack_cat' in df_copy.columns and 'label' not in df_copy.columns:
            log(INFO, "Standardizing 'attack_cat' column to 'label' during fit")
            df_copy = df_copy.rename(columns={'attack_cat': 'label'})
        elif 'Label' in df_copy.columns and 'label' not in df_copy.columns:
            log(INFO, "Standardizing 'Label' column to 'label' during fit")
            df_copy = df_copy.rename(columns={'Label': 'label'})
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df_copy.columns:
                unique_values = df_copy[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df_copy[df_copy[col] == val]
                        if 'label' in df_copy.columns and len(subset) > 0:
                            most_common_label = subset['label'].value_counts().idxmax()
                            label_pct = subset['label'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics - for original dataset or data validation
        # For engineered dataset, this will be minimal since data is already normalized
        if self.dataset_type == "unsw_nb15":
            for col in self.numerical_features:
                if col in df_copy.columns:
                    self.numerical_stats[col] = {
                        'mean': df_copy[col].mean(),
                        'std': df_copy[col].std(),
                        'median': df_copy[col].median(),
                        'q99': df_copy[col].quantile(0.99)
                    }
        else:
            # For engineered dataset, we only track basic stats for validation
            # No need for extensive normalization since data is already normalized
            for col in self.numerical_features:
                if col in df_copy.columns:
                    self.numerical_stats[col] = {
                        'min': df_copy[col].min(),
                        'max': df_copy[col].max(),
                        'median': df_copy[col].median(),
                    }
        # Fit label encoder for standardized 'label' column
        if 'label' in df_copy.columns:
            label_values = df_copy['label']
            if label_values.dtype == 'object' or isinstance(label_values.iloc[0], str):
                log(INFO, "Fitting label encoder for categorical labels")
                self.label_encoder.fit(label_values)
            else:
                log(INFO, "Labels are already numeric, no encoding needed")
        # For engineered dataset with numeric labels, just record the unique labels
        if 'label' in df_copy.columns and self.dataset_type == "engineered":
            self.unique_labels = sorted(df_copy['label'].unique())
            log(INFO, f"Found {len(self.unique_labels)} unique labels in engineered dataset: {self.unique_labels}")
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            # If not fitted and not training, we should fit it anyway to avoid errors
            # This is needed for the centralized evaluation case
            log(INFO, "FeatureProcessor not fitted but needed for transform. Fitting now.")
            self.fit(df)
        df = df.copy()
        # === LABEL COLUMN STANDARDIZATION ===
        # Ensure consistent label column naming throughout the pipeline
        if 'attack_cat' in df.columns and 'label' not in df.columns:
            # Convert attack_cat to standardized 'label' column for original dataset
            log(INFO, "Standardizing 'attack_cat' column to 'label' for consistent naming")
            df = df.rename(columns={'attack_cat': 'label'})
        elif 'Label' in df.columns and 'label' not in df.columns:
            # Convert uppercase 'Label' to lowercase 'label' for consistency
            log(INFO, "Standardizing 'Label' column to 'label' for consistent naming")
            df = df.rename(columns={'Label': 'label'})
        # Drop id column since it's just an identifier
        if 'id' in df.columns:
            df.drop(columns=['id'], inplace=True)
        # Transform categorical features (only needed for original dataset)
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with different approaches based on dataset type
        if self.dataset_type == "unsw_nb15":
            # Original dataset needs normalization and outlier handling
            for col in self.numerical_features:
                if col in df.columns and col in self.numerical_stats:
                    # Replace infinities
                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                    # Cap outliers using 99th percentile - fix dtype compatibility
                    q99 = self.numerical_stats[col]['q99']
                    # Ensure q99 has the same dtype as the column to avoid FutureWarning
                    if df[col].dtype.kind in 'biufc':  # numeric dtypes
                        q99 = df[col].dtype.type(q99)
                    df.loc[df[col] > q99, col] = q99  # Cap outliers
                    # Fill NaN with median
                    median = self.numerical_stats[col]['median']
                    # Ensure median has the same dtype as the column
                    if df[col].dtype.kind in 'biufc':  # numeric dtypes
                        median = df[col].dtype.type(median)
                    df[col] = df[col].fillna(median)
        else:
            # Engineered dataset is already normalized, just handle missing values
            for col in self.numerical_features:
                if col in df.columns:
                    # Replace infinities and NaN with 0 (since data is normalized, 0 is a reasonable default)
                    df[col] = df[col].replace([np.inf, -np.inf, np.nan], 0)
        # === IMPORTANT: DO NOT DROP THE STANDARDIZED 'label' COLUMN ===
        # The label column should be preserved so that downstream components can find it
        # The preprocess_data function will handle label extraction separately
        # Only drop the original attack_cat column if it still exists after renaming
        if 'attack_cat' in df.columns:
            log(INFO, "Dropping original 'attack_cat' column after standardization")
            df.drop(columns=['attack_cat'], inplace=True)
        return df
def preprocess_data(data: Union[pd.DataFrame, Dataset], processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification for both original and engineered datasets.
    Args:
        data (Union[pd.DataFrame, Dataset]): Input DataFrame or Hugging Face Dataset
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    if processor is None:
        # Auto-detect dataset type based on columns
        if 'attack_cat' in data.columns:
            processor = FeatureProcessor(dataset_type="unsw_nb15")
        elif 'tcp_seq_diff' in data.columns:
            processor = FeatureProcessor(dataset_type="engineered")
        else:
            log(WARNING, "Could not automatically detect dataset type. Defaulting to 'unsw_nb15'.")
            processor = FeatureProcessor(dataset_type="unsw_nb15")
    # === STANDARDIZED LABEL HANDLING ===
    # Process features first (this will standardize label column names)
    features = processor.transform(data, is_training)
    # Now extract labels from the standardized 'label' column
    labels = None
    if 'label' in features.columns:
        log(INFO, "Found standardized 'label' column in processed data")
        labels = features['label'].copy()
        # Handle label encoding based on dataset type
        if processor.dataset_type == "unsw_nb15":
            # For original dataset, labels need to be encoded if they're categorical
            if labels.dtype == 'object' or isinstance(labels.iloc[0], str):
                log(INFO, "Encoding categorical labels for UNSW_NB15 dataset")
                # Ensure label encoder is fitted if needed (during training)
                if is_training and not hasattr(processor.label_encoder, 'classes_'):
                    log(INFO, "Fitting label encoder during training preprocessing.")
                    processor.label_encoder.fit(labels)
                elif not hasattr(processor.label_encoder, 'classes_') or processor.label_encoder.classes_.size == 0:
                    log(WARNING, "Label encoder not fitted, cannot transform categorical labels.")
                    try:
                        processor.label_encoder.fit(labels)
                        log(WARNING, "Fitted label encoder on non-training data chunk.")
                    except Exception as fit_err:
                        log(ERROR, f"Could not fit label encoder on non-training data: {fit_err}")
                        labels = np.full(len(data), -1, dtype=int)
                # Transform labels if encoder is ready
                if hasattr(processor.label_encoder, 'classes_') and processor.label_encoder.classes_.size > 0:
                    try:
                        labels = processor.label_encoder.transform(labels)
                    except ValueError as e:
                        log(ERROR, f"Error transforming labels: {e}. Unseen labels might exist.")
                        labels = np.full(len(data), -1, dtype=int)
            else:
                # Labels are already numeric
                labels = labels.astype(int)
        else:
            # For engineered dataset, labels should already be numeric
            log(INFO, "Using direct numeric labels from engineered dataset.")
            labels = labels.astype(int)
        # Log label distribution
        if labels is not None:
            try:
                unique_labels, counts = np.unique(labels, return_counts=True)
                label_counts = dict(zip(unique_labels, counts))
                log(INFO, f"Label distribution: {label_counts}")
            except Exception as e:
                log(WARNING, f"Could not compute label distribution: {e}")
        # Remove label column from features to avoid data leakage
        features = features.drop(columns=['label'])
        log(INFO, "Removed 'label' column from features to prevent data leakage")
    else:
        # No label column found
        log(INFO, "No 'label' column found in processed data - assuming unlabeled data")
        labels = None
    return features, labels
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Uses hybrid temporal-stratified splitting to avoid data leakage while ensuring class coverage.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Auto-detect dataset type
    if 'attack_cat' in df.columns:
        print("Detected original UNSW_NB15 dataset with attack_cat column")
    elif 'tcp_seq_diff' in df.columns:
        print("Detected engineered dataset with normalized features")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, use hybrid temporal-stratified splitting to avoid data leakage
        # while ensuring all classes are present in both train and test splits
        if 'Stime' in df.columns and 'label' in df.columns:
            print("Using hybrid temporal-stratified split to preserve time order while ensuring class coverage")
            # Sort by time first to maintain temporal integrity
            df_sorted = df.sort_values('Stime').reset_index(drop=True)
            # Create temporal windows to split data while preserving time order
            n_windows = 10  # Split into 10 temporal windows
            window_size = len(df_sorted) // n_windows
            train_dfs = []
            test_dfs = []
            for i in range(n_windows):
                start_idx = i * window_size
                end_idx = (i + 1) * window_size if i < n_windows - 1 else len(df_sorted)
                window_df = df_sorted.iloc[start_idx:end_idx]
                # Within each window, use stratified split to ensure all classes are represented
                if len(window_df) > 0:
                    try:
                        from sklearn.model_selection import train_test_split
                        train_window, test_window = train_test_split(
                            window_df, test_size=0.2, random_state=42, 
                            stratify=window_df['label']
                        )
                        train_dfs.append(train_window)
                        test_dfs.append(test_window)
                    except ValueError as e:
                        # Some classes missing in this window - use temporal split as fallback
                        print(f"  Warning: Stratification failed for window {i}: {e}")
                        print(f"  Falling back to temporal split for this window")
                        window_train_size = int(0.8 * len(window_df))
                        train_dfs.append(window_df.iloc[:window_train_size])
                        test_dfs.append(window_df.iloc[window_train_size:])
            # Combine all windows
            train_df = pd.concat(train_dfs, ignore_index=True)
            test_df = pd.concat(test_dfs, ignore_index=True)
            # Verify all classes are present in both splits
            train_classes = set(train_df['label'].unique())
            test_classes = set(test_df['label'].unique())
            all_classes = set(df['label'].unique())
            print(f"Hybrid split: {len(train_df)} train samples, {len(test_df)} test samples")
            print(f"Train classes: {sorted(train_classes)} ({len(train_classes)} classes)")
            print(f"Test classes: {sorted(test_classes)} ({len(test_classes)} classes)")
            print(f"All classes: {sorted(all_classes)} ({len(all_classes)} classes)")
            # Check for missing classes and apply fallback if needed
            missing_train_classes = all_classes - train_classes
            missing_test_classes = all_classes - test_classes
            if missing_train_classes or missing_test_classes:
                print(f"⚠️ WARNING: Missing classes detected!")
                if missing_train_classes:
                    print(f"  Missing from training: {sorted(missing_train_classes)}")
                if missing_test_classes:
                    print(f"  Missing from testing: {sorted(missing_test_classes)}")
                print("  Applying fallback: Pure stratified split to ensure all classes")
                from sklearn.model_selection import train_test_split
                train_df, test_df = train_test_split(
                    df, test_size=0.2, random_state=42, stratify=df['label']
                )
                print(f"✓ Fallback complete: {len(train_df)} train, {len(test_df)} test samples")
                print(f"✓ All classes now present in both splits")
            else:
                print("✓ All classes successfully present in both train and test splits")
                # Show temporal ranges for verification
                print(f"Train Stime range: {train_df['Stime'].min():.4f} to {train_df['Stime'].max():.4f}")
                print(f"Test Stime range: {test_df['Stime'].min():.4f} to {test_df['Stime'].max():.4f}")
            # Final verification: print class distribution
            print("\nFinal class distribution:")
            train_counts = train_df['label'].value_counts().sort_index()
            test_counts = test_df['label'].value_counts().sort_index()
            for label in sorted(all_classes):
                train_count = train_counts.get(label, 0)
                test_count = test_counts.get(label, 0)
                total_count = train_count + test_count
                print(f"  Class {label}: {train_count} train, {test_count} test, {total_count} total")
            return DatasetDict({
                "train": Dataset.from_pandas(train_df),
                "test": Dataset.from_pandas(test_df)
            })
        else:
            # Fallback to stratified random split if no temporal column available
            print("No Stime column found, using stratified random split")
            train_test_dict = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label' if 'label' in df.columns else None)
            return DatasetDict({
                "train": train_test_dict["train"],
                "test": train_test_dict["test"]
            })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset (can be pandas DataFrame or Hugging Face Dataset)
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Now process the data using the pandas DataFrame
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # --- Logging before DMatrix creation ---
    log(INFO, f"[transform_dataset_to_dmatrix] is_training={is_training}")
    log(INFO, f"[transform_dataset_to_dmatrix] Features shape: {x.shape}")
    if y is not None:
        log(INFO, f"[transform_dataset_to_dmatrix] Labels shape: {y.shape}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels dtype: {y.dtype}")
        unique_labels, counts = np.unique(y, return_counts=True)
        log(INFO, f"[transform_dataset_to_dmatrix] Unique labels: {unique_labels.tolist()}")
        log(INFO, f"[transform_dataset_to_dmatrix] Label counts: {counts.tolist()}")
    else:
        log(INFO, "[transform_dataset_to_dmatrix] No labels provided (unlabeled data)")
    # Create DMatrix
    dmatrix = xgb.DMatrix(x, label=y)
    log(INFO, f"[transform_dataset_to_dmatrix] Created DMatrix with {dmatrix.num_row()} rows and {dmatrix.num_col()} features")
    return dmatrix
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into training and testing sets with proper feature processing.
    Returns training DMatrix, testing DMatrix, and fitted feature processor.
    Note: This function is DEPRECATED in favor of load_csv_data() which implements
    hybrid temporal-stratified splitting to prevent data leakage. This function
    remains for backward compatibility but should not be used for new code.
    Args:
        data: Input dataset (pandas DataFrame or Hugging Face Dataset)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        tuple: (train_dmatrix, test_dmatrix, fitted_processor)
    """
    log(WARNING, "train_test_split() is DEPRECATED. Use load_csv_data() with hybrid temporal-stratified splitting instead.")
    # Convert to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Auto-detect dataset type
    if 'attack_cat' in data.columns:
        processor = FeatureProcessor(dataset_type="unsw_nb15")
    elif 'tcp_seq_diff' in data.columns:
        processor = FeatureProcessor(dataset_type="engineered")
    else:
        log(WARNING, "Could not automatically detect dataset type. Defaulting to 'unsw_nb15'.")
        processor = FeatureProcessor(dataset_type="unsw_nb15")
    # Preprocess the data
    x, y = preprocess_data(data, processor=processor, is_training=True)
    if y is not None:
        # Use stratified split to maintain class distribution
        x_train, x_test, y_train, y_test = train_test_split_pandas(
            x, y, test_size=test_fraction, random_state=random_state,
            stratify=y
        )
        # Create DMatrix objects
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        test_dmatrix = xgb.DMatrix(x_test, label=y_test)
        log(INFO, f"Split dataset: {train_dmatrix.num_row()} training samples, {test_dmatrix.num_row()} testing samples")
        log(INFO, f"Features: {train_dmatrix.num_col()}")
        return train_dmatrix, test_dmatrix, processor
    else:
        # No labels available - split features only
        x_train, x_test = train_test_split_pandas(
            x, test_size=test_fraction, random_state=random_state
        )
        train_dmatrix = xgb.DMatrix(x_train)
        test_dmatrix = xgb.DMatrix(x_test)
        log(INFO, f"Split unlabeled dataset: {train_dmatrix.num_row()} training samples, {test_dmatrix.num_row()} testing samples")
        log(INFO, f"Features: {train_dmatrix.num_col()}")
        return train_dmatrix, test_dmatrix, processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Resplit an existing DatasetDict to redistribute data between train/test splits.
    This function combines train and test data, then applies a new stratified split.
    Note: This function bypasses the hybrid temporal-stratified splitting logic 
    that prevents data leakage. Use with caution and consider whether the original
    load_csv_data() approach is more appropriate for your use case.
    Args:
        dataset (DatasetDict): Dataset dictionary with 'train' and 'test' splits
    Returns:
        DatasetDict: New dataset dictionary with redistributed train/test splits
    """
    log(WARNING, "resplit() bypasses temporal-stratified splitting. Consider using load_csv_data() instead.")
    # Combine train and test data
    combined_dataset = concatenate_datasets([dataset["train"], dataset["test"]])
    # Convert to pandas for stratified splitting
    combined_df = combined_dataset.to_pandas()
    # Determine stratification column
    if 'label' in combined_df.columns:
        stratify_col = 'label'
    elif 'attack_cat' in combined_df.columns:
        stratify_col = 'attack_cat'
    else:
        stratify_col = None
        log(WARNING, "No label column found for stratification. Using random split.")
    # Split the combined dataset
    if stratify_col:
        try:
            train_df, test_df = train_test_split_pandas(
                combined_df, test_size=0.2, random_state=42,
                stratify=combined_df[stratify_col]
            )
            log(INFO, f"Resplit dataset with stratification on '{stratify_col}'")
        except ValueError as e:
            log(WARNING, f"Stratification failed: {e}. Using random split.")
            train_df, test_df = train_test_split_pandas(
                combined_df, test_size=0.2, random_state=42
            )
    else:
        train_df, test_df = train_test_split_pandas(
            combined_df, test_size=0.2, random_state=42
        )
    # Convert back to DatasetDict
    return DatasetDict({
        "train": Dataset.from_pandas(train_df),
        "test": Dataset.from_pandas(test_df)
    })
def create_global_feature_processor(data_file: str, output_dir: str = "outputs") -> str:
    """
    Create and save a global feature processor fitted on the entire dataset.
    This ensures consistent preprocessing across all federated learning clients.
    Args:
        data_file (str): Path to the CSV file containing the full dataset
        output_dir (str): Directory to save the fitted processor
    Returns:
        str: Path to the saved processor file
    """
    log(INFO, f"Creating global feature processor from {data_file}")
    # Load the full dataset
    df = pd.read_csv(data_file)
    log(INFO, f"Loaded dataset with {len(df)} samples and {len(df.columns)} features")
    # Auto-detect dataset type
    if 'attack_cat' in df.columns:
        processor = FeatureProcessor(dataset_type="unsw_nb15")
        log(INFO, "Detected original UNSW_NB15 dataset")
    elif 'tcp_seq_diff' in df.columns:
        processor = FeatureProcessor(dataset_type="engineered")
        log(INFO, "Detected engineered dataset")
    else:
        processor = FeatureProcessor(dataset_type="unsw_nb15")
        log(WARNING, "Could not detect dataset type. Defaulting to 'unsw_nb15'")
    # Fit the processor on the full dataset
    processor.fit(df)
    log(INFO, "Fitted feature processor on full dataset")
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Save the fitted processor
    processor_path = os.path.join(output_dir, "global_feature_processor.pkl")
    with open(processor_path, 'wb') as f:
        pickle.dump(processor, f)
    log(INFO, f"Saved global feature processor to {processor_path}")
    return processor_path
def load_global_feature_processor(processor_path: str) -> FeatureProcessor:
    """
    Load a previously saved feature processor.
    Args:
        processor_path (str): Path to the saved processor file
    Returns:
        FeatureProcessor: Loaded and fitted processor
    """
    try:
        with open(processor_path, 'rb') as f:
            processor = pickle.load(f)
        log(INFO, f"Loaded global feature processor from {processor_path}")
        if not processor.is_fitted:
            log(WARNING, "Loaded processor is not fitted!")
        return processor
    except FileNotFoundError:
        log(ERROR, f"Feature processor file not found: {processor_path}")
        raise
    except Exception as e:
        log(ERROR, f"Error loading feature processor: {e}")
        raise
def separate_xy(data):
    """
    Separate features (X) and labels (y) from a dataset.
    This is a convenience function that wraps preprocess_data.
    Args:
        data: Input dataset (pandas DataFrame or Hugging Face Dataset)
    Returns:
        tuple: (features, labels) where features is a numpy array and labels is a numpy array
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use preprocess_data to get features and labels
    features, labels = preprocess_data(data, processor=None, is_training=False)
    # Convert to numpy arrays for compatibility with existing code
    features_array = features.values if features is not None else None
    labels_array = labels.values if labels is not None else None
    return features_array, labels_array
</file>

<file path="core/shared_utils.py">
"""
Shared Utilities for FL-CML-Pipeline
This module provides centralized implementations of commonly used functionality
to eliminate code duplication across the federated learning pipeline.
Key Components:
- DMatrixFactory: Centralized XGBoost DMatrix creation
- MetricsCalculator: Centralized metrics calculation for classification
- XGBoostParamsBuilder: Centralized parameter management for XGBoost
Created for Phase 3: Code Deduplication
"""
from typing import Dict, List, Optional, Union, Tuple, Any
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, log_loss
)
from flwr.common.logger import log
from flwr.common.typing import Scalar
from logging import INFO, WARNING, ERROR
import pickle
import os
from dataclasses import dataclass
# Type aliases for clarity
Features = Union[np.ndarray, pd.DataFrame]
Labels = Union[np.ndarray, pd.Series]
ClientMetrics = List[Tuple[int, Dict[str, float]]]
@dataclass
class MetricsResult:
    """Container for classification metrics results."""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    mlogloss: Optional[float] = None
    confusion_matrix: Optional[np.ndarray] = None
    classification_report: Optional[str] = None
    raw_metrics: Optional[Dict[str, float]] = None
class DMatrixFactory:
    """
    Centralized XGBoost DMatrix creation with consistent configuration.
    Eliminates code duplication across modules and ensures consistent
    handling of missing values, logging, and validation.
    """
    @staticmethod
    def create_dmatrix(
        features: Features,
        labels: Optional[Labels] = None,
        handle_missing: bool = True,
        feature_names: Optional[List[str]] = None,
        weights: Optional[np.ndarray] = None,
        validate: bool = True,
        log_details: bool = True
    ) -> xgb.DMatrix:
        """
        Create XGBoost DMatrix with consistent handling and validation.
        Args:
            features: Feature data (numpy array or pandas DataFrame)
            labels: Target labels (optional for prediction-only usage)
            handle_missing: Whether to replace inf values with nan
            feature_names: Optional feature names for the DMatrix
            weights: Optional sample weights for training
            validate: Whether to validate input data integrity
            log_details: Whether to log detailed creation information
        Returns:
            xgb.DMatrix: Properly configured DMatrix object
        Raises:
            ValueError: If validation fails or incompatible data provided
            TypeError: If unsupported data types provided
        """
        if log_details:
            log(INFO, "[DMatrixFactory] Creating DMatrix...")
        # Convert to numpy arrays for consistent processing
        if isinstance(features, pd.DataFrame):
            feature_names = feature_names or list(features.columns)
            features_array = features.values
        else:
            features_array = np.asarray(features)
        # Handle labels conversion
        labels_array = None
        if labels is not None:
            if isinstance(labels, (pd.Series, pd.DataFrame)):
                labels_array = labels.values.ravel()
            else:
                labels_array = np.asarray(labels).ravel()
        # Validation checks
        if validate:
            DMatrixFactory._validate_input_data(features_array, labels_array, weights)
        # Handle missing values
        if handle_missing:
            features_array = np.where(np.isinf(features_array), np.nan, features_array)
        # Log data statistics
        if log_details:
            log(INFO, f"[DMatrixFactory] Features shape: {features_array.shape}")
            if labels_array is not None:
                log(INFO, f"[DMatrixFactory] Labels shape: {labels_array.shape}")
                log(INFO, f"[DMatrixFactory] Labels dtype: {labels_array.dtype}")
                unique_labels, counts = np.unique(labels_array, return_counts=True)
                log(INFO, f"[DMatrixFactory] Unique labels: {unique_labels.tolist()}")
                log(INFO, f"[DMatrixFactory] Label counts: {counts.tolist()}")
            else:
                log(INFO, "[DMatrixFactory] No labels provided (prediction mode)")
            if weights is not None:
                log(INFO, f"[DMatrixFactory] Sample weights shape: {weights.shape}")
        # Create DMatrix with appropriate parameters
        dmatrix_kwargs = {
            'data': features_array,
            'missing': np.nan  # Consistent missing value handling
        }
        if labels_array is not None:
            dmatrix_kwargs['label'] = labels_array
        if feature_names is not None:
            dmatrix_kwargs['feature_names'] = feature_names
        if weights is not None:
            dmatrix_kwargs['weight'] = weights
        # Create the DMatrix
        dmatrix = xgb.DMatrix(**dmatrix_kwargs)
        if log_details:
            log(INFO, f"[DMatrixFactory] Created DMatrix: {dmatrix.num_row()} rows, {dmatrix.num_col()} features")
        return dmatrix
    @staticmethod
    def _validate_input_data(
        features: np.ndarray, 
        labels: Optional[np.ndarray] = None, 
        weights: Optional[np.ndarray] = None
    ) -> None:
        """Validate input data for DMatrix creation."""
        if features.size == 0:
            raise ValueError("Features array is empty")
        if features.ndim != 2:
            raise ValueError(f"Features must be 2D array, got shape {features.shape}")
        if labels is not None:
            if len(labels) != features.shape[0]:
                raise ValueError(
                    f"Labels length ({len(labels)}) doesn't match "
                    f"features rows ({features.shape[0]})"
                )
        if weights is not None:
            if len(weights) != features.shape[0]:
                raise ValueError(
                    f"Weights length ({len(weights)}) doesn't match "
                    f"features rows ({features.shape[0]})"
                )
            if np.any(weights < 0):
                raise ValueError("Sample weights must be non-negative")
    @staticmethod
    def create_weighted_dmatrix(
        base_dmatrix: xgb.DMatrix,
        weights: np.ndarray,
        feature_names: Optional[List[str]] = None
    ) -> xgb.DMatrix:
        """
        Create a weighted DMatrix from an existing DMatrix.
        Common pattern in federated learning for class balancing.
        Args:
            base_dmatrix: Original DMatrix to extract data from
            weights: Sample weights to apply
            feature_names: Optional feature names
        Returns:
            New DMatrix with applied weights
        """
        return DMatrixFactory.create_dmatrix(
            features=base_dmatrix.get_data(),
            labels=base_dmatrix.get_label(),
            weights=weights,
            feature_names=feature_names or base_dmatrix.feature_names,
            log_details=False  # Avoid duplicate logging
        )
class MetricsCalculator:
    """
    Centralized metrics calculation with consistent implementation.
    Provides unified interface for calculating classification metrics
    across federated learning, hyperparameter tuning, and evaluation.
    """
    # Standard class names for UNSW-NB15 dataset (can be overridden)
    DEFAULT_CLASS_NAMES = [
        'Normal', 'Generic', 'Exploits', 'Reconnaissance', 'Fuzzers',
        'DoS', 'Analysis', 'Backdoor', 'Backdoors', 'Worms', 'Shellcode'
    ]
    @staticmethod
    def calculate_classification_metrics(
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_pred_proba: Optional[np.ndarray] = None,
        class_names: Optional[List[str]] = None,
        prefix: str = "",
        calculate_per_class: bool = False,
        return_confusion_matrix: bool = True
    ) -> MetricsResult:
        """
        Calculate comprehensive classification metrics.
        Args:
            y_true: True labels
            y_pred: Predicted labels
            y_pred_proba: Prediction probabilities (optional)
            class_names: Class names for reporting
            prefix: Prefix for metric keys
            calculate_per_class: Whether to calculate per-class metrics
            return_confusion_matrix: Whether to include confusion matrix
        Returns:
            MetricsResult object with all calculated metrics
        """
        # Ensure arrays are proper format
        y_true = np.asarray(y_true).astype(int)
        y_pred = np.asarray(y_pred).astype(int)
        if class_names is None:
            class_names = MetricsCalculator.DEFAULT_CLASS_NAMES
        # Calculate core metrics with consistent parameters
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        # Calculate log loss if probabilities provided
        mlogloss = None
        if y_pred_proba is not None:
            try:
                # Ensure probabilities are valid
                if y_pred_proba.ndim == 2 and y_pred_proba.shape[1] > 1:
                    mlogloss = log_loss(y_true, y_pred_proba, labels=np.unique(y_true))
                else:
                    log(WARNING, "[MetricsCalculator] Invalid probability shape for log_loss calculation")
            except Exception as e:
                log(WARNING, f"[MetricsCalculator] Could not calculate log_loss: {e}")
        # Calculate confusion matrix
        conf_matrix = None
        if return_confusion_matrix:
            try:
                conf_matrix = confusion_matrix(y_true, y_pred)
            except Exception as e:
                log(WARNING, f"[MetricsCalculator] Could not calculate confusion matrix: {e}")
        # Generate classification report
        class_report = None
        try:
            # Filter class names to match actual labels
            unique_labels = np.unique(np.concatenate([y_true, y_pred]))
            filtered_names = [class_names[i] for i in unique_labels if i < len(class_names)]
            class_report = classification_report(
                y_true, y_pred,
                labels=unique_labels,
                target_names=filtered_names,
                zero_division=0
            )
        except Exception as e:
            log(WARNING, f"[MetricsCalculator] Could not generate classification report: {e}")
        # Create raw metrics dictionary
        raw_metrics = {
            f"{prefix}accuracy": accuracy,
            f"{prefix}precision": precision,
            f"{prefix}recall": recall,
            f"{prefix}f1": f1
        }
        if mlogloss is not None:
            raw_metrics[f"{prefix}mlogloss"] = mlogloss
        # Add per-class metrics if requested
        if calculate_per_class:
            per_class_metrics = MetricsCalculator._calculate_per_class_metrics(
                y_true, y_pred, prefix
            )
            raw_metrics.update(per_class_metrics)
        return MetricsResult(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            mlogloss=mlogloss,
            confusion_matrix=conf_matrix,
            classification_report=class_report,
            raw_metrics=raw_metrics
        )
    @staticmethod
    def _calculate_per_class_metrics(
        y_true: np.ndarray,
        y_pred: np.ndarray,
        prefix: str = ""
    ) -> Dict[str, float]:
        """Calculate per-class precision, recall, and f1 scores."""
        per_class_metrics = {}
        unique_classes = np.unique(y_true)
        for class_idx in unique_classes:
            class_prefix = f"{prefix}class_{class_idx}_"
            # Binary classification metrics for this class
            y_true_binary = (y_true == class_idx).astype(int)
            y_pred_binary = (y_pred == class_idx).astype(int)
            if np.sum(y_true_binary) > 0:  # Only if class exists in true labels
                per_class_metrics[f"{class_prefix}precision"] = precision_score(
                    y_true_binary, y_pred_binary, zero_division=0
                )
                per_class_metrics[f"{class_prefix}recall"] = recall_score(
                    y_true_binary, y_pred_binary, zero_division=0
                )
                per_class_metrics[f"{class_prefix}f1"] = f1_score(
                    y_true_binary, y_pred_binary, zero_division=0
                )
        return per_class_metrics
    @staticmethod
    def aggregate_client_metrics(
        client_metrics: ClientMetrics,
        log_details: bool = True
    ) -> Dict[str, float]:
        """
        Aggregate metrics from multiple federated learning clients.
        Args:
            client_metrics: List of (num_examples, metrics_dict) tuples
            log_details: Whether to log aggregation details
        Returns:
            Dictionary of aggregated metrics
        """
        if not client_metrics:
            log(WARNING, "[MetricsCalculator] No client metrics provided for aggregation")
            return {}
        total_examples = sum([num for num, _ in client_metrics])
        if log_details:
            log(INFO, f"[MetricsCalculator] Aggregating metrics from {len(client_metrics)} clients")
            log(INFO, f"[MetricsCalculator] Total examples: {total_examples}")
        # Find common metrics across all clients
        common_metrics = set(client_metrics[0][1].keys())
        for _, metrics in client_metrics[1:]:
            common_metrics &= set(metrics.keys())
        aggregated = {}
        # Aggregate each metric using weighted average
        for metric_name in common_metrics:
            if metric_name == "confusion_matrix":
                # Special handling for confusion matrices
                aggregated[metric_name] = MetricsCalculator._aggregate_confusion_matrices(
                    client_metrics, total_examples
                )
            else:
                # Weighted average for scalar metrics
                weighted_sum = sum([
                    metrics[metric_name] * num 
                    for num, metrics in client_metrics
                    if metric_name in metrics
                ])
                aggregated[metric_name] = weighted_sum / total_examples
                if log_details:
                    individual_values = [
                        metrics[metric_name] for _, metrics in client_metrics
                        if metric_name in metrics
                    ]
                    log(INFO, f"[MetricsCalculator] {metric_name}: {individual_values} -> {aggregated[metric_name]:.4f}")
        return aggregated
    @staticmethod
    def _aggregate_confusion_matrices(
        client_metrics: ClientMetrics,
        total_examples: int
    ) -> Optional[List[List[float]]]:
        """Aggregate confusion matrices from multiple clients."""
        aggregated_matrix = None
        for num, metrics in client_metrics:
            if "confusion_matrix" in metrics:
                matrix = metrics["confusion_matrix"]
                if aggregated_matrix is None:
                    # Initialize with zeros
                    aggregated_matrix = [[0.0 for _ in range(len(matrix[0]))] 
                                       for _ in range(len(matrix))]
                # Add weighted contribution
                for i in range(len(matrix)):
                    for j in range(len(matrix[0])):
                        aggregated_matrix[i][j] += matrix[i][j] * num / total_examples
        return aggregated_matrix
class XGBoostParamsBuilder:
    """
    Centralized XGBoost parameter management with priority handling.
    Provides consistent parameter building across different components
    while respecting configuration hierarchy.
    """
    # Default parameters for UNSW-NB15 multi-class classification
    DEFAULT_PARAMS = {
        'objective': 'multi:softprob',
        'num_class': 11,
        'eval_metric': 'mlogloss',
        'learning_rate': 0.05,
        'max_depth': 6,
        'min_child_weight': 1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0,
        'reg_lambda': 1,
        'gamma': 0,
        'scale_pos_weight': 1.0,
        'seed': 42,
        'verbosity': 0
    }
    @staticmethod
    def build_params(
        config_manager=None,
        overrides: Optional[Dict[str, Any]] = None,
        use_tuned: bool = False,
        base_params: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Build XGBoost parameters with priority handling.
        Priority (highest to lowest):
        1. overrides - Direct parameter overrides
        2. config_manager - Parameters from configuration
        3. tuned_params - Previously tuned parameters
        4. base_params - Custom base parameters
        5. DEFAULT_PARAMS - Built-in defaults
        Args:
            config_manager: ConfigManager instance for parameter retrieval
            overrides: Direct parameter overrides (highest priority)
            use_tuned: Whether to load tuned parameters
            base_params: Custom base parameters
        Returns:
            Complete XGBoost parameter dictionary
        """
        # Start with defaults
        params = XGBoostParamsBuilder.DEFAULT_PARAMS.copy()
        # Apply base parameters if provided
        if base_params:
            params.update(base_params)
        # Apply tuned parameters if requested
        if use_tuned:
            try:
                tuned_params = XGBoostParamsBuilder._load_tuned_params()
                if tuned_params:
                    params.update(tuned_params)
                    log(INFO, "[XGBoostParamsBuilder] Applied tuned parameters")
            except Exception as e:
                log(WARNING, f"[XGBoostParamsBuilder] Could not load tuned parameters: {e}")
        # Apply config manager parameters
        if config_manager:
            try:
                config_params = config_manager.get_model_params_dict()
                if config_params:
                    params.update(config_params)
                    log(INFO, "[XGBoostParamsBuilder] Applied ConfigManager parameters")
            except Exception as e:
                log(WARNING, f"[XGBoostParamsBuilder] Could not get ConfigManager parameters: {e}")
        # Apply direct overrides (highest priority)
        if overrides:
            params.update(overrides)
            log(INFO, f"[XGBoostParamsBuilder] Applied parameter overrides: {list(overrides.keys())}")
        # Validate and ensure required parameters
        params = XGBoostParamsBuilder._validate_params(params)
        return params
    @staticmethod
    def _load_tuned_params() -> Optional[Dict[str, Any]]:
        """Load previously tuned parameters if available."""
        # Try to import and load tuned parameters
        try:
            from src.models.use_tuned_params import load_tuned_params
            return load_tuned_params()
        except ImportError:
            log(WARNING, "[XGBoostParamsBuilder] Could not import tuned parameters module")
            return None
        except Exception as e:
            log(WARNING, f"[XGBoostParamsBuilder] Error loading tuned parameters: {e}")
            return None
    @staticmethod
    def _validate_params(params: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and fix parameter values."""
        # Ensure required parameters exist
        required_params = ['objective', 'num_class']
        for param in required_params:
            if param not in params:
                params[param] = XGBoostParamsBuilder.DEFAULT_PARAMS[param]
                log(WARNING, f"[XGBoostParamsBuilder] Missing required parameter {param}, using default")
        # Convert numeric parameters to appropriate types
        int_params = ['num_class', 'max_depth', 'min_child_weight', 'seed', 'verbosity']
        for param in int_params:
            if param in params:
                try:
                    params[param] = int(params[param])
                except (ValueError, TypeError):
                    log(WARNING, f"[XGBoostParamsBuilder] Invalid {param} value, using default")
                    params[param] = XGBoostParamsBuilder.DEFAULT_PARAMS.get(param, 0)
        # Validate float parameters
        float_params = ['learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', 'reg_lambda', 'gamma']
        for param in float_params:
            if param in params:
                try:
                    params[param] = float(params[param])
                    # Ensure valid ranges
                    if param in ['subsample', 'colsample_bytree'] and not (0 < params[param] <= 1):
                        raise ValueError(f"{param} must be in (0, 1]")
                except (ValueError, TypeError):
                    log(WARNING, f"[XGBoostParamsBuilder] Invalid {param} value, using default")
                    params[param] = XGBoostParamsBuilder.DEFAULT_PARAMS.get(param, 1.0)
        return params
# Utility functions for backward compatibility and convenience
def create_dmatrix(
    features: Features,
    labels: Optional[Labels] = None,
    handle_missing: bool = True
) -> xgb.DMatrix:
    """
    Convenience function for DMatrix creation.
    Simple wrapper around DMatrixFactory.create_dmatrix for common usage.
    """
    return DMatrixFactory.create_dmatrix(
        features=features,
        labels=labels,
        handle_missing=handle_missing
    )
def calculate_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_pred_proba: Optional[np.ndarray] = None
) -> Dict[str, float]:
    """
    Convenience function for metrics calculation.
    Returns raw metrics dictionary for compatibility with existing code.
    """
    result = MetricsCalculator.calculate_classification_metrics(
        y_true=y_true,
        y_pred=y_pred,
        y_pred_proba=y_pred_proba
    )
    return result.raw_metrics
def build_xgb_params(
    config_manager=None,
    overrides: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Convenience function for parameter building.
    Simple wrapper around XGBoostParamsBuilder.build_params for common usage.
    """
    return XGBoostParamsBuilder.build_params(
        config_manager=config_manager,
        overrides=overrides
    )
</file>

<file path="federated/client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO, ERROR, WARNING
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from src.federated.utils import save_predictions_to_csv, get_class_names_list
import importlib.util
from sklearn.utils.class_weight import compute_sample_weight
def get_default_model_params():
    """
    Get default XGBoost parameters for UNSW_NB15 multi-class classification.
    Returns:
        dict: Default XGBoost parameters
    """
    return {
        'objective': 'multi:softprob',  # Multi-class classification with probabilities
        'num_class': 11,  # Classes: 0-10 (Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic, plus class 10)
        'eval_metric': 'mlogloss',  # Use single metric instead of list
        'learning_rate': 0.05,
        'max_depth': 6,
        'min_child_weight': 1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'scale_pos_weight': 1.0  # Removed class-specific weights as it's not compatible with multi-class with >3 classes
    }
def load_tuned_params():
    """
    Try to load tuned parameters if available.
    Returns:
        dict: Tuned parameters if available, else default parameters
    """
    try:
        # Check if tuned_params.py exists
        tuned_params_path = os.path.join(os.path.dirname(__file__), "tuned_params.py")
        if os.path.exists(tuned_params_path):
            # Dynamically import the tuned parameters
            spec = importlib.util.spec_from_file_location("tuned_params", tuned_params_path)
            tuned_params_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(tuned_params_module)
            # Use the tuned parameters
            log(INFO, "Using tuned XGBoost parameters from Ray Tune optimization")
            return tuned_params_module.TUNED_PARAMS
        else:
            return get_default_model_params()
    except Exception as e:
        log(INFO, f"Could not load tuned parameters: {str(e)}")
        return get_default_model_params()
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
        cid: Client ID for logging purposes
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        cid,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None,
        use_tuned_params=True,
        config_manager=None
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            cid: Client ID for logging purposes
            params (dict): XGBoost parameters (defaults to ConfigManager or fallback if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
            use_tuned_params (bool): Whether to use tuned parameters if available
            config_manager (ConfigManager): ConfigManager instance for getting model parameters
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.cid = cid
        # Set model parameters based on priority: provided params > ConfigManager > tuned params > defaults
        if params is not None:
            self.params = params
        elif config_manager is not None:
            self.params = config_manager.get_model_params_dict()
            log(INFO, "Using XGBoost parameters from ConfigManager")
        elif use_tuned_params:
            self.params = load_tuned_params()
        else:
            self.params = get_default_model_params()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Get training data with weights
        y_train = self.train_dmatrix.get_label()
        y_train_int = y_train.astype(int)
        # Compute sample weights for class imbalance
        try:
            sample_weights = compute_sample_weight('balanced', y_train_int)
        except Exception as e:
            log(INFO, f"Error computing sample weights in _local_boost: {e}. Using uniform weights.")
            sample_weights = np.ones(len(y_train_int))
        # Create weighted DMatrix for local training
        dtrain_weighted = xgb.DMatrix(
            self.train_dmatrix.get_data(), 
            label=y_train, 
            weight=sample_weights, 
            feature_names=self.train_dmatrix.feature_names
        )
        # Use xgb.train with early stopping for better performance
        bst = xgb.train(
            self.params,
            dtrain_weighted,
            num_boost_round=self.num_local_round,
            xgb_model=bst_input,  # Continue training from existing model
            evals=[(self.valid_dmatrix, "validate"), (dtrain_weighted, "train")],
            early_stopping_rounds=10,  # Reduced for faster convergence in FL
            verbose_eval=False  # Reduced verbosity for performance
        )
        # Handle model extraction based on training method
        if self.train_method == "bagging":
            # For bagging, extract only the last N trees
            total_trees = bst.num_boosted_rounds()
            start_tree = max(0, total_trees - self.num_local_round)
            bst_extracted = bst[start_tree:total_trees]
            return bst_extracted
        else:
            # For cyclic, return the entire model
            return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        # --- PHASE 1: Aggressive Regularization (Overrides any loaded/tuned params) --- REMOVED
        y_train = self.train_dmatrix.get_label()
        # --- Check if labels are empty ---
        if y_train.size == 0:
            log(ERROR, f"Client {self.cid}: Training DMatrix has no labels. Cannot proceed with fit.")
            return FitRes(
                status=Status(code=Code.FIT_NOT_IMPLEMENTED, message="Training data is missing labels."),
                parameters=Parameters(tensor_type="", tensors=[]), # Return empty params
                num_examples=0,
                metrics={}
            )
        # --- End Check ---
        # Ensure labels are integers for compute_sample_weight
        y_train_int = y_train.astype(int)
        class_counts = np.bincount(y_train_int)
        # Log class distribution for all classes - FIXED: Match server mapping order
        class_names = get_class_names_list()
        for i, count in enumerate(class_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        # --- Reduced Debugging for Performance ---
        log(INFO, f"Unique values in y_train_int: {np.unique(y_train_int)}")
        log(INFO, f"Min/Max values in y_train_int: {np.min(y_train_int)} / {np.max(y_train_int)}")
        # --- End Debugging ---
        # Compute sample weights for class imbalance
        try:
            sample_weights = compute_sample_weight('balanced', y_train_int) # Use integer labels
            log(INFO, f"Successfully computed sample weights. Shape: {sample_weights.shape}, dtype: {sample_weights.dtype}")
        except IndexError as e:
            log(INFO, f"IndexError during compute_sample_weight: {e}")
            log(INFO, f"Unique labels causing issue: {np.unique(y_train_int)}")
            # As a fallback, use uniform weights
            log(INFO, "Falling back to uniform sample weights.")
            sample_weights = np.ones(len(y_train_int))
        except Exception as e:
            log(INFO, f"Other error during compute_sample_weight: {e}")
            log(INFO, "Falling back to uniform sample weights due to unexpected error.")
            sample_weights = np.ones(len(y_train_int))
        # Create a new DMatrix with weights for training
        dtrain_weighted = xgb.DMatrix(self.train_dmatrix.get_data(), label=y_train, weight=sample_weights, feature_names=self.train_dmatrix.feature_names)
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch with sample weights - REDUCED early stopping for FL
            bst = xgb.train(
                self.params,
                dtrain_weighted,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (dtrain_weighted, "train")],
                early_stopping_rounds=10,  # Reduced from 20 for faster FL
                verbose_eval=False  # Reduced verbosity for performance
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        # Since objective is multi:softprob, predict() outputs probabilities
        y_pred_proba = bst.predict(self.valid_dmatrix)
        # Get class labels from probabilities
        y_pred_labels = np.argmax(y_pred_proba, axis=1)
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = get_class_names_list()
        num_classes_actual = len(class_names) # Or get from self.params if needed
        for i, count in enumerate(true_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics using predicted labels
        # Add zero_division=0 to handle cases where a class might not be predicted
        precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss using probabilities
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_true_int = y_true.astype(int)
        # Ensure y_true_int does not contain labels outside the expected range [0, num_classes-1]
        valid_indices = (y_true_int >= 0) & (y_true_int < num_classes_actual)
        if not np.all(valid_indices):
            log(WARNING, f"Found {np.sum(~valid_indices)} labels outside expected range [0, {num_classes_actual-1}]. Clamping for mlogloss calculation.")
            y_true_int = np.clip(y_true_int, 0, num_classes_actual - 1)
            # Optionally filter data if clamping is not desired:
            # y_true_int = y_true_int[valid_indices]
            # y_pred_proba = y_pred_proba[valid_indices]
        y_true_one_hot = np.eye(num_classes_actual)[y_true_int]
        # Ensure y_pred_proba has the correct shape and handle potential issues
        if y_pred_proba.shape == (len(y_true_int), num_classes_actual):
            # Clip probabilities to avoid log(0)
            y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)
            mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_clipped), axis=1))
        else:
            log(WARNING, f"Shape mismatch for mlogloss: y_pred_proba shape {y_pred_proba.shape}, expected ({len(y_true_int)}, {num_classes_actual}). Skipping mlogloss.")
            mlogloss = -1.0 # Indicate failure to calculate
        # Compute confusion matrix using predicted labels
        try:
            # Explicitly provide labels to ensure consistent matrix size
            conf_matrix = confusion_matrix(y_true, y_pred_labels, labels=range(num_classes_actual))
        except Exception as e:
            log(WARNING, f"Error computing confusion matrix: {str(e)}")
            # Create empty confusion matrix with the correct size
            conf_matrix = np.zeros((num_classes_actual, num_classes_actual), dtype=int)
        # Generate detailed classification report using predicted labels
        try:
            # Ensure target_names matches the actual number of classes
            unique_labels = np.unique(np.concatenate((y_true.astype(int), y_pred_labels))) # Get labels present in data
            target_names_filtered = [class_names[i] for i in range(num_classes_actual) if i in unique_labels]
            # Ensure we use labels consistent with target_names_filtered
            labels_for_report = [i for i in range(num_classes_actual) if i in unique_labels]
            class_report = classification_report(
                y_true, 
                y_pred_labels, 
                labels=labels_for_report, 
                target_names=target_names_filtered, 
                zero_division=0
            )
            log(INFO, f"Classification Report:\n{class_report}")
        except Exception as e:
            log(WARNING, f"Error generating classification report: {str(e)}")
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix shape: {conf_matrix.shape}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from src.federated.utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss)
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
# Alias for backward compatibility
XGBClient = XgbClient
</file>

<file path="federated/client.py">
"""
Client implementation for XGBoost federated learning.
This module implements the Flower client for federated XGBoost training,
including data loading, local training, and parameter exchange with the server.
"""
import os
import sys
import json
import numpy as np
import xgboost as xgb
from typing import Dict, List, Tuple, Union, Optional
import flwr as fl
from flwr.common.logger import log
from logging import INFO, WARNING, ERROR
from flwr.common import (
    NDArrays,
    Parameters,
    FitIns,
    FitRes,
    EvaluateIns,
    EvaluateRes,
)
# Import dataset and utility functions
from src.core.dataset import (
    load_csv_data,
    transform_dataset_to_dmatrix,
    FeatureProcessor,
    create_global_feature_processor,
    load_global_feature_processor,
    resplit,
    instantiate_partitioner,
)
from datasets import Dataset
from src.config.config_manager import get_config_manager, load_config
from .client_utils import XGBClient
def load_data(client_id: int, config, global_processor_path: str = None) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Load and partition data for a specific client.
    Args:
        client_id (int): The ID of the client (0-indexed)
        config: Configuration object from ConfigManager
        global_processor_path (str): Path to the global feature processor
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: Training data, test data, and processor
    """
    log(INFO, f"Loading data for client {client_id}")
    # Get data file path from config
    data_file = os.path.join(config.data.path, config.data.filename)
    # Load global feature processor if available
    processor = None
    if global_processor_path and os.path.exists(global_processor_path):
        log(INFO, f"Loading global feature processor from {global_processor_path}")
        processor = load_global_feature_processor(global_processor_path)
    else:
        log(WARNING, f"Global processor not found at {global_processor_path}, will create new one")
    # Load the dataset
    dataset = load_csv_data(data_file)
    # Create partitioner
    partitioner = instantiate_partitioner(
        config.federated.partitioner_type, 
        config.federated.num_partitions
    )
    # Apply the partitioner to the dataset
    partitioner.dataset = dataset
    # Get the partition for this client
    client_dataset = partitioner.load_partition(client_id, "train")  # Use train split for partitioning
    # Convert to DMatrix for training
    train_data = transform_dataset_to_dmatrix(client_dataset, processor=processor, is_training=True)
    # Use the test split from the main dataset for evaluation (not partitioned)
    test_dataset = dataset["test"]
    test_data = transform_dataset_to_dmatrix(test_dataset, processor=processor, is_training=False)
    log(INFO, f"Client {client_id} - Training samples: {train_data.num_row()}, Test samples: {test_data.num_row()}")
    log(INFO, f"Client {client_id} - Features: {train_data.num_col()}")
    return train_data, test_data, processor
def start_client(config, client_id: int, global_processor_path: str = None, use_https: bool = False):
    """
    Start a Flower client for federated XGBoost learning.
    Args:
        config: Configuration object from ConfigManager
        client_id (int): Unique identifier for this client
        global_processor_path (str): Path to the global feature processor
        use_https (bool): Whether to use HTTPS for server communication
    """
    log(INFO, f"Starting client {client_id}")
    log(INFO, f"Server address: 0.0.0.0:8080")  # Default server address
    log(INFO, f"Data path: {config.data.path}")
    log(INFO, f"Data filename: {config.data.filename}")
    log(INFO, f"Partition type: {config.federated.partitioner_type}")
    log(INFO, f"Number of partitions: {config.federated.num_partitions}")
    log(INFO, f"Global processor: {global_processor_path}")
    # Load client data
    train_data, test_data, processor = load_data(
        client_id=client_id,
        config=config,
        global_processor_path=global_processor_path
    )
    # Create XGBoost client
    client = XGBClient(
        train_data=train_data,
        test_data=test_data,
        processor=processor,
        client_id=client_id
    )
    # Connect to server
    server_address = "0.0.0.0:8080"  # Default server address
    if use_https:
        fl.client.start_client(
            server_address=server_address,
            client=client.to_client(),
            transport="grpc-bidi"
        )
    else:
        fl.client.start_client(
            server_address=server_address,
            client=client.to_client()
        )
def main():
    """Main function to start the federated learning client."""
    # Load configuration using ConfigManager
    log(INFO, "Loading configuration for federated client...")
    config = load_config()  # Load base configuration
    log(INFO, "Configuration loaded successfully:")
    log(INFO, "Data path: %s", config.data.path)
    log(INFO, "Data filename: %s", config.data.filename)
    log(INFO, "Partition type: %s", config.federated.partitioner_type)
    log(INFO, "Number of partitions: %d", config.federated.num_partitions)
    # For demonstration, start client 0 (in real deployment, this would be passed as argument)
    client_id = 0  # This could be passed as environment variable or command line arg
    # Validate data file exists
    data_file = os.path.join(config.data.path, config.data.filename)
    if not os.path.exists(data_file):
        log(ERROR, f"Data file not found: {data_file}")
        return
    # Set up global processor path
    global_processor_path = os.path.join(config.outputs.base_dir, "global_feature_processor.pkl")
    # Create global processor if it doesn't exist
    processor_dir = os.path.dirname(global_processor_path)
    if not os.path.exists(processor_dir):
        os.makedirs(processor_dir, exist_ok=True)
    if not os.path.exists(global_processor_path):
        log(INFO, f"Creating global feature processor at {global_processor_path}")
        create_global_feature_processor(data_file, processor_dir)
    # Start the client
    try:
        start_client(
            config=config,
            client_id=client_id,
            global_processor_path=global_processor_path,
            use_https=False
        )
    except KeyboardInterrupt:
        log(INFO, "Client stopped by user")
    except Exception as e:
        log(ERROR, f"Client failed with error: {e}")
        raise
if __name__ == "__main__":
    main()
</file>

<file path="federated/server.py">
"""
Server implementation for XGBoost federated learning.
This module implements the Flower server for federated XGBoost training,
including aggregation strategies, evaluation, and model persistence.
"""
import warnings
from logging import INFO, WARNING
import os
import sys
import json
import time
import numpy as np
import xgboost as xgb
from typing import Dict, List, Tuple, Union, Optional, Any
import flwr as fl
from flwr.common.logger import log
from flwr.common import Parameters, FitRes, EvaluateRes, parameters_to_ndarrays, ndarrays_to_parameters
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from flwr.server.client_proxy import ClientProxy
from src.config.config_manager import get_config_manager, load_config
from src.federated.utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
    reset_metrics_history,
    should_stop_early,
    save_evaluation_results,
    save_predictions_to_csv,
    METRICS_HISTORY,
    get_class_names_list  # Import the authoritative class names function
)
# Import dataset and utility functions
from src.core.dataset import transform_dataset_to_dmatrix, load_csv_data, FeatureProcessor, create_global_feature_processor, load_global_feature_processor
warnings.filterwarnings("ignore", category=UserWarning)
# Load configuration using ConfigManager
log(INFO, "Loading configuration for federated server...")
config = load_config()  # Load base configuration
log(INFO, "Configuration loaded successfully:")
log(INFO, "Training method: %s", config.federated.train_method)
log(INFO, "Pool size: %d", config.federated.pool_size)
log(INFO, "Number of rounds: %d", config.federated.num_rounds)
log(INFO, "Clients per round: %d", config.federated.num_clients_per_round)
log(INFO, "Centralized evaluation: %s", config.federated.centralised_eval)
# Get configuration values for server
train_method = config.federated.train_method
pool_size = config.federated.pool_size
num_rounds = config.federated.num_rounds
num_clients_per_round = config.federated.num_clients_per_round
num_evaluate_clients = config.federated.num_evaluate_clients
centralised_eval = config.federated.centralised_eval
# Get model parameters from ConfigManager
config_manager = get_config_manager()
config_manager._config = config  # Set the config in manager
BST_PARAMS = config_manager.get_model_params_dict()
# Create output directory structure
output_dir = setup_output_directory()
# Reset metrics history for new training run
reset_metrics_history()
# Create global feature processor before starting federated learning
log(INFO, "Creating global feature processor for consistent preprocessing across all clients...")
test_csv_path = os.path.join(config.data.path, config.data.filename)
global_processor_path = create_global_feature_processor(test_csv_path, output_dir)
global_processor = load_global_feature_processor(global_processor_path)
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    # Use the engineered dataset for testing
    log(INFO, "Using original final dataset with temporal splitting for centralized evaluation: %s", test_csv_path)
    test_set = load_csv_data(test_csv_path)["test"]
    test_set.set_format("pandas")
    test_df = test_set.to_pandas()
    # Use the global processor for consistent evaluation
    log(INFO, "Using global feature processor for centralized evaluation")
    # Transform to DMatrix with the global processor
    test_dmatrix = transform_dataset_to_dmatrix(
        test_df, 
        processor=global_processor,
        is_training=False
    )
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
class CustomFedXgbBagging(FedXgbBagging):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.early_stopping_patience = 3
        self.early_stopping_min_delta = 0.001
    def aggregate_evaluate(self, server_round, results, failures):
        if self.evaluate_metrics_aggregation_fn is not None:
            eval_metrics = []
            for r in results:
                # Case 1: Object with num_examples and metrics
                if hasattr(r, "num_examples") and hasattr(r, "metrics"):
                    eval_metrics.append((r.num_examples, r.metrics))
                # Case 2: Tuple of (num_examples, metrics_dict)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and isinstance(r[0], (int, float))
                    and isinstance(r[1], dict)
                ):
                    eval_metrics.append(r)
                # Case 3: Tuple of (client_proxy, EvaluateRes)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and hasattr(r[1], "num_examples")
                    and hasattr(r[1], "metrics")
                ):
                    eval_metrics.append((r[1].num_examples, r[1].metrics))
                else:
                    raise TypeError(
                        f"aggregate_evaluate: Unexpected result format: {type(r)}, value: {r}"
                    )
            aggregated_result = self.evaluate_metrics_aggregation_fn(eval_metrics)
            if not (isinstance(aggregated_result, tuple) and len(aggregated_result) == 2):
                raise TypeError("aggregate_evaluate must return (loss, dict)")
            loss, metrics = aggregated_result
            if not isinstance(metrics, dict):
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            # Check for early stopping after aggregating metrics
            if should_stop_early(self.early_stopping_patience, self.early_stopping_min_delta):
                log(INFO, "Early stopping triggered at round %d", server_round)
                # Note: Flower doesn't have a built-in way to stop training early
                # This will log the early stopping condition, but training will continue
                # In a production system, you might want to implement a custom server loop
            return loss, metrics
        return super().aggregate_evaluate(server_round, results, failures)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = CustomFedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        log(INFO, "[DEBUG] aggregate_evaluate received aggregated_result type: %s, value: %s", type(aggregated_result), aggregated_result)
        # Expect (loss, metrics_dict)
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "[ERROR] Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        log(INFO, "[ERROR] Unexpected format from aggregate_evaluate: %s", type(aggregated_result))
        raise TypeError("aggregate_evaluate must return (loss, dict)")
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add distributed losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["losses_distributed"] = history.losses_distributed
else:
    results["losses_distributed"] = []
    log(INFO, "No distributed losses found in history")
# Add centralized losses if available
if hasattr(history, 'losses_centralized') and history.losses_centralized:
    results["losses_centralized"] = history.losses_centralized
else:
    results["losses_centralized"] = []
    log(INFO, "No centralized losses found in history")
# Add distributed metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics_distributed"] = history.metrics_distributed
else:
    results["metrics_distributed"] = {}
    log(INFO, "No distributed metrics found in history")
# Add centralized metrics if available
if hasattr(history, 'metrics_centralized') and history.metrics_centralized:
    results["metrics_centralized"] = history.metrics_centralized
else:
    results["metrics_centralized"] = {}
    log(INFO, "No centralized metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    save_evaluation_results(history.metrics_distributed[-1][1], num_rounds, output_dir)
    # Also save as aggregated results for consistency
    save_evaluation_results(history.metrics_distributed[-1][1], "aggregated", output_dir)
elif hasattr(history, 'metrics_centralized') and history.metrics_centralized:
    # For centralized evaluation, save the final centralized metrics as aggregated
    final_centralized_metrics = history.metrics_centralized[-1][1] if len(history.metrics_centralized) > 0 else {}
    save_evaluation_results(final_centralized_metrics, "aggregated", output_dir)
    log(INFO, "Saved centralized evaluation results as aggregated for consistency")
else:
    log(INFO, "No metrics available to save")
log(INFO, "Generating additional visualizations...")
# Use the authoritative class names from the global mapping
CLASS_NAMES = get_class_names_list()
log(INFO, "Using authoritative class names: %s", CLASS_NAMES)
# Import visualization functions and other necessary modules
from src.utils.visualization import (
    plot_learning_curves,
    plot_confusion_matrix as vis_plot_confusion_matrix, # Alias to avoid conflict
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution,
    plot_per_class_metrics,
    plot_prediction_probability_distributions
)
from sklearn.metrics import confusion_matrix
import numpy as np
# 1. Plot Learning Curves (Loss and Metrics over rounds)
try:
    metrics_for_learning_curve = ['accuracy', 'precision', 'recall', 'f1', 'mlogloss'] # Common metrics
    results_pkl_path = os.path.join(output_dir, "results.pkl")
    if os.path.exists(results_pkl_path):
        plot_learning_curves(results_pkl_path, metrics_for_learning_curve, output_dir)
    else:
        log(WARNING, "results.pkl not found at %s, skipping learning curve plots.", results_pkl_path)
except Exception as e:
    log(WARNING, "Failed to generate learning curve plots: %s", e)
# 2. Generate other plots if centralised evaluation was performed and model is available
if centralised_eval and hasattr(strategy, 'global_model') and strategy.global_model is not None and 'test_dmatrix' in globals():
    log(INFO, "Performing final evaluation on centralised test set for detailed visualizations...")
    try:
        # Reconstruct the final model (Booster)
        final_bst = xgb.Booster(params=BST_PARAMS)
        if isinstance(strategy.global_model, (bytes, bytearray)):
            final_bst.load_model(bytearray(strategy.global_model))
        elif isinstance(strategy.global_model, xgb.Booster): # if it was already a booster (e.g. from a custom strategy)
            final_bst = strategy.global_model
        else:
            raise TypeError("Unsupported global_model type in strategy for visualization.")
        y_true = test_dmatrix.get_label()
        y_pred_proba = final_bst.predict(test_dmatrix)
        y_pred = np.argmax(y_pred_proba, axis=1)
        # Plot Confusion Matrix
        conf_matrix_data = confusion_matrix(y_true, y_pred)
        vis_plot_confusion_matrix(conf_matrix_data, CLASS_NAMES, os.path.join(output_dir, "final_confusion_matrix.png"))
        # Plot ROC Curves
        plot_roc_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_roc_curves.png"))
        # Plot Precision-Recall Curves
        plot_precision_recall_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_pr_curves.png"))
        # Plot Class Distribution (True vs Predicted on test set)
        plot_class_distribution(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_class_distribution.png"))
        # Plot Per-Class Metrics (Precision, Recall, F1)
        plot_per_class_metrics(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_per_class_metrics.png"))
        # Plot Prediction Probability Distributions
        # This function saves to output_dir/prediction_probability_distributions.png by default
        plot_prediction_probability_distributions(y_true, y_pred_proba, CLASS_NAMES, output_dir)
        log(INFO, "Successfully generated all detailed visualizations for the final model.")
    except Exception as e:
        log(WARNING, "Failed to generate final model visualizations: %s", e)
elif not centralised_eval:
    log(INFO, "Centralised evaluation was not enabled. Skipping final model detailed visualizations.")
elif not (hasattr(strategy, 'global_model') and strategy.global_model is not None):
    log(INFO, "No final global model available in strategy. Skipping final model detailed visualizations.")
elif 'test_dmatrix' not in globals():
    log(INFO, "Centralised test_dmatrix not available. Skipping final model detailed visualizations.")
log(INFO, "Server process finished.")
</file>

<file path="federated/sim.py">
import warnings
import os
import sys
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
# Add project root directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # Go up two levels to project root
sys.path.insert(0, project_root)
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from src.core.dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
    FeatureProcessor,
    create_global_feature_processor,
    load_global_feature_processor,
)
from src.config.config_manager import get_config_manager, load_config
from src.utils.enhanced_logging import get_enhanced_logger
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from src.config.tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    # We'll use the value from ConfigManager instead
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from ConfigManager")
from src.federated.utils import (
    setup_output_directory,
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager
)
from src.federated.client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            cid,
            params,
            train_method,
        )
    return client_fn
def main():
    # Get enhanced logger instance
    enhanced_logger = get_enhanced_logger()
    # Load configuration using ConfigManager
    enhanced_logger.logger.info("Loading configuration for federated simulation...")
    config = load_config()  # Load base configuration
    enhanced_logger.logger.info("Configuration loaded successfully:")
    enhanced_logger.logger.info("Training method: %s", config.federated.train_method)
    enhanced_logger.logger.info("Pool size: %d", config.federated.pool_size)
    enhanced_logger.logger.info("Number of rounds: %d", config.federated.num_rounds)
    enhanced_logger.logger.info("Clients per round: %d", config.federated.num_clients_per_round)
    enhanced_logger.logger.info("Centralized evaluation: %s", config.federated.centralised_eval)
    enhanced_logger.logger.info("Partitioner type: %s", config.federated.partitioner_type)
    # Get data file path
    csv_file_path = os.path.join(config.data.path, config.data.filename)
    enhanced_logger.logger.info("Loading dataset from: %s", csv_file_path)
    # Load CSV dataset
    dataset = load_csv_data(csv_file_path)
    # Log dataset statistics with proper error handling
    try:
        # Calculate total samples more robustly
        if hasattr(dataset, 'num_rows'):
            total_samples = dataset.num_rows
        else:
            train_samples = len(dataset['train']) if 'train' in dataset else 0
            test_samples = len(dataset['test']) if 'test' in dataset else 0
            total_samples = train_samples + test_samples
        # Extract features for logging
        if 'train' in dataset and len(dataset['train']) > 0:
            sample_data = dataset['train'][0]
            features = list(sample_data.keys()) if hasattr(sample_data, 'keys') else []
        else:
            features = []
        # Build data statistics safely
        data_stats = {
            'total_samples': int(total_samples),  # Ensure it's an integer
            'features': features,
            'train_samples': int(len(dataset['train']) if 'train' in dataset else 0),
            'test_samples': int(len(dataset['test']) if 'test' in dataset else 0),
        }
        enhanced_logger.log_data_statistics(data_stats)
    except Exception as e:
        enhanced_logger.logger.warning("Could not extract detailed dataset statistics: %s", str(e))
        enhanced_logger.logger.info("Dataset loaded successfully from: %s", csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=config.federated.partitioner_type, 
        num_partitions=config.federated.pool_size
    )
    fds = dataset
    # Load centralised test set
    if config.federated.centralised_eval:
        enhanced_logger.logger.info("Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    enhanced_logger.logger.info("Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(config.federated.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if config.federated.centralised_eval:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=config.federated.test_fraction, seed=config.data.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if config.federated.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if config.federated.centralised_eval else None
            ),
            fraction_fit=(float(config.federated.num_clients_per_round) / config.federated.pool_size),
            min_fit_clients=config.federated.num_clients_per_round,
            min_available_clients=config.federated.pool_size,
            min_evaluate_clients=(
                config.federated.num_evaluate_clients if not config.federated.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not config.federated.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not config.federated.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=config.federated.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": config.federated.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = config.model.num_local_rounds
    # Get model parameters from ConfigManager
    config_manager = get_config_manager()
    config_manager._config = config  # Set the config in manager
    params = config_manager.get_model_params_dict()
    # Setup learning rate
    if config.federated.train_method == "bagging" and config.federated.scaled_lr:
        new_lr = params["eta"] / config.federated.pool_size
        params.update({"eta": new_lr})
        enhanced_logger.logger.info("Scaled learning rate applied: %f", new_lr)
    enhanced_logger.logger.info("🚀 Starting simulation with %d rounds...", config.federated.num_rounds)
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            config.federated.train_method,
            params,
            num_local_round,
        ),
        num_clients=config.federated.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=config.federated.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if config.federated.train_method == "cyclic" else None,
    )
    enhanced_logger.logger.info("✅ Simulation completed successfully!")
if __name__ == "__main__":
    main()
</file>

<file path="federated/utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, log_loss, accuracy_score
from logging import INFO, WARNING
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
import os
import json
import shutil
from datetime import datetime
import pickle
import numpy as np
# Assuming visualization_utils.py is in the same directory or accessible via PYTHONPATH
from src.utils.visualization import (
    plot_confusion_matrix,
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution,
    plot_learning_curves
)
import warnings
# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
# Global variable to track metrics history for early stopping
METRICS_HISTORY = []
# Authoritative label mapping for UNSW_NB15 dataset (11 classes for engineered dataset)
UNSW_NB15_LABEL_MAPPING = {
    0: 'Normal',
    1: 'Generic', 
    2: 'Exploits',
    3: 'Reconnaissance',
    4: 'Fuzzers',
    5: 'DoS',
    6: 'Analysis',
    7: 'Backdoor',
    8: 'Backdoors',
    9: 'Worms',
    10: 'Shellcode'  # Engineered dataset has 11 classes (0-10)
}
# Helper function to get class names list
def get_class_names_list():
    """Get the list of class names in correct order."""
    return [UNSW_NB15_LABEL_MAPPING[i] for i in range(len(UNSW_NB15_LABEL_MAPPING))]
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    # aggregated_metrics["loss"] = loss  # REMOVED - Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["mlogloss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Add metrics to history for early stopping tracking
    add_metrics_to_history(aggregated_metrics)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    if not (isinstance(loss, (int, float)) and isinstance(aggregated_metrics, dict)):
        log(INFO, "[ERROR] Output of evaluate_metrics_aggregation is not (loss, dict): %s, %s", type(loss), type(aggregated_metrics))
        raise TypeError("evaluate_metrics_aggregation must return (loss, dict)")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices or array of probabilities)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'Normal', 'Reconnaissance', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Check if predictions is a 2D array (multi-class probabilities)
    if isinstance(predictions, np.ndarray) and len(predictions.shape) > 1:
        log(INFO, "Detected multi-class probability predictions with shape: %s", predictions.shape)
        # Convert probabilities to class labels
        predicted_labels = np.argmax(predictions, axis=1)
    else:
        # Already a list of class indices
        predicted_labels = predictions
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predicted_labels,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Use the global authoritative label mapping
        predictions_dict['prediction_type'] = [UNSW_NB15_LABEL_MAPPING.get(int(label), f'unknown_{label}') for label in predicted_labels]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
        # Generate and save visualizations if we have true labels to compare with
        try:
            class_names = get_class_names_list()
            num_classes = len(class_names)
            # Convert to numpy arrays if they're not already
            y_true = np.array(true_labels) if not isinstance(true_labels, np.ndarray) else true_labels
            y_pred = np.array(predicted_labels) if not isinstance(predicted_labels, np.ndarray) else predicted_labels
            # Create confusion matrix
            cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{round_num}.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{round_num}.png")
            plot_class_distribution(y_true, y_pred, class_names, dist_path)
            log(INFO, f"Visualizations saved for round {round_num}")
        except Exception as e:
            log(WARNING, f"Error generating visualizations: {e}")
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path, config_manager=None):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
        config_manager (ConfigManager, optional): ConfigManager instance for getting model parameters
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params from ConfigManager
            try:
                if config_manager is not None:
                    model_params = config_manager.get_model_params_dict()
                    bst = xgb.Booster(params=model_params)
                else:
                    # Fallback to basic params if no ConfigManager available
                    basic_params = {
                        "objective": "multi:softprob",
                        "num_class": 11,
                        "tree_method": "hist"
                    }
                    bst = xgb.Booster(params=basic_params)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path, config_manager=None):
    # Load the model
    model = load_saved_model(model_path, config_manager)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions shape: %s", raw_predictions.shape if hasattr(raw_predictions, 'shape') else 'scalar')
    # Log distribution of scores
    if hasattr(raw_predictions, 'shape'):
        log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
            np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # For multi-class with multi:softprob, the raw predictions will be probabilities for each class
    if hasattr(raw_predictions, 'shape') and len(raw_predictions.shape) > 1:
        log(INFO, "Processing multi-class probability predictions with shape: %s", raw_predictions.shape)
        predicted_labels = np.argmax(raw_predictions, axis=1)
        # Use the global authoritative label mapping
        # Save predictions to CSV with class names
        predictions_df = pd.DataFrame({
            'predicted_label': predicted_labels,
            'prediction_type': [UNSW_NB15_LABEL_MAPPING.get(int(p), f'unknown_{p}') for p in predicted_labels],
        })
        # Add probability columns for each class
        for i in range(raw_predictions.shape[1]):
            predictions_df[f'prob_class_{i}'] = raw_predictions[:, i]
    elif len(raw_predictions.shape) == 1:  # Binary case or multi:softmax
        # Check if this is binary classification or multi-class with direct labels
        if np.max(raw_predictions) <= 1.0 and np.min(raw_predictions) >= 0.0:
            # Binary case
            probabilities = raw_predictions  # Already probabilities
            predicted_labels = (probabilities >= 0.5).astype(int)
            # Save predictions to CSV
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
                'prediction_score': probabilities
            })
        else:
            # Likely multi:softmax with direct class labels
            predicted_labels = np.round(raw_predictions).astype(int)
            # Use the global authoritative label mapping
            # Save predictions to CSV with class names
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': [UNSW_NB15_LABEL_MAPPING.get(int(p), f'unknown_{p}') for p in predicted_labels],
            })
    else:
        # Fallback for unexpected prediction format
        log(WARNING, "Unexpected prediction format. Creating basic predictions DataFrame.")
        predictions_df = pd.DataFrame({
            'raw_prediction': raw_predictions
        })
    # Log predicted class distribution
    if 'predicted_label' in predictions_df.columns:
        unique, counts = np.unique(predictions_df['predicted_label'], return_counts=True)
        log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Generate visualizations if true labels are available
    try:
        true_labels = dmatrix.get_label()
        if true_labels is not None and 'predicted_label' in predictions_df.columns:
            output_dir = os.path.dirname(output_path)
            num_classes = max(11, np.max(true_labels) + 1)  # Ensure at least 11 classes for the engineered dataset
            class_names = [UNSW_NB15_LABEL_MAPPING.get(i, f'Class_{i}') for i in range(num_classes)]
            predicted_labels = predictions_df['predicted_label'].values
            # Create confusion matrix
            cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))
            cm_path = os.path.join(output_dir, "final_confusion_matrix.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, "final_class_distribution.png")
            plot_class_distribution(true_labels, predicted_labels, class_names, dist_path)
            # Plot ROC and Precision-Recall Curves (for multi-class)
            if len(raw_predictions.shape) > 1 and raw_predictions.shape[1] >= num_classes:
                roc_path = os.path.join(output_dir, "final_roc_curves.png")
                plot_roc_curves(true_labels, raw_predictions, class_names, roc_path)
                pr_path = os.path.join(output_dir, "final_precision_recall_curves.png")
                plot_precision_recall_curves(true_labels, raw_predictions, class_names, pr_path)
            log(INFO, "Visualizations saved with final model predictions")
    except Exception as e:
        log(WARNING, f"Error generating visualizations: {e}")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data, config_manager=None):
    """Return a function for centralised evaluation."""
    def evaluate_model(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            # Get model parameters from ConfigManager if available
            if config_manager is not None:
                model_params = config_manager.get_model_params_dict()
            else:
                # Fallback to basic params if no ConfigManager available
                model_params = {
                    "objective": "multi:softprob",
                    "num_class": 11,
                    "tree_method": "hist"
                }
            bst = xgb.Booster(params=model_params)
            para_b = None
            for para in parameters.tensors:
                para_b = bytearray(para)
                break  # Take the first parameter tensor
            if para_b is not None:
                bst.load_model(para_b)
            else:
                # No parameters provided, create a new model with default params
                log(WARNING, "No model parameters provided, using fresh model")
                bst = xgb.Booster(params=model_params)
            # Get predictions
            y_pred_proba = bst.predict(test_data)
            # For multi:softprob, we get probabilities for each class
            # Convert to labels by taking argmax if predictions are probabilities
            if isinstance(y_pred_proba, np.ndarray) and len(y_pred_proba.shape) > 1:
                y_pred_labels = np.argmax(y_pred_proba, axis=1)
                log(INFO, "Converting probability predictions to labels (argmax), shape: %s", y_pred_proba.shape)
            else:
                y_pred_labels = y_pred_proba  # Already labels
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_proba, server_round, "results", y_true)
            # Compute metrics using the predictions
            predictions = y_pred_labels  # Use the converted labels for metrics
            pred_proba = y_pred_proba    # The original probabilities for plots that need them
            # Ensure pred_proba has the correct shape for multi-class
            if len(pred_proba.shape) == 1 or pred_proba.shape[1] == 1:
                 # If predict gives labels or single class proba, try predict_proba if available
                 try:
                     # Note: XGBoost predict() with multi:softmax directly gives labels.
                     # To get probabilities, the objective might need to be multi:softprob
                     log(WARNING, "Predict output seems 1D, attempting to handle for multi-class probability plots...")
                     if model_params.get('objective') == 'multi:softmax':
                         # Create dummy probabilities centered around the predicted class
                         num_classes = model_params.get('num_class', 11) # Default to 11 if not set
                         pred_proba = np.zeros((len(predictions), num_classes))
                         for i, label in enumerate(predictions):
                             if 0 <= int(label) < num_classes: # Check bounds
                                pred_proba[i, int(label)] = 0.9 # Assign high prob to predicted
                                other_prob = 0.1 / max(1, (num_classes - 1))
                                for j in range(num_classes):
                                    if j != int(label):
                                        pred_proba[i,j] = other_prob
                             else:
                                 log(WARNING, f"Prediction label {label} out of bounds [0, {num_classes-1}]")
                                 # Assign uniform probability as fallback if label is invalid
                                 pred_proba[i, :] = 1.0 / num_classes
                         log(WARNING, "Reconstructed dummy probabilities for multi:softmax. Plots may be inaccurate. Consider using 'multi:softprob' objective for better probability estimates.")
                     else: # Cannot determine probabilities
                         pred_proba = None
                 except AttributeError:
                     log(WARNING, "Could not get probabilities, ROC and PR curves will not be generated.")
                     pred_proba = None
                 except Exception as e:
                     log(WARNING, f"Error processing probabilities: {e}. ROC/PR plots skipped.")
                     pred_proba = None
            elif pred_proba.shape[1] != model_params.get('num_class', 11):
                 log(WARNING, f"Probability shape mismatch ({pred_proba.shape[1]} columns vs {model_params.get('num_class', 11)} classes). Plots may fail.")
                 # Attempt to proceed, but plots requiring probabilities might error out
            # Calculate metrics
            # Ensure y_test is integer type for log_loss if using one-hot encoding
            y_test_int = y_true.astype(int)
            num_classes_actual = model_params.get('num_class', 11)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                 try:
                     # Manual clipping to replace deprecated eps parameter
                     epsilon = 1e-15
                     pred_proba_clipped = np.clip(pred_proba, epsilon, 1 - epsilon)
                     loss = log_loss(y_test_int, pred_proba_clipped, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"pred_proba shape: {pred_proba.shape}")
                     log(WARNING, f"pred_proba sample: {pred_proba[:5]}")
            else:
                 log(WARNING, "Calculating log_loss using one-hot encoding due to missing/invalid probabilities.")
                 try:
                     predictions_int = np.array(predictions).astype(int)
                     # Manual clipping to replace deprecated eps parameter
                     epsilon = 1e-15
                     one_hot_predictions = np.eye(num_classes_actual)[predictions_int]
                     one_hot_clipped = np.clip(one_hot_predictions, epsilon, 1 - epsilon)
                     loss = log_loss(y_test_int, one_hot_clipped, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during one-hot log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"predictions unique: {np.unique(predictions)}, shape: {predictions.shape}")
            accuracy = accuracy_score(y_true, predictions)
            precision = precision_score(y_true, predictions, average='weighted', zero_division=0)
            recall = recall_score(y_true, predictions, average='weighted', zero_division=0)
            f1 = f1_score(y_true, predictions, average='weighted', zero_division=0)
            cm = confusion_matrix(y_true, predictions, labels=range(num_classes_actual)) # Ensure labels match num_classes
            log(INFO, f"Centralized eval round {server_round} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            # --- Generate and Save Plots ---
            output_dir = config.get("output_dir", "results") # Get output dir from config or default
            # Instead of creating a separate plots directory, save directly in output_dir
            log(INFO, f"Saving evaluation plots to: {output_dir}")
            # Update class names to use the global authoritative mapping
            class_names = get_class_names_list()
            # Plot Confusion Matrix
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{server_round}.png")
            plot_confusion_matrix(cm, class_names[:num_classes_actual], cm_path) # Use actual num_classes
            # Plot Class Distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{server_round}.png")
            plot_class_distribution(y_test_int, predictions.astype(int), class_names[:num_classes_actual], dist_path)
            # Plot ROC and Precision-Recall Curves (only if probabilities are available and valid)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                roc_path = os.path.join(output_dir, f"roc_curves_round_{server_round}.png")
                plot_roc_curves(y_test_int, pred_proba, class_names[:num_classes_actual], roc_path)
                pr_path = os.path.join(output_dir, f"precision_recall_curves_round_{server_round}.png")
                plot_precision_recall_curves(y_test_int, pred_proba, class_names[:num_classes_actual], pr_path)
            else:
                 log(WARNING, f"Skipping ROC and PR curve generation due to unavailable/invalid probabilities (shape: {pred_proba.shape if pred_proba is not None else 'None'}).")
            # --- End Plot Generation ---
            # Return metrics
            return loss, {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}
    return evaluate_model
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
def check_convergence(metrics_history: List[Dict], patience: int = 3, min_delta: float = 0.001) -> bool:
    """
    Check if training has converged based on loss history.
    Args:
        metrics_history (List[Dict]): List of metrics from previous rounds
        patience (int): Number of rounds to wait for improvement before stopping
        min_delta (float): Minimum change in loss to be considered an improvement
    Returns:
        bool: True if training should stop (converged), False otherwise
    """
    if len(metrics_history) < patience + 1:
        return False
    # Extract recent losses (mlogloss)
    recent_losses = []
    for metrics in metrics_history[-(patience + 1):]:
        loss = metrics.get('mlogloss', metrics.get('loss', float('inf')))
        recent_losses.append(loss)
    # Calculate improvements between consecutive rounds
    improvements = []
    for i in range(len(recent_losses) - 1):
        improvement = recent_losses[i] - recent_losses[i + 1]
        improvements.append(improvement)
    # Check if all recent improvements are below threshold
    converged = all(imp < min_delta for imp in improvements)
    if converged:
        log(INFO, "Early stopping triggered: No significant improvement in last %d rounds", patience)
        log(INFO, "Recent losses: %s", recent_losses)
        log(INFO, "Recent improvements: %s", improvements)
    return converged
def reset_metrics_history():
    """Reset the global metrics history (useful for new training runs)."""
    global METRICS_HISTORY
    METRICS_HISTORY = []
    log(INFO, "Metrics history reset for new training run")
def add_metrics_to_history(metrics: Dict):
    """Add metrics from current round to history for convergence tracking."""
    global METRICS_HISTORY
    METRICS_HISTORY.append(metrics.copy())
    log(INFO, "Added metrics to history. Total rounds tracked: %d", len(METRICS_HISTORY))
def should_stop_early(patience: int = 3, min_delta: float = 0.001) -> bool:
    """
    Check if early stopping should be triggered based on current metrics history.
    Args:
        patience (int): Number of rounds to wait for improvement
        min_delta (float): Minimum improvement threshold
    Returns:
        bool: True if training should stop early
    """
    global METRICS_HISTORY
    return check_convergence(METRICS_HISTORY, patience, min_delta)
</file>

<file path="models/use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the
federated learning process to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data>
    --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json
    --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from src.federated.utils import load_saved_model
from src.core.dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get feature importance: %s", str(error))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get model parameters: %s", str(error))
def clean_data_for_xgboost(data_frame):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        data_frame (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = data_frame.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # Use max() to set minimum threshold
        threshold = max(threshold, 1e6)
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path):
    """
    Save detailed prediction information to CSV for multi-class classification.
    Args:
        predictions (np.ndarray): Raw predictions from the model
        output_path (str): Path to save the predictions
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are multi-dimensional (one-hot encoded)
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        # Store raw probabilities
        results_df['raw_probabilities'] = predictions.tolist()
        # Get predicted class (argmax)
        predicted_labels = np.argmax(predictions, axis=1)
        results_df['predicted_label'] = predicted_labels
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predicted_labels
        ]
        # Store confidence scores (probability of predicted class)
        results_df['prediction_score'] = predictions[
            np.arange(len(predicted_labels)),
            predicted_labels
        ]
    else:
        # For single class predictions
        results_df['predicted_label'] = predictions.astype(int)
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predictions
        ]
        # Default confidence score of 1.0 for direct class predictions
        results_df['prediction_score'] = 1.0
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    label_counts = results_df['predicted_label'].value_counts()
    log(INFO, "Prediction counts by class:")
    for label, count in label_counts.items():
        class_name = label_mapping.get(int(label), f'unknown_{label}')
        log(INFO, "  %s: %d", class_name, count)
    if 'prediction_score' in results_df.columns:
        log(INFO, "Confidence score statistics: min=%.6f, max=%.6f, mean=%.6f",
            results_df['prediction_score'].min(),
            results_df['prediction_score'].max(),
            results_df['prediction_score'].mean())
    return results_df
def evaluate_labeled_data(model, dataset, output_path):
    """Handle evaluation of labeled data."""
    # Convert to DMatrix
    dmatrix = transform_dataset_to_dmatrix(dataset)
    # Get true labels for evaluation
    y_true = dmatrix.get_label()
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save detailed predictions
    _ = save_detailed_predictions(raw_predictions, output_path)
    # Evaluate if data has labels
    if raw_predictions.ndim > 1:
        y_pred_labels = np.argmax(raw_predictions, axis=1)
    else:
        y_pred_labels = raw_predictions.astype(int)
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_labels)
    precision = precision_score(y_true, y_pred_labels, average='weighted')
    recall = recall_score(y_true, y_pred_labels, average='weighted')
    f1_score_val = f1_score(y_true, y_pred_labels, average='weighted')
    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_labels)
    # Generate classification report
    class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
    report = classification_report(y_true, y_pred_labels, target_names=class_names)
    # Log evaluation results
    log(INFO, "Evaluation Results:")
    log(INFO, "  Accuracy: %.4f", accuracy)
    log(INFO, "  Precision (weighted): %.4f", precision)
    log(INFO, "  Recall (weighted): %.4f", recall)
    log(INFO, "  F1 Score (weighted): %.4f", f1_score_val)
    log(INFO, "Confusion Matrix:\n%s", conf_matrix)
    log(INFO, "Classification Report:\n%s", report)
def predict_unlabeled_data(model, data_path, output_path):
    """Handle prediction of unlabeled data."""
    # Load unlabeled data
    data = pd.read_csv(data_path)
    # Clean data
    data = clean_data_for_xgboost(data)
    # Convert to DMatrix
    dmatrix = xgb.DMatrix(data)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save predictions
    save_detailed_predictions(raw_predictions, output_path)
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Process data based on whether it has labels
        if args.has_labels:
            try:
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                evaluate_labeled_data(model, dataset, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during evaluation: %s", str(error))
                raise
        else:
            try:
                predict_unlabeled_data(model, args.data_path, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during prediction: %s", str(error))
                raise
    except Exception as error:  # pylint: disable=broad-except
        log(INFO, "Error: %s", str(error))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="models/use_tuned_params.py">
"""
use_tuned_params.py
This script loads the optimized hyperparameters found by Ray Tune and integrates them
into the existing federated learning system. It replaces the default XGBoost parameters
in both client_utils.py and utils.py with the optimized ones.
Usage:
    python use_tuned_params.py --params-file ./tune_results/best_params.json
"""
import os
import sys
import json
import argparse
import logging
# Add project root directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # Go up two levels to project root
sys.path.insert(0, project_root)
from src.config.config_manager import ConfigManager
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def get_default_model_params():
    """
    Get default model parameters using ConfigManager or fallback values.
    Returns:
        dict: Default XGBoost parameters
    """
    try:
        # Try to get parameters from ConfigManager
        config_manager = ConfigManager()
        config_manager.load_config()  # Load the configuration first
        return config_manager.get_model_params_dict()
    except (ImportError, AttributeError, ValueError, KeyError, RuntimeError) as e:
        logger.warning("Could not load parameters from ConfigManager: %s", e)
        # Fallback to hardcoded defaults
        return {
            "objective": "multi:softprob",
            "num_class": 11,
            "eta": 0.05,
            "max_depth": 8,
            "min_child_weight": 5,
            "gamma": 0.5,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "colsample_bylevel": 0.8,
            "nthread": 16,
            "tree_method": "hist",
            "eval_metric": "mlogloss",
            "max_delta_step": 1,
            "reg_alpha": 0.1,
            "reg_lambda": 1.0,
            "base_score": 0.5,
            "scale_pos_weight": 1.0,
            "grow_policy": "depthwise",
            "normalize_type": "tree",
            "random_state": 42
        }
def load_tuned_params(params_file):
    """
    Load the optimized hyperparameters from a JSON file.
    Args:
        params_file (str): Path to the JSON file containing the optimized parameters
    Returns:
        dict: Optimized hyperparameters
    """
    if not os.path.exists(params_file):
        if params_file == "./tune_results/best_params.json":
            logger.error("Default parameters file not found: %s", params_file)
            logger.error("This usually means Ray Tune hasn't been run yet or completed successfully.")
            logger.error("Please run the Ray Tune optimization first or specify a different --params-file")
        raise FileNotFoundError("Parameters file not found: %s" % params_file)
    logger.info("Loading optimized parameters from %s", params_file)
    with open(params_file, 'r', encoding='utf-8') as f:
        params = json.load(f)
    return params
def create_xgboost_params(tuned_params):
    """
    Create XGBoost parameters dictionary from the tuned parameters.
    Args:
        tuned_params (dict): Optimized hyperparameters from Ray Tune
    Returns:
        dict: XGBoost parameters dictionary for use in the existing system
    """
    # Start with the base parameters from ConfigManager or defaults
    xgb_params = get_default_model_params()
    # Update with tuned parameters - convert float values to ints for integer parameters
    # Use .get() with defaults to handle missing parameters gracefully
    xgb_params.update({
        'max_depth': int(tuned_params.get('max_depth', 6)),
        'min_child_weight': int(tuned_params.get('min_child_weight', 1)),
        'eta': tuned_params.get('eta', 0.1),
        'subsample': tuned_params.get('subsample', 0.8),
        'colsample_bytree': tuned_params.get('colsample_bytree', 0.8),
        'reg_alpha': tuned_params.get('reg_alpha', 0.1),
        'reg_lambda': tuned_params.get('reg_lambda', 1.0)
    })
    # Add new hyperparameters if they exist in tuned_params
    if 'gamma' in tuned_params:
        xgb_params['gamma'] = tuned_params['gamma']
    if 'scale_pos_weight' in tuned_params:
        xgb_params['scale_pos_weight'] = tuned_params['scale_pos_weight']
    if 'max_delta_step' in tuned_params:
        xgb_params['max_delta_step'] = int(tuned_params['max_delta_step'])
    if 'colsample_bylevel' in tuned_params:
        xgb_params['colsample_bylevel'] = tuned_params['colsample_bylevel']
    if 'colsample_bynode' in tuned_params:
        xgb_params['colsample_bynode'] = tuned_params['colsample_bynode']
    # Add num_boost_round if it exists in tuned_params
    if 'num_boost_round' in tuned_params:
        xgb_params['num_boost_round'] = int(tuned_params['num_boost_round'])
    # Add GPU support if specified in tuned parameters
    if 'tree_method' in tuned_params:
        if isinstance(tuned_params['tree_method'], list) and len(tuned_params['tree_method']) > 0:
            # If it's from hp.choice, it will be a list
            xgb_params['tree_method'] = tuned_params['tree_method'][0]
        else:
            xgb_params['tree_method'] = tuned_params['tree_method']
    return xgb_params
def save_updated_params(params, output_file):
    """
    Save updated parameters to a Python file that can be imported by client_utils.py
    Args:
        params (dict): Updated XGBoost parameters
        output_file (str): Path to save the updated parameters
    """
    # Extract num_boost_round if present to use as NUM_LOCAL_ROUND
    num_local_round = None
    if 'num_boost_round' in params:
        num_local_round = int(params['num_boost_round'])
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# This file is generated automatically by use_tuned_params.py\n")
        f.write("# It contains optimized XGBoost parameters found by Ray Tune\n\n")
        # Add NUM_LOCAL_ROUND if it was extracted from num_boost_round
        if num_local_round is not None:
            f.write(f"NUM_LOCAL_ROUND = {num_local_round}\n\n")
        f.write("TUNED_PARAMS = {\n")
        for key, value in params.items():
            if isinstance(value, str):
                f.write(f"    '{key}': '{value}',\n")
            elif isinstance(value, list):
                f.write(f"    '{key}': {value},\n")
            else:
                f.write(f"    '{key}': {value},\n")
        f.write("}\n")
    logger.info("Updated XGBoost parameters saved to %s", output_file)
    if num_local_round is not None:
        logger.info("NUM_LOCAL_ROUND set to %d based on tuned num_boost_round", num_local_round)
def backup_original_params():
    """
    Backup the original parameters to a JSON file for reference.
    Returns:
        str: Path to the backup file
    """
    backup_file = "original_bst_params.json"
    original_params = get_default_model_params()
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(original_params, f, indent=2)
    logger.info("Original parameters backed up to %s", backup_file)
    return backup_file
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Use tuned hyperparameters with the existing XGBoost client")
    parser.add_argument(
        "--params-file", 
        type=str, 
        default="./tune_results/best_params.json",
        help="Path to the tuned parameters JSON file (default: ./tune_results/best_params.json)"
    )
    parser.add_argument("--output-file", type=str, default="tuned_params.py", help="Output Python file for updated parameters")
    args = parser.parse_args()
    # Log which parameters file is being used
    if args.params_file == "./tune_results/best_params.json":
        logger.info("Using default parameters file: %s", args.params_file)
    else:
        logger.info("Using specified parameters file: %s", args.params_file)
    # Backup original parameters
    backup_file = backup_original_params()
    # Load tuned parameters
    tuned_params = load_tuned_params(args.params_file)
    logger.info("Loaded the following optimized parameters:")
    for key, value in tuned_params.items():
        logger.info("  %s: %s", key, value)
    # Create updated XGBoost parameters
    updated_params = create_xgboost_params(tuned_params)
    # Save updated parameters
    save_updated_params(updated_params, args.output_file)
    # Success message
    logger.info("Optimized parameters saved to %s", args.output_file)
    logger.info("Original parameters backed up to %s", backup_file)
    logger.info("These parameters will be automatically used by the XGBoost client")
if __name__ == "__main__":
    main()
</file>

<file path="tuning/ray_tune_xgboost.py">
"""
Ray Tune XGBoost Training with Improved Hyperparameter Optimization
Enhanced implementation with expanded search spaces, better early stopping,
and robust error handling for federated learning environments.
"""
import os
import sys
import warnings
import json
import pickle
import time
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
import pandas as pd
import xgboost as xgb
from functools import partial
# Suppress warnings for cleaner output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
# Ray Tune imports
import ray
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.stopper import TrialPlateauStopper
from ray.air import session
# Scikit-learn imports  
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
# Add project root directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))  # Go up two levels to project root
sys.path.insert(0, project_root)
# Import local modules
from src.core.dataset import load_csv_data, transform_dataset_to_dmatrix, create_global_feature_processor, load_global_feature_processor
# Ray setup - reduce verbosity and resource warnings
ray.init(
    ignore_reinit_error=True,
    log_to_driver=False,
    configure_logging=False,
    local_mode=False  # Set to True for debugging
)
# Global constants for the enhanced hyperparameter search with wider variety
ENHANCED_PARAM_SPACE = {
    # Learning rate - significantly expanded range for better exploration
    'eta': tune.loguniform(0.001, 0.8),  # Expanded from (0.01, 0.5) to include very low and high learning rates
    # Tree depth - wider range for complex patterns  
    'max_depth': tune.randint(2, 20),  # Expanded from (3, 15) to include shallow and very deep trees
    # Minimum child weight - helps with overfitting, expanded range
    'min_child_weight': tune.randint(1, 25),  # Expanded from (1, 15) to include higher values
    # Subsample ratio for training instances
    'subsample': tune.uniform(0.3, 1.0),  # Added subsample parameter for better diversity
    # Column subsampling ratios - prevent overfitting with wider ranges
    'colsample_bytree': tune.uniform(0.3, 1.0),  # Expanded from (0.5, 1.0) to allow more aggressive subsampling
    'colsample_bylevel': tune.uniform(0.3, 1.0),  # Expanded from (0.5, 1.0)
    'colsample_bynode': tune.uniform(0.3, 1.0),   # Expanded from (0.5, 1.0)
    # Regularization - L1 and L2 with much wider ranges
    'reg_alpha': tune.loguniform(1e-10, 1000),  # Expanded from (1e-8, 100) for stronger regularization
    'reg_lambda': tune.loguniform(1e-10, 1000), # Expanded from (1e-8, 100) for stronger regularization
    # Gamma - minimum loss reduction required to make split, expanded range
    'gamma': tune.loguniform(1e-10, 10.0),  # Expanded from (1e-8, 1.0) to allow stronger pruning
    # Scale positive weight for imbalanced datasets
    'scale_pos_weight': tune.loguniform(0.1, 10.0),  # New parameter for handling class imbalance
    # Maximum delta step - limits the max output of tree leaf
    'max_delta_step': tune.choice([0, 1, 2, 5, 10]),  # New parameter for controlling leaf output
    # Number of boosting rounds with much wider range
    'num_boost_round': tune.randint(10, 1000),  # Expanded from (50, 500) to include very few and many rounds
    # Tree growing policy
    'grow_policy': tune.choice(['depthwise', 'lossguide']),  # New parameter for tree construction strategy
    # Maximum number of leaves for lossguide policy
    'max_leaves': tune.randint(8, 4096),  # New parameter that works with lossguide policy
}
# Import FeatureProcessor for consistent preprocessing
from src.core.dataset import FeatureProcessor
class EnhancedXGBoostTrainer:
    """Enhanced XGBoost trainer with improved hyperparameter optimization."""
    def __init__(self, data_file: str, test_data_file: str = None, 
                 num_samples: int = 100, max_concurrent_trials: int = 4,
                 use_global_processor: bool = True):
        """
        Initialize the enhanced XGBoost trainer.
        Args:
            data_file (str): Path to training data CSV file
            test_data_file (str): Path to test data CSV file (optional)
            num_samples (int): Number of hyperparameter configurations to try
            max_concurrent_trials (int): Maximum concurrent Ray Tune trials
            use_global_processor (bool): Whether to use global feature processor
        """
        self.data_file = data_file
        self.test_data_file = test_data_file
        self.num_samples = num_samples
        self.max_concurrent_trials = max_concurrent_trials
        self.use_global_processor = use_global_processor
        # Initialize data structures - Store raw data instead of DMatrix
        self.train_data = None
        self.test_data = None
        self.train_features = None
        self.train_labels = None
        self.test_features = None
        self.test_labels = None
        self.global_processor = None
        self.best_params = None
        self.best_score = None
        self.results_history = []
        print(f"Initialized Enhanced XGBoost Trainer")
        print(f"Training data: {data_file}")
        print(f"Test data: {test_data_file if test_data_file else 'Using train/test split'}")
        print(f"Hyperparameter samples: {num_samples}")
        print(f"Max concurrent trials: {max_concurrent_trials}")
        print(f"Using global processor: {use_global_processor}")
    def load_and_prepare_data(self):
        """Load and prepare training and test data."""
        print("\n" + "="*60)
        print("LOADING AND PREPARING DATA")
        print("="*60)
        # Create or load global feature processor
        if self.use_global_processor:
            processor_path = "outputs/global_feature_processor.pkl"
            if os.path.exists(processor_path):
                print(f"Loading existing global feature processor from {processor_path}")
                self.global_processor = load_global_feature_processor(processor_path)
            else:
                print(f"Creating new global feature processor")
                processor_path = create_global_feature_processor(self.data_file, "outputs")
                self.global_processor = load_global_feature_processor(processor_path)
        # Load training data
        print(f"\nLoading training data from: {self.data_file}")
        dataset_dict = load_csv_data(self.data_file)
        # Convert to DMatrix format for final model training
        train_dataset = dataset_dict["train"]
        self.train_data = transform_dataset_to_dmatrix(
            train_dataset, 
            processor=self.global_processor,
            is_training=True
        )
        # Extract raw features and labels for Ray Tune (these can be pickled)
        # Convert Dataset to pandas DataFrame first
        train_df = train_dataset.to_pandas()
        self.train_features = train_df.drop(columns=['label']).values
        self.train_labels = train_df['label'].values
        print(f"Training data: {self.train_data.num_row()} samples, {self.train_data.num_col()} features")
        # Load test data
        if self.test_data_file and os.path.exists(self.test_data_file):
            print(f"\nLoading separate test data from: {self.test_data_file}")
            # Load test data using the same processor
            test_dataset_dict = load_csv_data(self.test_data_file)
            test_dataset = test_dataset_dict["test"]  # Use test split from test file
            self.test_data = transform_dataset_to_dmatrix(
                test_dataset,
                processor=self.global_processor,
                is_training=False
            )
        else:
            print(f"\nUsing train/test split from main dataset")
            test_dataset = dataset_dict["test"]
            self.test_data = transform_dataset_to_dmatrix(
                test_dataset,
                processor=self.global_processor,
                is_training=False
            )
        # Extract raw features and labels for Ray Tune
        # Convert Dataset to pandas DataFrame first
        test_df = test_dataset.to_pandas()
        self.test_features = test_df.drop(columns=['label']).values
        self.test_labels = test_df['label'].values
        print(f"Test data: {self.test_data.num_row()} samples, {self.test_data.num_col()} features")
        # Verify data integrity
        train_labels = self.train_data.get_label()
        test_labels = self.test_data.get_label()
        print(f"\nData integrity check:")
        print(f"Training labels - min: {train_labels.min()}, max: {train_labels.max()}, unique: {len(np.unique(train_labels))}")
        print(f"Test labels - min: {test_labels.min()}, max: {test_labels.max()}, unique: {len(np.unique(test_labels))}")
        # Check for class distribution
        train_unique, train_counts = np.unique(train_labels, return_counts=True)
        test_unique, test_counts = np.unique(test_labels, return_counts=True)
        print(f"\nClass distribution:")
        print(f"Training: {dict(zip(train_unique, train_counts))}")
        print(f"Test: {dict(zip(test_unique, test_counts))}")
    def tune_hyperparameters(self):
        """Run hyperparameter tuning using Ray Tune."""
        print("\n" + "="*60)
        print("HYPERPARAMETER TUNING")
        print("="*60)
        if self.train_features is None:
            self.load_and_prepare_data()
        # Create partial function with raw data arrays (these can be pickled)
        train_func = partial(
            train_with_config,
            train_features=self.train_features,
            train_labels=self.train_labels,
            test_features=self.test_features,
            test_labels=self.test_labels
        )
        # Configure ASHA scheduler for early stopping
        scheduler = ASHAScheduler(
            metric="f1_weighted",  # Use F1 score as main metric
            mode="max",
            max_t=500,  # Maximum number of boosting rounds
            grace_period=50,  # Minimum rounds before stopping
            reduction_factor=2
        )
        # Configure trial stopping criteria
        stopper = TrialPlateauStopper(
            metric="f1_weighted",
            mode="max"
        )
        print(f"Starting hyperparameter tuning with {self.num_samples} trials...")
        print(f"Search space: {ENHANCED_PARAM_SPACE}")
        # Run hyperparameter tuning
        analysis = tune.run(
            train_func,
            config=ENHANCED_PARAM_SPACE,
            num_samples=self.num_samples,
            scheduler=scheduler,
            stop=stopper,
            resources_per_trial={"cpu": 1},
            max_concurrent_trials=self.max_concurrent_trials,
            verbose=1,
            raise_on_failed_trial=False  # Continue even if some trials fail
        )
        # Get best parameters
        best_trial = analysis.get_best_trial("f1_weighted", "max", "last")
        self.best_params = best_trial.config
        self.best_score = best_trial.last_result["f1_weighted"]
        print(f"\n" + "="*60)
        print("BEST HYPERPARAMETERS FOUND")
        print("="*60)
        print(f"Best F1 Score: {self.best_score:.4f}")
        print(f"Best Parameters:")
        for param, value in self.best_params.items():
            print(f"  {param}: {value}")
        # Store results for analysis
        self.results_history = analysis.results_df
        return analysis
    def train_final_model(self):
        """Train final model with best hyperparameters."""
        print("\n" + "="*60)
        print("TRAINING FINAL MODEL")
        print("="*60)
        if self.best_params is None:
            raise ValueError("No best parameters found. Run tune_hyperparameters() first.")
        # Prepare final parameters
        final_params = {
            'objective': 'multi:softprob',
            'eval_metric': 'mlogloss',
            'eta': self.best_params['eta'],
            'max_depth': int(self.best_params['max_depth']),
            'min_child_weight': int(self.best_params['min_child_weight']),
            'colsample_bytree': self.best_params['colsample_bytree'],
            'colsample_bylevel': self.best_params.get('colsample_bylevel', 1.0),
            'colsample_bynode': self.best_params.get('colsample_bynode', 1.0),
            'reg_alpha': self.best_params.get('reg_alpha', 0),
            'reg_lambda': self.best_params.get('reg_lambda', 1),
            'gamma': self.best_params.get('gamma', 0),
            'seed': 42,
            'verbosity': 1
        }
        # Determine number of classes
        train_labels = self.train_data.get_label()
        num_classes = len(np.unique(train_labels))
        final_params['num_class'] = num_classes
        num_boost_round = int(self.best_params['num_boost_round'])
        print(f"Training final model with {num_boost_round} boosting rounds...")
        print(f"Number of classes: {num_classes}")
        # Train final model
        evallist = [(self.train_data, 'train'), (self.test_data, 'eval')]
        evals_result = {}
        final_model = xgb.train(
            final_params,
            self.train_data,
            num_boost_round=num_boost_round,
            evals=evallist,
            evals_result=evals_result,
            verbose_eval=50
        )
        # Evaluate final model
        test_pred_proba = final_model.predict(self.test_data)
        if len(test_pred_proba.shape) > 1:
            test_pred = np.argmax(test_pred_proba, axis=1)
        else:
            test_pred = (test_pred_proba > 0.5).astype(int)
        test_true = self.test_data.get_label().astype(int)
        # Calculate comprehensive metrics
        accuracy = accuracy_score(test_true, test_pred)
        print(f"\n" + "="*60)
        print("FINAL MODEL PERFORMANCE")
        print("="*60)
        print(f"Test Accuracy: {accuracy:.4f}")
        # Detailed classification report
        print("\nClassification Report:")
        print(classification_report(test_true, test_pred))
        # Save final model and parameters
        os.makedirs("outputs", exist_ok=True)
        model_path = "outputs/final_xgboost_model.json"
        final_model.save_model(model_path)
        print(f"\nFinal model saved to: {model_path}")
        params_path = "outputs/final_hyperparameters.json"
        with open(params_path, 'w') as f:
            json.dump(self.best_params, f, indent=2)
        print(f"Best hyperparameters saved to: {params_path}")
        return final_model, final_params
    def run_complete_tuning(self):
        """Run complete hyperparameter tuning and training pipeline."""
        print("Starting Enhanced XGBoost Hyperparameter Tuning Pipeline")
        print(f"Data file: {self.data_file}")
        start_time = time.time()
        try:
            # Step 1: Load and prepare data
            self.load_and_prepare_data()
            # Step 2: Tune hyperparameters
            analysis = self.tune_hyperparameters()
            # Step 3: Train final model
            final_model, final_params = self.train_final_model()
            total_time = time.time() - start_time
            print(f"\n" + "="*60)
            print("PIPELINE COMPLETED SUCCESSFULLY")
            print("="*60)
            print(f"Total time: {total_time/60:.2f} minutes")
            print(f"Best F1 Score: {self.best_score:.4f}")
            return {
                'model': final_model,
                'params': final_params,
                'best_score': self.best_score,
                'analysis': analysis,
                'total_time': total_time
            }
        except Exception as e:
            print(f"Pipeline failed with error: {e}")
            raise
        finally:
            # Shutdown Ray
            ray.shutdown()
def train_with_config(config: Dict[str, Any], train_features: np.ndarray, train_labels: np.ndarray, 
                     test_features: np.ndarray, test_labels: np.ndarray):
    """
    Standalone training function that can be pickled for Ray Tune.
    This function recreates DMatrix objects inside each worker.
    """
    try:
        # Create DMatrix objects inside the worker (avoiding pickling issues)
        train_dmatrix = xgb.DMatrix(train_features, label=train_labels)
        test_dmatrix = xgb.DMatrix(test_features, label=test_labels)
        # Extract hyperparameters from config
        params = {
            'objective': 'multi:softprob',  # Multi-class classification
            'eval_metric': 'mlogloss',      # Multi-class log loss
            'eta': config['eta'],
            'max_depth': int(config['max_depth']),
            'min_child_weight': int(config['min_child_weight']),
            'subsample': config.get('subsample', 1.0),  # Added subsample parameter
            'colsample_bytree': config['colsample_bytree'],
            'colsample_bylevel': config.get('colsample_bylevel', 1.0),
            'colsample_bynode': config.get('colsample_bynode', 1.0),
            'reg_alpha': config.get('reg_alpha', 0),
            'reg_lambda': config.get('reg_lambda', 1),
            'gamma': config.get('gamma', 0),
            'scale_pos_weight': config.get('scale_pos_weight', 1.0),  # Added for class imbalance
            'max_delta_step': config.get('max_delta_step', 0),  # Added for controlling leaf output
            'grow_policy': config.get('grow_policy', 'depthwise'),  # Added tree growing policy
            'seed': 42,
            'verbosity': 0  # Reduce XGBoost verbosity
        }
        # Handle max_leaves parameter only for lossguide policy
        if params['grow_policy'] == 'lossguide':
            params['max_leaves'] = int(config.get('max_leaves', 256))
        num_boost_round = int(config['num_boost_round'])
        # Determine number of classes for multi-class setup
        num_classes = len(np.unique(train_labels))
        params['num_class'] = num_classes
        # Early stopping configuration
        early_stopping_rounds = max(10, num_boost_round // 10)
        # Create evaluation list for monitoring
        evallist = [(train_dmatrix, 'train'), (test_dmatrix, 'eval')]
        # Train the model with early stopping
        evals_result = {}
        model = xgb.train(
            params,
            train_dmatrix,
            num_boost_round=num_boost_round,
            evals=evallist,
            evals_result=evals_result,
            early_stopping_rounds=early_stopping_rounds,
            verbose_eval=False  # Disable verbose evaluation
        )
        # Make predictions on test set
        test_pred_proba = model.predict(test_dmatrix)
        # For multi-class, convert probabilities to class predictions
        if len(test_pred_proba.shape) > 1:
            test_pred = np.argmax(test_pred_proba, axis=1)
        else:
            test_pred = (test_pred_proba > 0.5).astype(int)
        # Get true labels
        test_true = test_labels.astype(int)
        # Calculate metrics
        accuracy = accuracy_score(test_true, test_pred)
        # Calculate per-class metrics for multi-class
        if num_classes > 2:
            # Get weighted average F1 score for multi-class
            from sklearn.metrics import f1_score
            f1 = f1_score(test_true, test_pred, average='weighted')
            # Use F1 score as the main metric for multi-class problems
            main_metric = f1
            metric_name = 'f1_weighted'
        else:
            # For binary classification, use accuracy
            main_metric = accuracy
            metric_name = 'accuracy'
        # Get the final evaluation loss
        final_train_loss = evals_result['train']['mlogloss'][-1]
        final_eval_loss = evals_result['eval']['mlogloss'][-1]
        # Report metrics to Ray Tune using correct format (metrics dictionary)
        metrics = {
            'accuracy': accuracy,
            metric_name: main_metric,
            'train_loss': final_train_loss,
            'eval_loss': final_eval_loss,
            'num_boost_round_used': model.best_iteration + 1 if hasattr(model, 'best_iteration') else num_boost_round
        }
        session.report(metrics)
    except Exception as e:
        print(f"Training failed with error: {e}")
        # Report poor performance for failed trials using correct format
        metrics = {
            'accuracy': 0.0,
            'f1_weighted': 0.0,
            'train_loss': float('inf'),
            'eval_loss': float('inf')
        }
        session.report(metrics)
def main():
    """Main function to run hyperparameter tuning."""
    import argparse
    parser = argparse.ArgumentParser(description="Enhanced XGBoost Hyperparameter Tuning")
    parser.add_argument("--data-file", type=str, default="data/UNSW-NB15_1.csv",
                       help="Path to the training data CSV file")
    parser.add_argument("--test-data-file", type=str, default=None,
                       help="Path to separate test data CSV file (optional)")
    parser.add_argument("--num-samples", type=int, default=100,
                       help="Number of hyperparameter configurations to try")
    parser.add_argument("--cpus-per-trial", type=int, default=2,
                       help="Number of CPUs per trial")
    parser.add_argument("--output-dir", type=str, default="./tune_results",
                       help="Output directory for results")
    args = parser.parse_args()
    # Use command line arguments
    data_file = args.data_file
    test_data_file = args.test_data_file
    num_samples = args.num_samples
    max_concurrent_trials = args.cpus_per_trial
    print("Enhanced XGBoost Hyperparameter Tuning")
    print("="*60)
    print(f"Data file: {data_file}")
    print(f"Test data file: {test_data_file}")
    print(f"Number of samples: {num_samples}")
    print(f"CPUs per trial: {max_concurrent_trials}")
    print(f"Output directory: {args.output_dir}")
    # Check if data file exists
    if not os.path.exists(data_file):
        print(f"Error: Data file not found at {data_file}")
        print("Please check the data file path")
        return
    # Initialize trainer
    trainer = EnhancedXGBoostTrainer(
        data_file=data_file,
        test_data_file=test_data_file,
        num_samples=num_samples,
        max_concurrent_trials=max_concurrent_trials,
        use_global_processor=True
    )
    # Run complete tuning pipeline
    try:
        results = trainer.run_complete_tuning()
        # Save results to the specified output directory
        os.makedirs(args.output_dir, exist_ok=True)
        # Save best parameters to tune_results directory as expected by use_tuned_params.py
        best_params_path = os.path.join(args.output_dir, "best_params.json")
        with open(best_params_path, 'w') as f:
            json.dump(trainer.best_params, f, indent=2)
        print(f"Best parameters saved to: {best_params_path}")
        print(f"\nTuning completed successfully!")
        print(f"Best model saved in outputs/")
        print(f"Best parameters saved in {args.output_dir}/")
    except Exception as e:
        print(f"Error during tuning: {e}")
        # Create a dummy best_params.json file so the pipeline can continue
        os.makedirs(args.output_dir, exist_ok=True)
        dummy_params = {
            "eta": 0.1,
            "max_depth": 6,
            "min_child_weight": 1,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "colsample_bylevel": 0.8,
            "colsample_bynode": 0.8,
            "reg_alpha": 0.1,
            "reg_lambda": 1.0,
            "gamma": 0.0,
            "num_boost_round": 100
        }
        best_params_path = os.path.join(args.output_dir, "best_params.json")
        with open(best_params_path, 'w') as f:
            json.dump(dummy_params, f, indent=2)
        print(f"Created dummy parameters file at: {best_params_path}")
if __name__ == "__main__":
    main()
</file>

<file path="utils/enhanced_logging.py">
"""
Enhanced logging utilities for the Federated Learning Pipeline.
This module provides improved logging capabilities with:
- Better formatting and visual structure
- Timing information and performance metrics
- Progress tracking
- Configuration summaries
- Result summaries
"""
import logging
import time
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path
import sys
from contextlib import contextmanager
class EnhancedLogger:
    """Enhanced logger for the federated learning pipeline."""
    def __init__(self, name: str = "fl_pipeline", log_file: Optional[str] = None):
        """Initialize the enhanced logger.
        Args:
            name: Logger name (changed from "flwr" to avoid conflicts)
            log_file: Optional log file path
        """
        self.logger = logging.getLogger(name)
        self.start_time = time.time()
        self.step_times = {}
        self.step_counter = 0
        # Setup enhanced formatting
        self._setup_logging(log_file)
    def _setup_logging(self, log_file: Optional[str] = None):
        """Setup enhanced logging with better formatting."""
        # Only setup if not already configured
        if self.logger.handlers:
            return
        # Create formatter with enhanced format
        formatter = logging.Formatter(
            '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        # File handler if specified
        if log_file:
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_path)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
        self.logger.setLevel(logging.INFO)
        # Prevent propagation to parent loggers to avoid duplicates
        self.logger.propagate = False
    def pipeline_start(self, config: Any):
        """Log pipeline start with configuration summary."""
        self.start_time = time.time()
        self.logger.info("🚀 Starting Federated Learning Pipeline with Enhanced Monitoring")
        self.logger.info("=" * 100)
        self.logger.info("📊 PIPELINE CONFIGURATION SUMMARY")
        self.logger.info("=" * 100)
        # Configuration summary
        self.logger.info("📁 Dataset: %s/%s", config.data.path, config.data.filename)
        self.logger.info("🔄 Training Method: %s", config.federated.train_method)
        self.logger.info("🔢 Federated Rounds: %d", config.federated.num_rounds)
        self.logger.info("👥 Pool Size: %d", config.federated.pool_size)
        self.logger.info("🎯 Clients per Round: %d", config.federated.num_clients_per_round)
        self.logger.info("📈 Local Rounds: %d", config.model.num_local_rounds)
        self.logger.info("🔧 Hyperparameter Tuning: %s", '✅ Enabled' if config.tuning.enabled else '❌ Disabled')
        self.logger.info("📊 Centralized Evaluation: %s", '✅ Enabled' if config.federated.centralised_eval else '❌ Disabled')
        self.logger.info("🌱 Random Seed: %d", config.data.seed)
        # Model parameters summary
        self.logger.info("")
        self.logger.info("🤖 MODEL PARAMETERS")
        self.logger.info("-" * 50)
        key_params = ['eta', 'max_depth', 'min_child_weight', 'gamma', 'subsample', 'colsample_bytree']
        for param in key_params:
            if hasattr(config.model.params, param):
                value = getattr(config.model.params, param)
                self.logger.info("   %s: %s", param, value)
        self.logger.info("=" * 100)
    def step_start(self, step_name: str, description: str, command: Optional[str] = None):
        """Log the start of a pipeline step."""
        self.step_counter += 1
        step_start_time = time.time()
        self.step_times[step_name] = step_start_time
        self.logger.info("")
        self.logger.info("📋 STEP %d: %s", self.step_counter, step_name.upper())
        self.logger.info("─" * 80)
        self.logger.info("📝 Description: %s", description)
        if command:
            self.logger.info("💻 Command: %s", command)
        self.logger.info("⏰ Started at: %s", datetime.now().strftime('%H:%M:%S'))
        self.logger.info("🔄 Running...")
    def step_success(self, step_name: str, output: Optional[str] = None, metrics: Optional[Dict[str, Any]] = None):
        """Log successful completion of a pipeline step."""
        if step_name in self.step_times:
            duration = time.time() - self.step_times[step_name]
            duration_str = str(timedelta(seconds=int(duration)))
        else:
            duration_str = "Unknown"
        self.logger.info("")
        self.logger.info("✅ Step %d (%s) completed successfully!", self.step_counter, step_name)
        self.logger.info("⏱️  Duration: %s", duration_str)
        if metrics:
            self.logger.info("📊 Metrics:")
            for key, value in metrics.items():
                self.logger.info("   %s: %s", key, value)
        if output:
            # Log output with proper formatting
            self.logger.info("📄 Output:")
            for line in output.strip().split('\n'):
                if line.strip():
                    self.logger.info("   %s", line)
        self.logger.info("─" * 80)
    def step_error(self, step_name: str, error_msg: str, exit_code: Optional[int] = None):
        """Log error in pipeline step."""
        if step_name in self.step_times:
            duration = time.time() - self.step_times[step_name]
            duration_str = str(timedelta(seconds=int(duration)))
        else:
            duration_str = "Unknown"
        self.logger.error("")
        self.logger.error("❌ Step %d (%s) failed!", self.step_counter, step_name)
        self.logger.error("⏱️  Duration: %s", duration_str)
        if exit_code is not None:
            self.logger.error("🔢 Exit Code: %d", exit_code)
        self.logger.error("💥 Error: %s", error_msg)
        self.logger.error("─" * 80)
    def _safe_format_number(self, value: Any, name: str) -> Optional[str]:
        """Safely format a number with commas, handling errors gracefully."""
        try:
            num_value = int(value)
            return f"{num_value:,}"
        except (ValueError, TypeError) as e:
            self.logger.warning("Could not format %s: %s", name, e)
            return None
    def log_data_statistics(self, data_stats: Dict[str, Any]):
        """Log detailed data statistics."""
        self.logger.info("")
        self.logger.info("📊 DATASET STATISTICS")
        self.logger.info("─" * 50)
        # Log basic statistics
        if 'total_samples' in data_stats:
            formatted = self._safe_format_number(data_stats['total_samples'], 'total_samples')
            if formatted:
                self.logger.info("📈 Total Samples: %s", formatted)
        if 'features' in data_stats and data_stats['features']:
            self.logger.info("🔢 Number of Features: %d", len(data_stats['features']))
        if 'classes' in data_stats and data_stats['classes']:
            self.logger.info("🎯 Number of Classes: %d", len(data_stats['classes']))
        if 'train_samples' in data_stats:
            formatted = self._safe_format_number(data_stats['train_samples'], 'train_samples')
            if formatted:
                self.logger.info("🎓 Training Samples: %s", formatted)
        if 'test_samples' in data_stats:
            formatted = self._safe_format_number(data_stats['test_samples'], 'test_samples')
            if formatted:
                self.logger.info("🧪 Test Samples: %s", formatted)
        # Class distribution
        if 'class_distribution' in data_stats:
            try:
                self.logger.info("")
                self.logger.info("📊 Class Distribution:")
                for class_id, counts in data_stats['class_distribution'].items():
                    train_count = int(counts.get('train', 0))
                    test_count = int(counts.get('test', 0))
                    total_count = int(counts.get('total', train_count + test_count))
                    self.logger.info("   Class %s: %s train, %s test, %s total", 
                                   class_id, f"{train_count:,}", f"{test_count:,}", f"{total_count:,}")
            except (ValueError, TypeError) as e:
                self.logger.warning("Could not format class_distribution: %s", e)
        self.logger.info("─" * 50)
    def log_federated_progress(self, round_num: int, total_rounds: int, metrics: Optional[Dict[str, float]] = None):
        """Log federated learning round progress."""
        progress_pct = (round_num / total_rounds) * 100
        progress_bar = "█" * int(progress_pct // 5) + "░" * (20 - int(progress_pct // 5))
        self.logger.info("🔄 Round %d/%d [%s] %.1f%%", round_num, total_rounds, progress_bar, progress_pct)
        if metrics:
            metric_str = " | ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            self.logger.info("📊 Metrics: %s", metric_str)
    def pipeline_complete(self, results_dir: str, final_metrics: Optional[Dict[str, Any]] = None):
        """Log pipeline completion with summary."""
        total_duration = time.time() - self.start_time
        duration_str = str(timedelta(seconds=int(total_duration)))
        self.logger.info("")
        self.logger.info("🎉 FEDERATED LEARNING PIPELINE COMPLETED SUCCESSFULLY!")
        self.logger.info("=" * 100)
        self.logger.info("⏱️  Total Execution Time: %s", duration_str)
        self.logger.info("📁 Results Directory: %s", results_dir)
        if final_metrics:
            self.logger.info("")
            self.logger.info("📊 FINAL RESULTS")
            self.logger.info("─" * 50)
            for metric, value in final_metrics.items():
                if isinstance(value, float):
                    self.logger.info("   %s: %.6f", metric, value)
                else:
                    self.logger.info("   %s: %s", metric, value)
        self.logger.info("")
        self.logger.info("✨ KEY IMPROVEMENTS ACHIEVED:")
        self.logger.info("   ✅ Consistent preprocessing across all phases")
        self.logger.info("   ✅ Temporal splitting to prevent data leakage")
        self.logger.info("   ✅ Global feature processor for uniform data representation")
        self.logger.info("   ✅ Distributed learning with privacy preservation")
        self.logger.info("   ✅ Robust evaluation and monitoring")
        # Step timing summary
        if self.step_times:
            self.logger.info("")
            self.logger.info("⏱️  STEP TIMING SUMMARY")
            self.logger.info("─" * 50)
            for step_name, start_time in self.step_times.items():
                duration = time.time() - start_time
                self.logger.info("   %s: %s", step_name, str(timedelta(seconds=int(duration))))
        self.logger.info("=" * 100)
        self.logger.info("🏁 Pipeline execution completed at: %s", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    @contextmanager
    def timed_step(self, step_name: str, description: str, command: Optional[str] = None):
        """Context manager for timing pipeline steps."""
        self.step_start(step_name, description, command)
        step_start_time = time.time()
        try:
            yield self
        except Exception as e:
            self.step_error(step_name, str(e))
            raise
        finally:
            if step_name not in self.step_times:
                self.step_times[step_name] = step_start_time
# Global enhanced logger instance
_enhanced_logger = None
def get_enhanced_logger(log_file: Optional[str] = None) -> EnhancedLogger:
    """Get the global enhanced logger instance."""
    # pylint: disable=global-statement
    global _enhanced_logger
    if _enhanced_logger is None:
        _enhanced_logger = EnhancedLogger(log_file=log_file)
    return _enhanced_logger
def setup_enhanced_logging(log_file: Optional[str] = None) -> EnhancedLogger:
    """Setup enhanced logging for the pipeline."""
    # pylint: disable=global-statement
    global _enhanced_logger
    # Reset the global logger to ensure clean setup
    _enhanced_logger = None
    return get_enhanced_logger(log_file)
</file>

<file path="utils/visualization.py">
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import pickle
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
from itertools import cycle
from flwr.common.logger import log
from logging import INFO, WARNING
def plot_confusion_matrix(conf_matrix, class_names, output_path):
    """Generates and saves a heatmap visualization of the confusion matrix."""
    try:
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names, ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Confusion matrix plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate confusion matrix plot: %s", e)
def plot_roc_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves ROC curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        fpr = {}
        tpr = {}
        roc_auc = {}
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(fpr[i], tpr[i], color=color, lw=2,
                     label='ROC curve of class {0} (area = {1:0.2f})'
                     ''.format(class_names[i], roc_auc[i]))
        ax.plot([0, 1], [0, 1], 'k--', lw=2)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('Receiver Operating Characteristic (ROC) - One-vs-Rest')
        ax.legend(loc="lower right")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "ROC curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate ROC curve plot: %s", e)
def plot_precision_recall_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves Precision-Recall curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        precision = {}
        recall = {}
        average_precision = {}
        for i in range(n_classes):
            precision[i], recall[i], _ = precision_recall_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            average_precision[i] = auc(recall[i], precision[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(recall[i], precision[i], color=color, lw=2,
                     label='PR curve of class {0} (AP = {1:0.2f})'
                     ''.format(class_names[i], average_precision[i]))
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_ylim([0.0, 1.05])
        ax.set_xlim([0.0, 1.0])
        ax.set_title('Precision-Recall Curve - One-vs-Rest')
        ax.legend(loc="lower left")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Precision-Recall curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate Precision-Recall curve plot: %s", e)
def plot_class_distribution(y_true, y_pred, class_names, output_path):
    """Generates and saves bar plots comparing true and predicted class distributions."""
    try:
        n_classes = len(class_names)
        true_counts = np.bincount(y_true.astype(int), minlength=n_classes)
        pred_counts = np.bincount(y_pred.astype(int), minlength=n_classes)
        x = np.arange(n_classes)
        width = 0.35
        fig, ax = plt.subplots(figsize=(12, 6))
        rects1 = ax.bar(x - width/2, true_counts, width, label='True Labels')
        rects2 = ax.bar(x + width/2, pred_counts, width, label='Predicted Labels')
        ax.set_ylabel('Count')
        ax.set_title('True vs Predicted Class Distribution')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.bar_label(rects1, padding=3)
        ax.bar_label(rects2, padding=3)
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Class distribution plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate class distribution plot: %s", e)
def plot_per_class_metrics(y_true, y_pred, class_names, output_path):
    """Generates and saves a bar chart of per-class precision, recall, and F1-score."""
    try:
        from sklearn.metrics import classification_report
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
        metrics_to_plot = ['precision', 'recall', 'f1-score']
        class_metrics = {class_name: [] for class_name in class_names}
        for class_name in class_names:
            if class_name in report:
                for metric in metrics_to_plot:
                    class_metrics[class_name].append(report[class_name][metric])
            else: 
                for _ in metrics_to_plot:
                    class_metrics[class_name].append(0)
        x = np.arange(len(class_names)) 
        width = 0.2  
        multiplier = 0
        fig, ax = plt.subplots(figsize=(max(12, len(class_names) * 1.5), 6))
        for i, metric_name in enumerate(metrics_to_plot):
            metric_values = [class_metrics[cn][i] for cn in class_names]
            offset = width * multiplier
            rects = ax.bar(x + offset, metric_values, width, label=metric_name.capitalize())
            ax.bar_label(rects, padding=3, fmt='%.2f')
            multiplier += 1
        ax.set_ylabel('Score')
        ax.set_title('Per-Class Precision, Recall, and F1-Score')
        ax.set_xticks(x + width, class_names, rotation=45, ha="right")
        ax.legend(loc='lower right', ncols=len(metrics_to_plot))
        ax.set_ylim(0, 1.1) 
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Per-class metrics plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate per-class metrics plot: %s", e)
def _plot_single_metric_curve(ax, history_attr, metric_key, label_prefix, marker):
    """Helper function to plot a single metric curve on a given axis."""
    plot_successful = False
    if hasattr(history_attr, '__contains__') and metric_key in history_attr: # Flower 1.x format
        if isinstance(history_attr[metric_key], list) and history_attr[metric_key]:
            rounds, values = zip(*history_attr[metric_key])
            ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
            plot_successful = True
    # Flower 2.x format: history_attr is a list of (round, {dict_of_metrics})
    elif isinstance(history_attr, list) and history_attr:
        if all(isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict) for item in history_attr):
            rounds = [item[0] for item in history_attr if metric_key in item[1]]
            values = [item[1][metric_key] for item in history_attr if metric_key in item[1]]
            if rounds and values:
                ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
                plot_successful = True
    return plot_successful
def plot_learning_curves(results_pkl_path, metrics_to_plot, output_dir):
    """Generates and saves learning curve plots (loss and specified metrics vs. round)
       from a pickled Flower history object.
    Args:
        results_pkl_path (str): Path to the results.pkl file.
        metrics_to_plot (list): A list of metric keys (str) to plot from the history object.
        output_dir (str): Directory to save the plots.
    """
    try:
        if not os.path.exists(results_pkl_path):
            log(WARNING, "Results file not found: %s", results_pkl_path)
            return
        with open(results_pkl_path, 'rb') as f:
            history_data = pickle.load(f) # history_data is now a dict
        # Plot Loss
        fig_loss, ax_loss = plt.subplots(figsize=(12, 6))
        loss_plotted = False
        # Access as dictionary keys
        if "losses_distributed" in history_data and history_data["losses_distributed"]:
            rounds_dist, losses_dist = zip(*history_data["losses_distributed"])
            ax_loss.plot(rounds_dist, losses_dist, label='Distributed Loss', marker='o')
            loss_plotted = True
        if "losses_centralized" in history_data and history_data["losses_centralized"]:
            rounds_cent, losses_cent = zip(*history_data["losses_centralized"])
            ax_loss.plot(rounds_cent, losses_cent, label='Centralized Loss', marker='x')
            loss_plotted = True
        if loss_plotted:
            ax_loss.set_title('Loss Over Federated Learning Rounds')
            ax_loss.set_xlabel('Server Round')
            ax_loss.set_ylabel('Loss')
            ax_loss.legend()
            ax_loss.grid(True)
            fig_loss.tight_layout()
            loss_plot_path = os.path.join(output_dir, "learning_curve_loss.png")
            fig_loss.savefig(loss_plot_path)
            log(INFO, "Loss learning curve plot saved to %s", loss_plot_path)
        else:
            log(INFO, "No loss data found to plot.")
        plt.close(fig_loss)
        # Plot Specified Metrics
        if not metrics_to_plot:
            log(INFO, "No metrics specified for plotting learning curves.")
            return
        num_metrics = len(metrics_to_plot)
        fig_metrics, axes_metrics = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=True)
        if num_metrics == 1:
            axes_metrics = [axes_metrics] 
        any_metric_plotted = False
        for i, metric_key in enumerate(metrics_to_plot):
            ax = axes_metrics[i]
            dist_plotted = False
            cent_plotted = False
            # Access as dictionary keys
            if "metrics_distributed" in history_data and history_data["metrics_distributed"]:
                dist_plotted = _plot_single_metric_curve(ax, history_data["metrics_distributed"], metric_key, 'Distributed', 'o')
            if "metrics_centralized" in history_data and history_data["metrics_centralized"]:
                cent_plotted = _plot_single_metric_curve(ax, history_data["metrics_centralized"], metric_key, 'Centralized', 'x')
            if dist_plotted or cent_plotted:
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} Over Federated Learning Rounds')
                ax.set_ylabel(metric_key.capitalize())
                ax.legend()
                ax.grid(True)
                any_metric_plotted = True
            else:
                log(WARNING, "Could not plot metric '%s' from history. Data not found or in unexpected format.", metric_key)
                ax.text(0.5, 0.5, f"Data for '{metric_key}' not found \nor in unexpected format.", 
                        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} (Data Unavailable)')
        if any_metric_plotted:
            axes_metrics[-1].set_xlabel('Server Round') # Set x-label only for the bottom-most plot
            fig_metrics.tight_layout()
            metrics_plot_path = os.path.join(output_dir, "learning_curves_metrics.png")
            fig_metrics.savefig(metrics_plot_path)
            log(INFO, "Metrics learning curves plot saved to %s", metrics_plot_path)
        else:
            log(INFO, "No data found for any of the specified metrics to plot.")
        plt.close(fig_metrics)
    except FileNotFoundError:
        log(WARNING, "Results file not found: %s", results_pkl_path)
    except pickle.UnpicklingError:
        log(WARNING, "Error unpickling results file: %s", results_pkl_path)
    except Exception as e:
        log(WARNING, "Could not generate learning curve plots: %s", e)
def plot_prediction_probability_distributions(y_true, y_pred_proba, class_names, output_dir, bins=50):
    """Generates and saves histograms of prediction probabilities for each true class.
    For each class, this plot shows the distribution of the predicted probabilities 
    assigned to that class, for samples that actually belong to that class.
    High probabilities bunched towards 1.0 are desirable.
    Args:
        y_true (np.array): Array of true labels (integers).
        y_pred_proba (np.array): Array of predicted probabilities, shape (n_samples, n_classes).
        class_names (list): List of class names (strings).
        output_dir (str): Directory to save the plot.
        bins (int): Number of bins for the histograms.
    """
    try:
        n_classes = len(class_names)
        if y_pred_proba.shape[1] != n_classes:
            log(WARNING, "Number of classes in y_pred_proba does not match len(class_names).")
            return
        # Determine the number of rows and columns for subplots
        n_cols = 3 
        n_rows = (n_classes + n_cols - 1) // n_cols # Ceiling division
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=True, sharey=False)
        axes = axes.flatten() # Flatten to easily iterate regardless of shape
        for i in range(n_classes):
            ax = axes[i]
            # Get probabilities for the current class where the true label is this class
            true_class_indices = np.where(y_true == i)[0]
            if len(true_class_indices) == 0:
                ax.text(0.5, 0.5, "No true samples for this class", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{class_names[i]} (No true samples)')
                ax.set_xlabel('Predicted Probability')
                ax.set_ylabel('Frequency')
                continue
            class_probabilities = y_pred_proba[true_class_indices, i]
            ax.hist(class_probabilities, bins=bins, range=(0,1), edgecolor='black', alpha=0.7)
            ax.set_title(f'Class: {class_names[i]}')
            ax.set_xlabel('Predicted Probability for this Class')
            ax.set_ylabel('Frequency')
            ax.grid(True, axis='y', linestyle='--', alpha=0.7)
            mean_proba = np.mean(class_probabilities)
            ax.axvline(mean_proba, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_proba:.2f}')
            ax.legend(fontsize='small')
        # Hide any unused subplots
        for j in range(n_classes, n_rows * n_cols):
            fig.delaxes(axes[j])
        fig.suptitle('Distribution of Predicted Probabilities for True Classes', fontsize=16, y=1.02)
        fig.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for suptitle
        plot_path = os.path.join(output_dir, "prediction_probability_distributions.png")
        fig.savefig(plot_path)
        plt.close(fig)
        log(INFO, "Prediction probability distribution plot saved to %s", plot_path)
    except Exception as e:
        log(WARNING, "Could not generate prediction probability distribution plot: %s", e)
</file>

</files>
