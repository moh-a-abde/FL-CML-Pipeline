# @package _global_
# Random Forest experiment configuration
# This config demonstrates Random Forest model usage in federated learning

# Override model configuration for Random Forest
model:
  type: "random_forest"
  # Define complete Random Forest parameters (will override XGBoost params)
  params:
    # Core Random Forest parameters
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: "sqrt"
    criterion: "gini"
    bootstrap: true
    oob_score: false
    n_jobs: -1
    random_state: 42
    class_weight: "balanced"
    
    # Advanced parameters for fine-tuning
    max_samples: null
    min_weight_fraction_leaf: 0.0
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    warm_start: false

# Federated learning configuration optimized for Random Forest
federated:
  pool_size: 5
  num_rounds: 10
  train_method: "bagging"  # Random Forest works well with bagging
  fraction_fit: 0.8
  fraction_evaluate: 0.5

# Hyperparameter tuning for Random Forest
tuning:
  enabled: false
  num_samples: 5
  cpus_per_trial: 2
  max_concurrent_trials: 2
  output_dir: "./tune_results_rf"
  
  # Ray Tune scheduler optimized for Random Forest
  scheduler:
    type: "ASHA"
    max_t: 200
    grace_period: 20
    reduction_factor: 3

# Data configuration overrides for Random Forest
data:
  # Random Forest works well with the default data settings
  # No specific overrides needed for data configuration
  train_test_split: 0.8  # Keep standard split

# Pipeline execution settings (only override specific early stopping settings)
early_stopping:
  enabled: true
  patience: 5
  min_delta: 0.001

# Output configuration (override experiment name)
outputs:
  experiment_name: "random_forest_federated" 