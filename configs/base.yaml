# FL-CML-Pipeline Base Configuration
# This file consolidates all configuration options from legacy argument parsers

defaults:
  - _self_
  # No default experiment - will be specified by ConfigManager

# Data configuration
data:
  path: ./data/received  # Changed from ${hydra:runtime.cwd}/data/received to avoid interpolation issues
  filename: final_dataset.csv
  train_test_split: 0.8
  stratified: true
  temporal_window_size: 1000
  seed: 42

# Model configuration (XGBoost parameters from BST_PARAMS)
model:
  type: xgboost
  num_local_rounds: 702
  params:
    objective: "multi:softprob"
    num_class: 11
    eta: 0.020810960704017334
    max_depth: 18
    min_child_weight: 5
    gamma: 0.9251758546112311
    subsample: 0.9756753850290512
    colsample_bytree: 0.38159969563216367
    colsample_bylevel: 0.9005982104516306
    colsample_bynode: 0.6725376242659837
    nthread: 16
    tree_method: "hist"
    eval_metric: ["mlogloss", "merror"]
    max_delta_step: 5
    reg_alpha: 2.2769018964159371e-10
    reg_lambda: 2.643013384540637e-07
    base_score: 0.5
    scale_pos_weight: 0.7776869643957478
    grow_policy: "lossguide"
    max_leaves: 3065
    normalize_type: "tree"
    random_state: 42

# Federated learning configuration
federated:
  train_method: "bagging"  # choices: ["bagging", "cyclic"]
  
  # Server configuration
  pool_size: 5
  num_rounds: 5
  num_clients_per_round: 5
  num_evaluate_clients: 5
  centralised_eval: true
  
  # Client configuration
  num_partitions: 10
  partitioner_type: "uniform"  # choices: ["uniform", "linear", "square", "exponential"]
  test_fraction: 0.2
  scaled_lr: false
  
  # Simulation specific
  num_cpus_per_client: 2

# Hyperparameter tuning configuration
tuning:
  enabled: false
  num_samples: 5
  cpus_per_trial: 2
  max_concurrent_trials: 4
  output_dir: "./tune_results"
  
  # Ray Tune scheduler
  scheduler:
    type: "ASHA"
    max_t: 500
    grace_period: 50
    reduction_factor: 3

# Pipeline configuration
pipeline:
  steps:
    - create_global_processor
    - hyperparameter_tuning  # optional based on tuning.enabled
    - generate_tuned_params
    - federated_learning
  
  # Step-specific configurations
  global_processor:
    force_recreate: true
    output_dir: "outputs"
  
  preprocessing:
    consistent_across_phases: true
    global_processor_path: null  # Will be set dynamically

# Logging configuration
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: ./logs/fl-cml-pipeline.log  # Changed from ${hydra:runtime.cwd}/logs/${now:%Y-%m-%d_%H-%M-%S}.log

# Output configuration  
outputs:
  base_dir: ./outputs  # Changed from ${hydra:runtime.cwd}/outputs
  create_timestamped_dirs: true
  save_results_pickle: true
  save_model: true
  generate_visualizations: true

# Early stopping configuration
early_stopping:
  enabled: true
  patience: 3
  min_delta: 0.001 