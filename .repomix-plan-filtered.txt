This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: client_utils.py, utils.py, server.py, server_utils.py, client.py, sim.py, dataset.py, multi_class_implementation_plan.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
client_utils.py
client.py
dataset.py
multi_class_implementation_plan.md
server_utils.py
server.py
sim.py
utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
# Default XGBoost parameters for multi-class classification
BST_PARAMS = {
    'objective': 'multi:softmax',  # Changed to multi-class classification
    'num_class': 3,  # Three classes: benign, dns_tunneling, icmp_tunneling
    'eval_metric': ['mlogloss', 'merror'],  # Multi-class metrics
    'learning_rate': 0.1,
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': [2.0, 1.0, 1.0]  # Weights for each class
}
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            params (dict): XGBoost parameters (defaults to BST_PARAMS if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.params = params if params is not None else BST_PARAMS.copy()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        y_train = self.train_dmatrix.get_label()
        class_counts = np.bincount(y_train.astype(int))
        # Log class distribution for all three classes
        class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        for i, count in enumerate(class_counts):
            class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch
            bst = xgb.train(
                self.params,
                self.train_dmatrix,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (self.train_dmatrix, "train")],
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        y_pred_proba = bst.predict(self.valid_dmatrix, output_margin=True)  # Get raw predictions for mlogloss
        y_pred_labels = bst.predict(self.valid_dmatrix)  # Get class predictions
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        for i, count in enumerate(true_counts):
            class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics
        precision = precision_score(y_true, y_pred_labels, average='weighted')
        recall = recall_score(y_true, y_pred_labels, average='weighted')
        f1 = f1_score(y_true, y_pred_labels, average='weighted')
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss manually
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_pred_proba_softmax = np.exp(y_pred_proba) / np.sum(np.exp(y_pred_proba), axis=1, keepdims=True)
        y_true_one_hot = np.zeros_like(y_pred_proba_softmax)
        y_true_one_hot[np.arange(len(y_true)), y_true.astype(int)] = 1
        mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_softmax + epsilon), axis=1))
        # Compute confusion matrix
        conf_matrix = confusion_matrix(y_true, y_pred_labels)
        # Generate detailed classification report
        class_report = classification_report(y_true, y_pred_labels, target_names=class_names)
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix:\n{conf_matrix}")
        log(INFO, f"Classification Report:\n{class_report}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from server_utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss),
            # Store confusion matrix elements as individual metrics
            "conf_00": int(conf_matrix[0][0]),  # benign correct
            "conf_01": int(conf_matrix[0][1]),  # benign misclassified as dns
            "conf_02": int(conf_matrix[0][2]),  # benign misclassified as icmp
            "conf_10": int(conf_matrix[1][0]),  # dns misclassified as benign
            "conf_11": int(conf_matrix[1][1]),  # dns correct
            "conf_12": int(conf_matrix[1][2]),  # dns misclassified as icmp
            "conf_20": int(conf_matrix[2][0]),  # icmp misclassified as benign
            "conf_21": int(conf_matrix[2][1]),  # icmp misclassified as dns
            "conf_22": int(conf_matrix[2][2]),  # icmp correct
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data
)
from utils import client_args_parser, BST_PARAMS, NUM_LOCAL_ROUND
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    #latest_csv_path = get_latest_csv(data_directory)
    #labeled_dataset = load_csv_data(latest_csv_path)
    #unlabeled_dataset = load_csv_data(latest_csv_path)
    # Load labeled data for training
    labeled_csv_path = "data/received/network_train_60.csv"
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction
    unlabeled_csv_path = "data/received/network_test_40_nolabel.csv"
    unlabeled_dataset = load_csv_data(unlabeled_csv_path)
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    # The ExponentialPartitioner doesn't have get_indices, but provides partition method
    # which returns the partition directly rather than just indices
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except AttributeError:
        # If that fails, try using the partition method (used in newer versions)
        try:
            # Newer versions use a different API
            train_partition = partitioner.partition(full_train_data)[args.partition_id]
        except (AttributeError, TypeError, IndexError):
            # As a fallback, if all partition methods fail, use a simple numerical partition
            # by getting evenly spaced indices based on partition ID
            total_samples = len(full_train_data)
            samples_per_partition = total_samples // args.num_partitions
            start_idx = args.partition_id * samples_per_partition
            end_idx = start_idx + samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
            partition_indices = list(range(start_idx, end_idx))
            train_partition = full_train_data.select(partition_indices)
            log(INFO, "Used fallback partitioning. Partition %d: samples %d to %d", 
                args.partition_id, start_idx, end_idx)
    log(INFO, "Partition size: %d samples (out of %d total)", 
        len(train_partition), len(full_train_data))
    # Handle data splitting based on evaluation strategy
    if args.centralised_eval:
        # Use centralized test set for evaluation
        train_data = train_partition
        valid_data = labeled_dataset["test"]
        valid_data.set_format("numpy")
        num_train = train_data.shape[0]
        num_val = valid_data.shape[0]
    else:
        # Perform local train/test split using the updated function
        # This now returns the fitted processor as well
        log(INFO, "Performing local train/test split...")
        # Generate a unique seed for each client based on partition_id to ensure
        # different clients get different train/test splits
        client_specific_seed = args.seed + (args.partition_id * 1000)
        log(INFO, "Using client-specific random seed for train/test split: %d", client_specific_seed)
        train_dmatrix, valid_dmatrix, processor = train_test_split( # Capture processor
            train_partition,
            test_fraction=args.test_fraction,
            random_state=client_specific_seed  # Use client-specific seed
        )
        # Get counts from the DMatrix objects
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
        log(INFO, "Local split: %d train samples, %d validation samples", num_train, num_val)
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self):
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        # Define feature groups
        self.categorical_features = [
            'id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history', 
            'validation_status', 'method', 'status_msg', 'is_orig',
            'local_orig', 'local_resp'
        ]
        self.numerical_features = [
            'id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
            'missed_bytes', 'orig_pkts',
            'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'ts_delta',
            'rtt', 'acks', 'percent_lost', 'request_body_len',
            'response_body_len', 'seen_bytes', 'missing_bytes', 'overflow_bytes'
        ]
        # Removed object_columns as they may cause data leakage
        log(INFO, "Note: Removed object_columns (uid, client_initial_dcid, server_scid) to prevent potential data leakage")
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df.columns:
                unique_values = df[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df[df[col] == val]
                        if 'label' in df.columns and len(subset) > 0:
                            most_common_label = subset['label'].value_counts().idxmax()
                            label_pct = subset['label'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics
        for col in self.numerical_features:
            if col in df.columns:
                self.numerical_stats[col] = {
                    'mean': df[col].mean(),
                    'std': df[col].std(),
                    'median': df[col].median(),
                    'q99': df[col].quantile(0.99)
                }
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            raise ValueError("FeatureProcessor must be fitted before transform")
        df = df.copy()
        # Drop object columns since they might be causing data leakage
        object_columns_to_remove = ['uid', 'client_initial_dcid', 'server_scid']
        columns_dropped = []
        for col in object_columns_to_remove:
            if col in df.columns:
                df.drop(columns=[col], inplace=True)
                columns_dropped.append(col)
        if columns_dropped and not is_training:
            log(INFO, "Dropped potential leakage columns: %s", columns_dropped)
        # Transform categorical features
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with added noise for validation data
        for col in self.numerical_features:
            if col in df.columns and col in self.numerical_stats:
                # Replace infinities
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                # Add noise to all numerical features for validation data
                # This ensures validation is a truly independent test
                if not is_training:
                    # Add noise proportional to the standard deviation of each feature
                    std = self.numerical_stats[col]['std']
                    # If std is 0 or NaN, use a small default value
                    noise_scale = max(std, 0.1) * 0.05  # 5% of standard deviation
                    noise = np.random.normal(0, noise_scale, size=df.shape[0])
                    df[col] = df[col] + noise
                # Cap outliers using 99th percentile
                q99 = self.numerical_stats[col]['q99']
                df.loc[df[col] > q99, col] = q99  # Re-enabled outlier capping
                # Fill NaN with median plus small noise
                median = self.numerical_stats[col]['median']
                # Find NaN positions
                nan_mask = df[col].isna()
                # First fill with median value
                df[col] = df[col].fillna(median)
                # Then add noise only to the previously NaN positions if not training
                if not is_training and nan_mask.any():
                    # Add a tiny bit of noise to medians for previously NaN values
                    noise_scale = median * 0.01 if median != 0 else 0.001
                    noise = np.random.normal(0, noise_scale, size=nan_mask.sum())
                    df.loc[nan_mask, col] += noise
        return df
def preprocess_data(data: pd.DataFrame, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification with three classes: benign (0), dns_tunneling (1), and icmp_tunneling (2).
    Args:
        data (pd.DataFrame): Input DataFrame
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    if processor is None:
        processor = FeatureProcessor()
    # Process features
    df = processor.transform(data, is_training)
    # Handle labels
    if 'label' in df.columns:
        features = df.drop(columns=['label'])
        labels = df['label'].astype(int)
        # Validate labels
        unique_labels = labels.unique()
        if not all(label in [0, 1, 2] for label in unique_labels):
            print(f"Warning: Unexpected label values found: {unique_labels}")
            labels = labels.map(lambda x: x if x in [0, 1, 2] else -1)
        return features, labels
    return df, None
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, create a proper 80/20 split to avoid data leakage
        # Use a specific random seed for reproducibility
        train_test_dict = dataset.train_test_split(test_size=0.2, seed=42)
        return DatasetDict({
            "train": train_test_dict["train"],
            "test": train_test_dict["test"]
        })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def preprocess_data_deprec2(data):
    """/
    Preprocess the data by encoding categorical features and separating features and labels.
    Args:
        data (pd.DataFrame): Input DataFrame
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Define categorical and numerical features
    categorical_features = ['id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history']
    numerical_features = ['id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
                         'local_orig', 'local_resp', 'missed_bytes', 'orig_pkts', 
                         'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Convert categorical features to category type
    for col in categorical_features:
        df[col] = df[col].astype('category')
        # Get numerical codes for categories
        df[col] = df[col].cat.codes
    # Ensure numerical features are float type
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Check if this is labeled or unlabeled data
    if 'label' in df.columns:
        # For labeled data
        features = df.drop(columns=['label'])
        labels = df['label'].astype(float)
        return features, labels
    # For unlabeled data
    return df, None
def preprocess_data_deprec(data):
    """
    Preprocess the static_data.csv dataset by:
      - Dropping the 'Timestamp' column.
      - Converting 'Dst Port' and 'Protocol' to categorical features.
      - Converting remaining features (except 'Label') to numerical (float).
      - Separating features and target (Label), and encoding the target if necessary.
    Args:
        filepath (str): Path to the static_data.csv file.
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Drop 'Timestamp' as it is not used for training directly
    if 'Timestamp' in df.columns:
        df.drop(columns=['Timestamp'], inplace=True)
    # Define which columns to treat as categorical based on domain knowledge
    categorical_features = []
    if 'Dst Port' in df.columns:
        categorical_features.append('Dst Port')
    if 'Protocol' in df.columns:
        categorical_features.append('Protocol')
    # Convert categorical features to type 'category' and then to numerical codes
    for col in categorical_features:
        df[col] = df[col].astype('category').cat.codes
    # The numerical features are all columns except the ones we have categorized or the target
    numerical_features = [col for col in df.columns if col not in categorical_features + ['Label']]
    # Convert these numerical features to float
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Replace inf with NaN and cap large values
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    for col in numerical_features:
        max_val = 1e15  # Example max value, adjust as needed
        df[col] = np.where(df[col] > max_val, np.nan, df[col])
    # Process the target variable if present
    if 'Label' in df.columns:
        # If the label column is non-numeric (object), encode it as categorical codes.
        if df['Label'].dtype == object:
            labels = df['Label'].astype('category').cat.codes
        else:
            labels = df['Label']
        features = df.drop(columns=['Label'])
        return features, labels
    # If no label column, return the processed DataFrame and None for labels
    return df, None
def separate_xy(data):
    """
    Separate features and labels from the dataset.
    Args:
        data: Input dataset
    Returns:
        tuple: (features, labels or None if unlabeled)
    """
    return preprocess_data(data.to_pandas())
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # The input 'data' should already be a pandas DataFrame in this context
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # Handle case where preprocess_data might return None for labels (e.g., unlabeled data)
    if y is None:
        log(INFO, "No labels found in data. This appears to be unlabeled data for prediction only.")
        return xgb.DMatrix(x, missing=np.nan)
    # For validation data, log label distribution to help identify issues
    if not is_training:
        # Count occurrences of each label
        label_counts = np.bincount(y.astype(int))
        label_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        log(INFO, "Label distribution in validation data:")
        for i, count in enumerate(label_counts):
            class_name = label_names[i] if i < len(label_names) else f'unknown_{i}'
            log(INFO, f"  {class_name}: {count}")
    return xgb.DMatrix(x, label=y, missing=np.nan)
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into train and test sets, preprocess, and return DMatrices and the fitted processor.
    Args:
        data: Input dataset (Hugging Face Dataset or pandas DataFrame)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: 
            - Training DMatrix
            - Test DMatrix
            - Fitted FeatureProcessor instance
    """
    # Convert to pandas if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use sklearn's train_test_split with shuffle=True to ensure data is properly randomized
    log(INFO, "Original data shape before splitting: %s", data.shape)
    # Add multiple random noise features to make the problem harder
    np.random.seed(random_state)
    # Add 3 noise columns with different distributions and scales
    data['random_noise1'] = np.random.normal(0, 0.5, size=data.shape[0])  # Gaussian noise (higher variance)
    data['random_noise2'] = np.random.uniform(-1, 1, size=data.shape[0])  # Uniform noise
    data['random_noise3'] = np.random.exponential(0.5, size=data.shape[0])  # Exponential noise
    # Check if 'label' column exists in data
    if 'label' not in data.columns:
        log(INFO, "Warning: No 'label' column found in data. Available columns: %s", data.columns.tolist())
    else:
        # Report class distribution
        label_counts = data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in original data: %s", label_counts)
    # Check for data leakage indicators
    if 'uid' in data.columns:
        uid_label_counts = data.groupby('uid')['label'].value_counts()
        uid_with_multiple_labels = uid_label_counts.index.get_level_values(0).duplicated(keep=False)
        if not any(uid_with_multiple_labels):
            log(WARNING, "CRITICAL: Each UID has only one label, indicating potential perfect data leakage through UIDs")
    # Generate a completely different random_state for validation split
    validation_random_state = (random_state * 17 + 3) % 10000
    log(INFO, "Using different random states for train/validation split: %d/%d", 
        random_state, validation_random_state)
    # Split data ensuring complete partition separation
    if 'uid' in data.columns:
        # If we have UIDs, use them to ensure no data leakage across train/test
        log(INFO, "Using UID-based splitting to ensure no data leakage")
        unique_uids = data['uid'].unique()
        np.random.seed(validation_random_state)
        np.random.shuffle(unique_uids)
        test_size = int(len(unique_uids) * test_fraction)
        test_uids = unique_uids[:test_size]
        train_uids = unique_uids[test_size:]
        # Split based on UIDs
        train_data = data[data['uid'].isin(train_uids)].copy()
        test_data = data[data['uid'].isin(test_uids)].copy()
        log(INFO, "Split by UIDs: %d train UIDs, %d test UIDs", len(train_uids), len(test_uids))
    else:
        # If no UIDs, use standard stratified split
        train_data, test_data = train_test_split_pandas(
            data,
            test_size=test_fraction,
            random_state=validation_random_state,
            shuffle=True,  # Ensure data is shuffled for a proper split
            stratify=data['label'] if 'label' in data.columns else None  # Use stratified split if possible
        )
    # Log the shapes to verify they're different sets
    log(INFO, "Train data shape: %s, Test data shape: %s", train_data.shape, test_data.shape)
    # Verify label distributions to ensure proper stratification
    if 'label' in data.columns:
        train_label_counts = train_data['label'].value_counts().to_dict()
        test_label_counts = test_data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in train data: %s", train_label_counts)
        log(INFO, "Class distribution in test data: %s", test_label_counts)
    # Check for unique values in both sets to verify they're actually different
    if 'uid' in data.columns:
        train_uids = set(train_data['uid'].unique())
        test_uids = set(test_data['uid'].unique())
        common_uids = train_uids.intersection(test_uids)
        if common_uids:
            log(WARNING, "WARNING: Found %d UIDs in both train and test sets! This indicates data leakage.", 
                len(common_uids))
        else:
            log(INFO, "Good: Train and test sets have completely separate UIDs (no overlap).")
    # Add dataset-specific noise to test set to make it more challenging
    # This helps prevent the model from simply memorizing patterns
    for col in train_data.columns:
        if col.startswith('id.') or col in ['proto', 'conn_state', 'duration', 'bytes']:
            continue  # Skip these columns
        if pd.api.types.is_numeric_dtype(train_data[col]):
            # Add noise only to numerical columns in test set
            col_std = test_data[col].std()
            if col_std > 0 and not pd.isna(col_std):
                noise_scale = col_std * 0.1  # 10% of standard deviation
                test_data[col] = test_data[col] + np.random.normal(0, noise_scale, size=test_data.shape[0])
                log(INFO, "Added noise to test column: %s", col)
    # Initialize feature processor
    processor = FeatureProcessor()
    # Fit processor on training data and transform both sets
    # Note: transform calls fit implicitly if is_training=True and not fitted
    train_dmatrix = transform_dataset_to_dmatrix(train_data, processor=processor, is_training=True)
    test_dmatrix = transform_dataset_to_dmatrix(test_data, processor=processor, is_training=False)
    # Log number of examples for verification
    log(INFO, "Train DMatrix has %d rows, Test DMatrix has %d rows", 
        train_dmatrix.num_row(), test_dmatrix.num_row())
    return train_dmatrix, test_dmatrix, processor # Return the fitted processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
class ModelPredictor:
    """
    Handles model prediction and dataset labeling
    """
    def __init__(self, model_path: str):
        self.model = xgb.Booster()
        self.model.load_model(model_path)
    def predict_and_save(
        self,
        input_data: Union[str, pd.DataFrame],
        output_path: str,
        include_confidence: bool = True
    ):
        """
        Predict on new data and save labeled dataset
        """
        # Load/preprocess input data
        data = self._prepare_data(input_data)
        # Generate predictions
        predictions = self.model.predict(data)
        confidence = None
        if include_confidence:
            confidence = self.model.predict(data, output_margin=True)
        # Save labeled dataset
        self._save_output(data, predictions, confidence, output_path)
</file>

<file path="multi_class_implementation_plan.md">
# Multi-Class Classification Implementation Plan

## Current State
- Binary classification (benign vs malicious) using XGBoost
- Features include both categorical and numerical network traffic data
- Using federated learning with Flower framework
- Need to expand to classify specific types of malicious traffic

## Implementation Steps

### 1. Data Preprocessing Modifications (`dataset.py`)
- [ ] Update `preprocess_data()` function:
  ```python
  def preprocess_data(data):
      # ... existing code ...
      if 'label' in df.columns:
          features = df.drop(columns=['label'])
          
          # New label mapping for three classes
          label_mapping = {
              'benign': 0, 
              'dns_tunneling': 1, 
              'icmp_tunneling': 2
          }
          labels_series = df['label'].map(label_mapping)
          
          # Handle unmapped labels
          if labels_series.isnull().any():
              unmapped_labels = df['label'][labels_series.isnull()].unique()
              print(f"Warning: Unmapped labels found: {unmapped_labels}")
              labels_series = labels_series.fillna(-1)
          
          labels = labels_series.astype(int)
          return features, labels
      else:
          return df, None
  ```

### 2. Model Configuration Updates (`client_utils.py`)
- [ ] Update XGBoost parameters in `utils.py`:
  ```python
  BST_PARAMS = {
      'objective': 'multi:softmax',
      'num_class': 3,
      'eval_metric': ['mlogloss', 'merror'],
      'learning_rate': 0.1,
      'max_depth': 6,
      'min_child_weight': 1,
      'subsample': 0.8,
      'colsample_bytree': 0.8,
      'scale_pos_weight': [1.0, 2.0, 2.0]  # Adjust based on class distribution
  }
  ```

- [ ] Modify `evaluate()` method in `XgbClient` class:
  ```python
  def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
      # ... existing model loading code ...
      
      # Generate predictions - No thresholding needed for multi-class
      y_pred_proba = bst.predict(self.valid_dmatrix)
      y_pred_labels = np.argmax(y_pred_proba, axis=1)
      
      # Get ground truth labels
      y_true = self.valid_dmatrix.get_label()
      
      # Compute multi-class metrics
      precision = precision_score(y_true, y_pred_labels, average='weighted')
      recall = recall_score(y_true, y_pred_labels, average='weighted')
      f1 = f1_score(y_true, y_pred_labels, average='weighted')
      accuracy = accuracy_score(y_true, y_pred_labels)
      
      # Multi-class loss calculation
      epsilon = 1e-10
      log_likelihood = -np.log(y_pred_proba[np.arange(len(y_true)), y_true.astype(int)] + epsilon)
      loss = np.mean(log_likelihood)
      
      # Multi-class confusion matrix
      conf_matrix = confusion_matrix(y_true, y_pred_labels)
      
      metrics = {
          "precision": float(precision),
          "recall": float(recall),
          "f1": float(f1),
          "accuracy": float(accuracy),
          "loss": float(loss),
          "confusion_matrix": conf_matrix.tolist(),
          "num_predictions": self.num_val
      }
      
      return metrics
  ```

### 3. Server-Side Updates (`server_utils.py`)
- [ ] Update metrics aggregation:
  ```python
  def evaluate_metrics_aggregation(eval_metrics):
      total_num = sum([num for num, _ in eval_metrics])
      
      # Aggregate evaluation metrics
      metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy', 'loss']
      aggregated_metrics = {}
      
      for metric in metrics_to_aggregate:
          if all(metric in metrics for _, metrics in eval_metrics):
              aggregated_metrics[metric] = sum([metrics[metric] * num for num, metrics in eval_metrics]) / total_num
          else:
              aggregated_metrics[metric] = 0.0
      
      # Aggregate confusion matrix
      aggregated_conf_matrix = None
      for num, metrics in eval_metrics:
          conf_matrix = np.array(metrics.get("confusion_matrix", [[0, 0, 0], [0, 0, 0], [0, 0, 0]]))
          if aggregated_conf_matrix is None:
              aggregated_conf_matrix = conf_matrix
          else:
              aggregated_conf_matrix += conf_matrix
      
      aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix.tolist()
      aggregated_metrics["prediction_mode"] = eval_metrics[0][1].get("prediction_mode", False)
      
      return aggregated_metrics["loss"], aggregated_metrics
  ```

- [ ] Update prediction saving:
  ```python
  def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None):
      predictions_dict = {
          'predicted_label': predictions,
          'prediction_type': [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling') 
              for p in predictions
          ]
      }
      # ... rest of the function
  ```

### 4. Prediction Updates (`use_saved_model.py`)
- [ ] Update prediction saving:
  ```python
  def save_detailed_predictions(predictions, output_path):
      results_df = pd.DataFrame()
      
      if predictions.ndim > 1 and predictions.shape[1] > 1:
          results_df['raw_probabilities'] = predictions.tolist()
          predicted_labels = np.argmax(predictions, axis=1)
          results_df['predicted_label'] = predicted_labels
          
          results_df['prediction_type'] = [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling')
              for p in predicted_labels
          ]
          
          results_df['prediction_score'] = predictions[
              np.arange(len(predicted_labels)), 
              predicted_labels
          ]
      
      results_df.to_csv(output_path, index=False)
      return results_df
  ```

- [ ] Update main evaluation:
  ```python
  def main():
      # ... existing code ...
      if args.has_labels:
          y_pred_labels = np.argmax(raw_predictions, axis=1)
          accuracy = accuracy_score(y_true, y_pred_labels)
          
          cm = confusion_matrix(y_true, y_pred_labels)
          report = classification_report(y_true, y_pred_labels)
          
          log(INFO, f"Accuracy: {accuracy:.4f}")
          log(INFO, f"Confusion Matrix:\n{cm}")
          log(INFO, f"Classification Report:\n{report}")
  ```

## Testing Strategy

### 1. Unit Tests
- [ ] Test label mapping in `dataset.py`
- [ ] Test multi-class metrics calculation
- [ ] Test prediction format and class probabilities
- [ ] Test confusion matrix calculation

### 2. Integration Tests
- [ ] Test full training pipeline with three classes
- [ ] Test federated learning convergence
- [ ] Test model saving and loading
- [ ] Test prediction pipeline

### 3. System Tests
- [ ] End-to-end training and evaluation
- [ ] Performance testing with large datasets
- [ ] Class imbalance handling
- [ ] Error handling and edge cases

## Success Criteria
1. Model successfully trains on three-class data
2. All evaluation metrics properly calculated and reported
3. Predictions include class probabilities
4. Federated learning pipeline works with multiple classes
5. High accuracy in distinguishing between benign and malicious traffic
6. Accurate classification of DNS vs ICMP tunneling attacks
7. Documentation is complete and accurate
8. Test cases pass successfully

## Potential Challenges and Mitigations
1. Class imbalance
   - Solution: Use class weights in model parameters
   - Monitor class distribution in training data
   - Consider data augmentation for underrepresented classes

2. Model complexity
   - Solution: Start with simpler model and gradually increase complexity
   - Monitor training metrics for overfitting
   - Use cross-validation for parameter tuning

3. Federated learning convergence
   - Solution: Adjust learning rate and number of rounds
   - Monitor loss curves across clients
   - Consider using FedAvg with momentum

4. Performance impact
   - Solution: Profile code for bottlenecks
   - Optimize prediction pipeline
   - Consider batch processing for large datasets
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from logging import INFO
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    aggregated_metrics["loss"] = loss  # Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Also store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["loss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'benign', 'dns_tunneling', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predictions,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Default mapping for multi-class predictions
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        predictions_dict['prediction_type'] = [label_mapping.get(int(p), 'unknown') for p in predictions]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions: %s", raw_predictions)
    # Log distribution of scores
    log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
        np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # Convert raw predictions to probabilities if necessary
    # (Assuming a binary classification with a threshold of 0.5)
    probabilities = 1 / (1 + np.exp(-raw_predictions))  # Example for sigmoid transformation
    predicted_labels = (probabilities >= 0.5).astype(int)
    # Log predicted class distribution
    unique, counts = np.unique(predicted_labels, return_counts=True)
    log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Save predictions to CSV
    predictions_df = pd.DataFrame({
        'predicted_label': predicted_labels,
        'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
        'prediction_score': probabilities
    })
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_fn(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Predict on test data
            y_pred = bst.predict(test_data)
            y_pred_labels = y_pred.astype(int)
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_labels, server_round, "results", y_true)
            # Compute metrics
            precision = precision_score(y_true, y_pred_labels, average='weighted')
            recall = recall_score(y_true, y_pred_labels, average='weighted')
            f1 = f1_score(y_true, y_pred_labels, average='weighted')
            # Generate confusion matrix
            conf_matrix = confusion_matrix(y_true, y_pred_labels)
            # Create metrics dictionary
            metrics = {
                "precision": float(precision),
                "recall": float(recall),
                "f1": float(f1),
                "true_negatives": int(conf_matrix[0][0]),
                "false_positives": int(conf_matrix[0][1]),
                "false_negatives": int(conf_matrix[1][0]),
                "true_positives": int(conf_matrix[1][1]),
                "predictions_file": output_path
            }
            log(INFO, f"Precision = {precision}, Recall = {recall}, F1 Score = {f1} at round {server_round}")
            log(INFO, f"Dataset with predictions saved to: {output_path}")
            return 0, metrics
    return evaluate_fn
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    test_set = load_csv_data("data/static_data.csv")["test"]
    test_set.set_format("pandas")
    test_dmatrix = transform_dataset_to_dmatrix(test_set)
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = FedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["loss"] = history.losses_distributed
else:
    results["loss"] = []
    log(INFO, "No distributed losses found in history")
# Add metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics"] = history.metrics_distributed
else:
    results["metrics"] = {}
    log(INFO, "No distributed metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    NUM_LOCAL_ROUND,
    BST_PARAMS,
)
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 1
BST_PARAMS = {
    "objective": "multi:softmax",
    "num_class": 3,
    "eta": 0.05,  # Reduced learning rate to prevent overfitting
    "max_depth": 3,  # Reduced max_depth to prevent memorization
    "min_child_weight": 10,  # Increased to prevent fitting to small samples
    "gamma": 1.0,  # Increased minimum loss reduction for split
    "subsample": 0.7,  # Sample fewer rows per iteration
    "colsample_bytree": 0.6,  # Sample fewer features per tree
    "colsample_bylevel": 0.6,  # Sample fewer features per level
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 5,
    "reg_alpha": 2.0,  # Increased L1 regularization
    "reg_lambda": 5.0,  # Increased L2 regularization
    "base_score": 0.5,  # Neutral starting point
    "scale_pos_weight": 1.0,  # No specific class weight
    "grow_policy": "lossguide",  # Alternative tree growing policy
    "normalize_type": "tree",  # Helps with interpretability
    "random_state": 42  # Fixed seed for reproducibility
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

</files>
