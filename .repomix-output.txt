This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    cml.yaml
diagrams/
  client_operations.mmd
  data_handling.mmd
  overall_workflow.mmd
  server_operations.mmd
.gitattributes
.gitignore
.repomixignore
client_utils.py
client.py
commit.sh
create_global_processor.py
dataset.py
go_to_work.sh
multi_class_implementation_plan.md
original_bst_params.json
PREPROCESSING_CONSISTENCY_FIX.md
pretrained_model_usage.md
progress.md
pyproject.toml
ray_tune_README.md
ray_tune_xgboost_updated.py
ray_tune_xgboost.py
README.md
requirements.txt
run_bagging copy.sh
run_bagging.sh
run_cyclic.sh
run_ray_tune.sh
run.py
server_utils.py
server.py
sim.py
tuned_params.py
update_ray_tune_xgboost.py
use_saved_model.py
use_tuned_params.py
utils.py
visualization_utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/cml.yaml">
name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
        with:
          lfs: true
      # Ensure LFS objects are properly pulled
      - name: Pull LFS objects
        run: |
          git lfs install
          git lfs pull
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: 'pip'
      - uses: iterative/setup-cml@v1
      # Cache conda/mamba environments
      - name: Cache conda environment
        uses: actions/cache@v3
        with:
          path: |
            ~/.conda
            ~/miniconda3
            ./venv
          key: ${{ runner.os }}-conda-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-conda-
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          # Check if venv exists before creating
          if [ ! -d "venv" ]; then
            echo "Setting up virtual environment for the first time"
            pip install virtualenv
            virtualenv venv
          fi
          source venv/bin/activate
          # Check if mamba is installed
          if ! command -v mamba &> /dev/null; then
            echo "Installing mamba"
            pip install mamba
            mamba init
            source ~/.bashrc
            mamba create
            mamba activate
          fi
          # Install dependencies
          pip install -r requirements.txt
          # Check if main packages are installed to avoid reinstalling them
          if ! python -c "import xgboost" &> /dev/null; then
            pip install xgboost
          fi
          if ! python -c "import flwr" &> /dev/null; then
            pip install -U flwr["simulation"]
          fi
          if ! python -c "import ray" &> /dev/null; then
            pip install -U "ray[all]"
          fi
          if ! python -c "import torch" &> /dev/null; then
            pip install torch torchvision torchaudio
          fi
          if ! python -c "import hydra" &> /dev/null; then
            pip install hydra-core
          fi
          if ! python -c "import imblearn" &> /dev/null; then
            pip install imbalanced-learn
          fi
      - name: Install hyperopt
        run: |
          source venv/bin/activate
          pip install hyperopt
      - name: Run Ray Tune hyperparameter optimization
        run: |
          source venv/bin/activate
          # Check if data directory exists, create if not
          mkdir -p data/sample
          # Use the same dataset that federated learning will use for consistency
          FINAL_DATASET="data/received/final_dataset.csv"
          # Check if the final dataset exists
          if [ -f "$FINAL_DATASET" ]; then
            echo "Using final dataset for hyperparameter tuning: $FINAL_DATASET"
            # Create output directory for Ray Tune results
            mkdir -p tune_results
            # Run Ray Tune with the final dataset for consistency
            bash run_ray_tune.sh --data-file "$FINAL_DATASET" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
          else
            echo "Final dataset not found at $FINAL_DATASET, checking for fallback data..."
            # Define train and test file paths for UNSW_NB15 as fallback
            TRAIN_FILE="data/received/UNSW_NB15_training-set.csv"
            TEST_FILE="data/received/UNSW_NB15_testing-set.csv"
            # Check if the UNSW_NB15 files exist, otherwise use sample data
            if [ -f "$TRAIN_FILE" ] && [ -f "$TEST_FILE" ]; then
              echo "Using UNSW_NB15 data files for hyperparameter tuning"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            else
              echo "No suitable data found, creating synthetic data for testing"
              # Try to find any CSV file to use as a fallback
              SAMPLE_DATA=""
              for csv_file in $(find data -name "*.csv" | head -n 1); do
                SAMPLE_DATA="$csv_file"
                break
              done
              if [ -z "$SAMPLE_DATA" ]; then
                echo "No CSV data found, creating a sample dataset for tuning"
                # Generate synthetic data for train and test (kept as fallback)
                python -c "import pandas as pd; import numpy as np; np.random.seed(42); n = 1000; df = pd.DataFrame({'feature1': np.random.normal(0, 1, n), 'feature2': np.random.normal(0, 1, n), 'feature3': np.random.normal(0, 1, n), 'feature4': np.random.normal(0, 1, n), 'feature5': np.random.normal(0, 1, n), 'label': np.random.choice([0, 1, 2], n)}); train = df.sample(frac=0.8, random_state=42); test = df.drop(train.index); train.to_csv('data/sample/synthetic_train.csv', index=False); test.to_csv('data/sample/synthetic_test.csv', index=False)"
                TRAIN_FILE="data/sample/synthetic_train.csv"
                TEST_FILE="data/sample/synthetic_test.csv"
              else
                # If we have a single file, create train/test split (kept as fallback)
                python -c "import pandas as pd; from sklearn.model_selection import train_test_split; df = pd.read_csv('$SAMPLE_DATA'); train, test = train_test_split(df, test_size=0.2, random_state=42); train.to_csv('data/sample/split_train.csv', index=False); test.to_csv('data/sample/split_test.csv', index=False)"
                TRAIN_FILE="data/sample/split_train.csv"
                TEST_FILE="data/sample/split_test.csv"
              fi
              echo "Using train file: $TRAIN_FILE"
              echo "Using test file: $TEST_FILE"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              # Use a small number of samples in CI for speed
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            fi
          fi
          # Apply the tuned parameters to the federated learning system
          if [ -f "tune_results/best_params.json" ]; then
            python use_tuned_params.py --params-file "./tune_results/best_params.json"
            echo "Successfully applied tuned parameters to federated learning"
          else
            echo "Warning: No tuned parameters found, will use default parameters"
          fi
      - name: Run federated learning pipeline
        run: |
          source venv/bin/activate
          # Run the federated learning with tuned parameters
          ./run_bagging.sh
      - name: Evaluate model performance
        run: |
          source venv/bin/activate
          # Compare model performance with and without tuned parameters
          echo "## XGBoost Model Performance" > model_performance.md
          echo "" >> model_performance.md
          # Extract metrics from the results directory
          if [ -d "results" ]; then
            echo "### Federated Learning Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "| Metric | Value |" >> model_performance.md
            echo "| ------ | ----- |" >> model_performance.md
            # Extract and add key metrics if available
            for metric_file in $(find results -name "*.txt" | grep -i "metrics\|accuracy\|f1\|precision\|recall"); do
              metric_name=$(basename "$metric_file" .txt)
              metric_value=$(cat "$metric_file" | head -n 1)
              echo "| $metric_name | $metric_value |" >> model_performance.md
            done
          fi
          # Add Ray Tune optimization results
          if [ -f "tune_results/best_params.json" ]; then
            echo "" >> model_performance.md
            echo "### Hyperparameter Optimization Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "Best hyperparameters:" >> model_performance.md
            echo '```json' >> model_performance.md
            cat tune_results/best_params.json >> model_performance.md
            echo '```' >> model_performance.md
          fi
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          git checkout multi-class-predictions
          # Add Ray Tune results
          git add tune_results/
          # Add aggegrated results files
          git add results/
          # Add model performance report
          git add model_performance.md
          # Add new files in outputs directory
          git add outputs/
          # Explicitly add any plot files 
          git add "outputs/**/*.png"
          # Commit all changes
          git commit -m "workflow with Ray Tune optimization in actionðŸš€"
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true
</file>

<file path="diagrams/client_operations.mmd">
graph LR
    subgraph ClientOperations [Client Operations client.py_client_utils.py]
        A[Receive Parameters from Server] --> B[Local Training: fit]
        B --> C[Evaluation: evaluate]
        C --> D[Metrics Calculation]
        D --> E[Send Results to Server]
        B --> F[Update Model]
        F --> B
        C --> G[Prediction if unlabeled data]
        G --> H[Save Predictions]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ClientOperations component;
</file>

<file path="diagrams/data_handling.mmd">
graph LR
    subgraph DataHandling [Data Handling dataset.py]
        A[Load CSV: load_csv_data] --> B[Preprocessing: preprocess_data]
        B --> C[Separate Features/Labels: separate_xy]
        C --> D[DMatrix Conversion: transform_dataset_to_dmatrix]
        D --> E[Train/Test Split: train_test_split]
        B --> F[Handle inf/NaN]
        F --> C
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class DataHandling component;
</file>

<file path="diagrams/overall_workflow.mmd">
graph LR
    subgraph OverallWorkflow [Overall Workflow]
        A[Client] --> B[Server]
        B --> A
        A --> C[Data]
        C --> A
    end
    
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class OverallWorkflow component
</file>

<file path="diagrams/server_operations.mmd">
graph LR
    subgraph ServerOperations [Server Operations server.py]
        A[Strategy Selection: FedXgbBagging/FedXgbCyclic] --> B[Client Management]
        B --> C[Model Aggregation]
        C --> D[Send Parameters to Clients]
        B --> E[Receive Results from Clients]
        E --> F[Metrics Aggregation]
        C --> G[Evaluation if centralized]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ServerOperations component;
</file>

<file path=".gitattributes">
data/received/final_dataset.csv filter=lfs diff=lfs merge=lfs -text
</file>

<file path=".gitignore">
.venv/
# Virtual environments (should NEVER be in Git)
.venv/
venv/
env/
ENV/

# Python cache
__pycache__/
*.pyc
*.pyo
*.pyd

# Data files (too large for Git)
data/old/
data/received/
received/
outputs/
*.csv
*.log

# Keep only essential CSV files
!data/received/final_dataset.csv
</file>

<file path=".repomixignore">
# Ignore data directory containing large files
data/

# Common large file types
*.csv
*.parquet
*.pkl
*.model

# Ignore outputs and results
outputs/
results/
local_utils/
received/
notebooks/
tune_results/

# Cache directories
__pycache__/
.cache/ 
.cursor/
</file>

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO, ERROR, WARNING
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
import importlib.util
from sklearn.utils.class_weight import compute_sample_weight
# Default XGBoost parameters for UNSW_NB15 multi-class classification
BST_PARAMS = {
    'objective': 'multi:softprob',  # Multi-class classification with probabilities
    'num_class': 11,  # Classes: 0-10 (Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic, plus class 10)
    'eval_metric': ['mlogloss', 'merror'],  # Multi-class metrics
    'learning_rate': 0.05,
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': 1.0  # Removed class-specific weights as it's not compatible with multi-class with >3 classes
}
# Try to import tuned parameters if available
try:
    # Check if tuned_params.py exists
    tuned_params_path = os.path.join(os.path.dirname(__file__), "tuned_params.py")
    if os.path.exists(tuned_params_path):
        # Dynamically import the tuned parameters
        spec = importlib.util.spec_from_file_location("tuned_params", tuned_params_path)
        tuned_params_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(tuned_params_module)
        # Use the tuned parameters
        TUNED_PARAMS = tuned_params_module.TUNED_PARAMS
        log(INFO, "Using tuned XGBoost parameters from Ray Tune optimization")
    else:
        TUNED_PARAMS = BST_PARAMS.copy()
except Exception as e:
    log(INFO, f"Could not load tuned parameters: {str(e)}")
    TUNED_PARAMS = BST_PARAMS.copy()
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
        cid: Client ID for logging purposes
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        cid,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None,
        use_tuned_params=True
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            cid: Client ID for logging purposes
            params (dict): XGBoost parameters (defaults to BST_PARAMS if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
            use_tuned_params (bool): Whether to use tuned parameters if available
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.cid = cid
        # Use tuned parameters if available and requested
        if params is not None:
            self.params = params
        elif use_tuned_params:
            self.params = TUNED_PARAMS.copy()
            log(INFO, "Using tuned parameters for XGBoost training")
        else:
            self.params = BST_PARAMS.copy()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        # --- PHASE 1: Aggressive Regularization (Overrides any loaded/tuned params) --- REMOVED
        y_train = self.train_dmatrix.get_label()
        # --- Check if labels are empty ---
        if y_train.size == 0:
            log(ERROR, f"Client {self.cid}: Training DMatrix has no labels. Cannot proceed with fit.")
            return FitRes(
                status=Status(code=Code.FIT_NOT_IMPLEMENTED, message="Training data is missing labels."),
                parameters=Parameters(tensor_type="", tensors=[]), # Return empty params
                num_examples=0,
                metrics={}
            )
        # --- End Check ---
        # Ensure labels are integers for compute_sample_weight
        y_train_int = y_train.astype(int)
        class_counts = np.bincount(y_train_int)
        # Log class distribution for all classes
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        for i, count in enumerate(class_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        # --- Debugging Sample Weight Calculation ---
        log(INFO, f"Type of y_train before weight calc: {type(y_train)}")
        log(INFO, f"Shape of y_train: {y_train.shape if hasattr(y_train, 'shape') else 'N/A'}")
        try:
            log(INFO, f"Unique values in y_train: {np.unique(y_train)}")
        except Exception as e:
            log(INFO, f"Could not get unique values of y_train: {e}")
        log(INFO, f"Type of y_train_int: {type(y_train_int)}")
        log(INFO, f"Shape of y_train_int: {y_train_int.shape}")
        log(INFO, f"Unique values in y_train_int: {np.unique(y_train_int)}")
        log(INFO, f"dtype of y_train_int: {y_train_int.dtype}")
        # Only log min/max if array is not empty
        if y_train_int.size > 0:
            log(INFO, f"Min/Max values in y_train_int: {np.min(y_train_int)} / {np.max(y_train_int)}")
        else:
            log(INFO, "y_train_int is empty, cannot calculate Min/Max.")
        # --- End Debugging ---
        # Compute sample weights for class imbalance
        try:
            sample_weights = compute_sample_weight('balanced', y_train_int) # Use integer labels
            log(INFO, f"Successfully computed sample weights. Shape: {sample_weights.shape}, dtype: {sample_weights.dtype}")
        except IndexError as e:
            log(INFO, f"IndexError during compute_sample_weight: {e}")
            log(INFO, f"Unique labels causing issue: {np.unique(y_train_int)}")
            # As a fallback, use uniform weights
            log(INFO, "Falling back to uniform sample weights.")
            sample_weights = np.ones(len(y_train_int))
        except Exception as e:
            log(INFO, f"Other error during compute_sample_weight: {e}")
            log(INFO, "Falling back to uniform sample weights due to unexpected error.")
            sample_weights = np.ones(len(y_train_int))
        # Create a new DMatrix with weights for training
        dtrain_weighted = xgb.DMatrix(self.train_dmatrix.get_data(), label=y_train, weight=sample_weights, feature_names=self.train_dmatrix.feature_names)
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch with sample weights
            bst = xgb.train(
                self.params,
                dtrain_weighted,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (dtrain_weighted, "train")],
                early_stopping_rounds=20,
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        # Since objective is multi:softprob, predict() outputs probabilities
        y_pred_proba = bst.predict(self.valid_dmatrix)
        # Get class labels from probabilities
        y_pred_labels = np.argmax(y_pred_proba, axis=1)
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        num_classes_actual = len(class_names) # Or get from self.params if needed
        for i, count in enumerate(true_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics using predicted labels
        # Add zero_division=0 to handle cases where a class might not be predicted
        precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss using probabilities
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_true_int = y_true.astype(int)
        # Ensure y_true_int does not contain labels outside the expected range [0, num_classes-1]
        valid_indices = (y_true_int >= 0) & (y_true_int < num_classes_actual)
        if not np.all(valid_indices):
            log(WARNING, f"Found {np.sum(~valid_indices)} labels outside expected range [0, {num_classes_actual-1}]. Clamping for mlogloss calculation.")
            y_true_int = np.clip(y_true_int, 0, num_classes_actual - 1)
            # Optionally filter data if clamping is not desired:
            # y_true_int = y_true_int[valid_indices]
            # y_pred_proba = y_pred_proba[valid_indices]
        y_true_one_hot = np.eye(num_classes_actual)[y_true_int]
        # Ensure y_pred_proba has the correct shape and handle potential issues
        if y_pred_proba.shape == (len(y_true_int), num_classes_actual):
            # Clip probabilities to avoid log(0)
            y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)
            mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_clipped), axis=1))
        else:
            log(WARNING, f"Shape mismatch for mlogloss: y_pred_proba shape {y_pred_proba.shape}, expected ({len(y_true_int)}, {num_classes_actual}). Skipping mlogloss.")
            mlogloss = -1.0 # Indicate failure to calculate
        # Compute confusion matrix using predicted labels
        try:
            # Explicitly provide labels to ensure consistent matrix size
            conf_matrix = confusion_matrix(y_true, y_pred_labels, labels=range(num_classes_actual))
        except Exception as e:
            log(WARNING, f"Error computing confusion matrix: {str(e)}")
            # Create empty confusion matrix with the correct size
            conf_matrix = np.zeros((num_classes_actual, num_classes_actual), dtype=int)
        # Generate detailed classification report using predicted labels
        try:
            # Ensure target_names matches the actual number of classes
            unique_labels = np.unique(np.concatenate((y_true.astype(int), y_pred_labels))) # Get labels present in data
            target_names_filtered = [class_names[i] for i in range(num_classes_actual) if i in unique_labels]
            # Ensure we use labels consistent with target_names_filtered
            labels_for_report = [i for i in range(num_classes_actual) if i in unique_labels]
            class_report = classification_report(
                y_true, 
                y_pred_labels, 
                labels=labels_for_report, 
                target_names=target_names_filtered, 
                zero_division=0
            )
            log(INFO, f"Classification Report:\n{class_report}")
        except Exception as e:
            log(WARNING, f"Error generating classification report: {str(e)}")
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix shape: {conf_matrix.shape}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from server_utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss)
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data,
    load_global_feature_processor,
    create_global_feature_processor,
    transform_dataset_to_dmatrix
)
from utils import client_args_parser, BST_PARAMS
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    # Ensure data/received/ directory exists
    os.makedirs("data/received", exist_ok=True)
    # Load labeled data for training - using the original final dataset with proper temporal splitting
    labeled_csv_path = "data/received/final_dataset.csv"
    log(INFO, "Using original final dataset with temporal splitting: %s", labeled_csv_path)
    # Check for global feature processor first
    global_processor_path = "outputs/global_feature_processor.pkl"
    if os.path.exists(global_processor_path):
        log(INFO, "Loading global feature processor for consistent preprocessing")
        global_processor = load_global_feature_processor(global_processor_path)
    else:
        log(WARNING, "Global feature processor not found, creating one from the dataset")
        global_processor_path = create_global_feature_processor(labeled_csv_path, "outputs")
        global_processor = load_global_feature_processor(global_processor_path)
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction if available
    # For now, we'll use a portion of the labeled data as unlabeled
    # This should be updated once an unlabeled version of the engineered dataset is available
    unlabeled_csv_path = labeled_csv_path  # Using the same file for now
    unlabeled_dataset = labeled_dataset    # Using the same dataset for now
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    # The ExponentialPartitioner doesn't have get_indices, but provides partition method
    # which returns the partition directly rather than just indices
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except Exception as e:
        log(INFO, f"get_partition failed ({e}), using fallback partitioning")
        # Fallback to simple index-based partitioning if get_partition is not available
        total_samples = len(full_train_data)
        samples_per_partition = total_samples // args.num_partitions
        start_idx = args.partition_id * samples_per_partition
        end_idx = (args.partition_id + 1) * samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
        log(INFO, f"Used fallback partitioning. Partition {args.partition_id}: samples {start_idx} to {end_idx}")
        log(INFO, f"Partition size: {end_idx - start_idx} samples (out of {total_samples} total)")
        # Get the partition slice
        train_partition = full_train_data.select(range(start_idx, end_idx))
    # Convert to pandas for easier manipulation
    train_partition_df = train_partition.to_pandas()
    # Perform local train/test split using client-specific random seed
    client_seed = args.seed + args.partition_id * 1000  # Make each client's seed unique
    log(INFO, f"Using client-specific random seed for train/test split: {client_seed}")
    # Log data shape before splitting
    log(INFO, f"Original data shape before splitting: {train_partition_df.shape}")
    # Check class distribution before splitting
    if 'label' in train_partition_df.columns:
        class_dist = train_partition_df['label'].value_counts().to_dict()
        log(INFO, f"Class distribution in original data: {class_dist}")
    # Use different random states for train/validation split to ensure no overlap
    train_random_state = client_seed
    val_random_state = client_seed + 5000  # Different seed for validation
    log(INFO, f"Using different random states for train/validation split: {train_random_state}/{val_random_state}")
    # Perform the split
    from sklearn.model_selection import train_test_split as sklearn_split
    train_df, valid_df = sklearn_split(
        train_partition_df, 
        test_size=args.test_fraction, 
        random_state=train_random_state,
        stratify=train_partition_df['label'] if 'label' in train_partition_df.columns else None
    )
    log(INFO, f"Train data shape: {train_df.shape}, Test data shape: {valid_df.shape}")
    # Log class distributions after splitting
    if 'label' in train_df.columns:
        train_class_dist = train_df['label'].value_counts().to_dict()
        valid_class_dist = valid_df['label'].value_counts().to_dict()
        log(INFO, f"Class distribution in train data: {train_class_dist}")
        log(INFO, f"Class distribution in test data: {valid_class_dist}")
    # Use the global processor to transform the data
    log(INFO, "Transforming data using global feature processor")
    try:
        # Transform using the global processor
        train_processed = global_processor.transform(train_df, is_training=True)
        valid_processed = global_processor.transform(valid_df, is_training=False)
        # Convert to DMatrix
        train_dmatrix = transform_dataset_to_dmatrix(train_processed, processor=global_processor, is_training=True)
        valid_dmatrix = transform_dataset_to_dmatrix(valid_processed, processor=global_processor, is_training=False)
        num_train = len(train_processed)
        num_val = len(valid_processed)
        log(INFO, f"Train DMatrix has {train_dmatrix.num_row()} rows, Test DMatrix has {valid_dmatrix.num_row()} rows")
        log(INFO, f"Local split: {num_train} train samples, {num_val} validation samples")
    except Exception as e:
        log(ERROR, f"Error in data transformation: {str(e)}")
        log(INFO, "Falling back to original train_test_split method")
        # Fallback to the original method if global processor fails
        train_dmatrix, valid_dmatrix, _ = train_test_split(
            train_partition, test_fraction=args.test_fraction, random_state=args.seed
        )
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=global_processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        cid=args.partition_id,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="commit.sh">
#!/bin/bash
# Stage all changes
git add .
# Commit the changes with a commit message
git commit -m "commit.sh"
# Push the changes to the remote repository
git push
</file>

<file path="create_global_processor.py">
#!/usr/bin/env python3
"""
create_global_processor.py
Script to create a global feature processor that ensures consistent preprocessing 
across Ray Tune hyperparameter optimization and Federated Learning phases.
This addresses the disconnection between individual client training and federated
learning by ensuring all phases use the same preprocessing statistics.
"""
import argparse
import os
import sys
from dataset import create_global_feature_processor, load_global_feature_processor
from flwr.common.logger import log
from logging import INFO
def main():
    """Create global feature processor for consistent preprocessing."""
    parser = argparse.ArgumentParser(
        description="Create global feature processor for consistent preprocessing"
    )
    parser.add_argument(
        "--data-file",
        type=str,
        default="data/received/final_dataset.csv",
        help="Path to the dataset file (default: data/received/final_dataset.csv)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs",
        help="Output directory for the processor (default: outputs)"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force recreation of processor even if it already exists"
    )
    args = parser.parse_args()
    # Check if data file exists
    if not os.path.exists(args.data_file):
        log(INFO, "Error: Data file not found: %s", args.data_file)
        sys.exit(1)
    # Check if processor already exists
    processor_path = os.path.join(args.output_dir, "global_feature_processor.pkl")
    if os.path.exists(processor_path) and not args.force:
        log(INFO, "Global feature processor already exists at: %s", processor_path)
        log(INFO, "Use --force to recreate it")
        # Load and display info about existing processor
        try:
            processor = load_global_feature_processor(processor_path)
            log(INFO, "Existing processor details:")
            log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
            log(INFO, "  Categorical features: %d", len(processor.categorical_features))
            log(INFO, "  Numerical features: %d", len(processor.numerical_features))
            log(INFO, "  Is fitted: %s", processor.is_fitted)
        except (FileNotFoundError, ImportError, AttributeError) as e:
            log(INFO, "Error loading existing processor: %s", str(e))
            log(INFO, "Consider using --force to recreate it")
        sys.exit(0)
    # Create the global processor
    log(INFO, "Creating global feature processor...")
    try:
        processor_path = create_global_feature_processor(args.data_file, args.output_dir)
        log(INFO, "Successfully created global feature processor at: %s", processor_path)
        # Verify the processor
        processor = load_global_feature_processor(processor_path)
        log(INFO, "Verification successful!")
        log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
        log(INFO, "  Categorical features: %d", len(processor.categorical_features))
        log(INFO, "  Numerical features: %d", len(processor.numerical_features))
        log(INFO, "  Is fitted: %s", processor.is_fitted)
        if hasattr(processor, 'unique_labels'):
            log(INFO, "  Unique labels: %s", processor.unique_labels)
    except (FileNotFoundError, ImportError, AttributeError, ValueError) as e:
        log(INFO, "Error creating global feature processor: %s", str(e))
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING, ERROR
import pickle
import os
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self, dataset_type="unsw_nb15"):
        """
        Initialize the feature processor.
        Args:
            dataset_type (str): Type of dataset to process.
                Options: "unsw_nb15" (original) or "engineered" (new dataset)
        """
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        self.label_encoder = LabelEncoder()
        self.dataset_type = dataset_type
        # Define feature groups based on dataset type
        if dataset_type == "unsw_nb15":
            # Original UNSW_NB15 dataset features
            self.categorical_features = [
                'proto', 'service', 'state', 'is_ftp_login', 'is_sm_ips_ports'
            ]
            self.numerical_features = [
                'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 
                'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 
                'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 
                'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 
                'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 
                'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst'
            ]
        elif dataset_type == "engineered":
            # New engineered dataset features - all are numerical (pre-normalized)
            self.categorical_features = []
            self.numerical_features = [
                'dur', 'sbytes', 'dbytes', 'Sload', 'swin', 'smeansz', 'Sjit', 'Stime',
                'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_dst_src_ltm',
                'duration', 'jit_ratio', 'inter_pkt_ratio', 'tcp_setup_ratio',
                'byte_pkt_interaction_dst', 'load_jit_interaction_dst', 'tcp_seq_diff'
            ]
        else:
            raise ValueError(f"Unknown dataset type: {dataset_type}")
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df.columns:
                unique_values = df[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df[df[col] == val]
                        if 'attack_cat' in df.columns and len(subset) > 0:
                            most_common_label = subset['attack_cat'].value_counts().idxmax()
                            label_pct = subset['attack_cat'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics - for original dataset or data validation
        # For engineered dataset, this will be minimal since data is already normalized
        if self.dataset_type == "unsw_nb15":
            for col in self.numerical_features:
                if col in df.columns:
                    self.numerical_stats[col] = {
                        'mean': df[col].mean(),
                        'std': df[col].std(),
                        'median': df[col].median(),
                        'q99': df[col].quantile(0.99)
                    }
        else:
            # For engineered dataset, we only track basic stats for validation
            # No need for extensive normalization since data is already normalized
            for col in self.numerical_features:
                if col in df.columns:
                    self.numerical_stats[col] = {
                        'min': df[col].min(),
                        'max': df[col].max(),
                        'median': df[col].median(),
                    }
        # Fit label encoder for attack_cat if present (original dataset)
        if 'attack_cat' in df.columns:
            self.label_encoder.fit(df['attack_cat'])
        # For engineered dataset with numeric labels, just record the unique labels
        if 'label' in df.columns and self.dataset_type == "engineered":
            self.unique_labels = sorted(df['label'].unique())
            log(INFO, f"Found {len(self.unique_labels)} unique labels in engineered dataset: {self.unique_labels}")
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            # If not fitted and not training, we should fit it anyway to avoid errors
            # This is needed for the centralized evaluation case
            log(INFO, "FeatureProcessor not fitted but needed for transform. Fitting now.")
            self.fit(df)
        df = df.copy()
        # Drop id column since it's just an identifier
        if 'id' in df.columns:
            df.drop(columns=['id'], inplace=True)
        # Transform categorical features (only needed for original dataset)
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with different approaches based on dataset type
        if self.dataset_type == "unsw_nb15":
            # Original dataset needs normalization and outlier handling
            for col in self.numerical_features:
                if col in df.columns and col in self.numerical_stats:
                    # Replace infinities
                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                    # Cap outliers using 99th percentile
                    q99 = self.numerical_stats[col]['q99']
                    df.loc[df[col] > q99, col] = q99  # Cap outliers
                    # Fill NaN with median
                    median = self.numerical_stats[col]['median']
                    df[col] = df[col].fillna(median)
        else:
            # Engineered dataset is already normalized, just handle missing values
            for col in self.numerical_features:
                if col in df.columns:
                    # Replace infinities and NaN with 0 (since data is normalized, 0 is a reasonable default)
                    df[col] = df[col].replace([np.inf, -np.inf, np.nan], 0)
        # Explicitly drop the raw attack_cat column if it exists (original dataset)
        if 'attack_cat' in df.columns:
            df.drop(columns=['attack_cat'], inplace=True)
        # For engineered dataset, we keep the 'label' column
        # This will be handled in preprocess_data instead
        if 'label' in df.columns and self.dataset_type == "unsw_nb15":
            df.drop(columns=['label'], inplace=True)
        return df
def preprocess_data(data: Union[pd.DataFrame, Dataset], processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification for both original and engineered datasets.
    Args:
        data (Union[pd.DataFrame, Dataset]): Input DataFrame or Hugging Face Dataset
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    if processor is None:
        # Auto-detect dataset type based on columns
        if 'attack_cat' in data.columns:
            processor = FeatureProcessor(dataset_type="unsw_nb15")
        elif 'tcp_seq_diff' in data.columns:
            processor = FeatureProcessor(dataset_type="engineered")
        else:
            log(WARNING, "Could not automatically detect dataset type. Defaulting to 'unsw_nb15'.")
            processor = FeatureProcessor(dataset_type="unsw_nb15")
    # --- Handle labels based on dataset type ---
    labels = None
    # For original UNSW_NB15 dataset with attack_cat
    if processor.dataset_type == "unsw_nb15" and 'attack_cat' in data.columns:
        # Extract 'attack_cat' before transforming features
        attack_labels = data['attack_cat'].copy()
        # Ensure label encoder is fitted if needed (during training)
        if is_training and not hasattr(processor.label_encoder, 'classes_'):
             log(INFO, "Fitting label encoder during training preprocessing.")
             processor.label_encoder.fit(attack_labels)
        elif not hasattr(processor.label_encoder, 'classes_') or processor.label_encoder.classes_.size == 0:
            # If not training but encoder isn't fitted, can't proceed reliably
            log(WARNING, "Label encoder not fitted, cannot transform attack_cat labels.")
            # Try fitting on the current chunk, might be incomplete
            try:
                 processor.label_encoder.fit(attack_labels)
                 log(WARNING, "Fitted label encoder on non-training data chunk.")
            except Exception as fit_err:
                 log(ERROR, f"Could not fit label encoder on non-training data: {fit_err}")
                 # Use a specific value like -1 or np.nan if XGBoost handles it, else zeros
                 labels = np.full(len(data), -1, dtype=int) # Fallback to -1
        # Transform labels if encoder is ready
        if hasattr(processor.label_encoder, 'classes_') and processor.label_encoder.classes_.size > 0:
            try:
                labels = processor.label_encoder.transform(attack_labels)
            except ValueError as e:
                log(ERROR, f"Error transforming labels: {e}. Unseen labels might exist.")
                labels = np.full(len(data), -1, dtype=int) # Simple fallback for now
        elif labels is None: # If fitting failed or wasn't possible earlier
             log(WARNING, "Assigning fallback labels because fitting failed or was not possible.")
             labels = np.full(len(data), -1, dtype=int)
    # For engineered dataset with direct numeric labels
    elif processor.dataset_type == "engineered" and 'label' in data.columns:
        log(INFO, "Using direct numeric labels from engineered dataset.")
        labels = data['label'].values
        # Log label distribution
        label_counts = data['label'].value_counts().to_dict()
        log(INFO, f"Label distribution in engineered dataset: {label_counts}")
    elif 'label' in data.columns:
        # Fallback case for binary label in original dataset
        log(WARNING, "Only binary 'label' column found, but multi-class labels expected for training.")
        labels = None 
    else:
        # No label column found
        log(INFO, "No label column found in data.")
        labels = None
    # --- Process features AFTER handling labels --- 
    # The processor's transform method will handle dropping labels differently based on dataset_type
    features = processor.transform(data, is_training)
    return features, labels
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Uses temporal splitting based on Stime column to avoid data leakage.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Auto-detect dataset type
    if 'attack_cat' in df.columns:
        print("Detected original UNSW_NB15 dataset with attack_cat column")
    elif 'tcp_seq_diff' in df.columns:
        print("Detected engineered dataset with normalized features")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, use temporal splitting to avoid data leakage
        # Sort by Stime column if available for temporal integrity
        if 'Stime' in df.columns:
            print("Using temporal splitting based on Stime column to avoid data leakage")
            df_sorted = df.sort_values('Stime').reset_index(drop=True)
            # Split temporally: first 80% for training, last 20% for testing
            train_size = int(0.8 * len(df_sorted))
            train_df = df_sorted.iloc[:train_size]
            test_df = df_sorted.iloc[train_size:]
            # Optional: shuffle within each set to add randomness while maintaining temporal integrity
            train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)
            test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)
            print(f"Temporal split: {len(train_df)} train samples, {len(test_df)} test samples")
            print(f"Train Stime range: {train_df['Stime'].min():.4f} to {train_df['Stime'].max():.4f}")
            print(f"Test Stime range: {test_df['Stime'].min():.4f} to {test_df['Stime'].max():.4f}")
            # Verify no temporal overlap
            if train_df['Stime'].max() <= test_df['Stime'].min():
                print("âœ“ No temporal overlap between train and test sets")
            else:
                print("âš  Warning: Temporal overlap detected between train and test sets")
            return DatasetDict({
                "train": Dataset.from_pandas(train_df),
                "test": Dataset.from_pandas(test_df)
            })
        else:
            # Fallback to stratified random split if no temporal column available
            print("No Stime column found, using stratified random split")
            train_test_dict = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label' if 'label' in df.columns else None)
            return DatasetDict({
                "train": train_test_dict["train"],
                "test": train_test_dict["test"]
            })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset (can be pandas DataFrame or Hugging Face Dataset)
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Now process the data using the pandas DataFrame
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # --- Logging before DMatrix creation ---
    log(INFO, f"[transform_dataset_to_dmatrix] is_training={is_training}")
    log(INFO, f"[transform_dataset_to_dmatrix] Features shape: {x.shape}")
    if y is not None:
        log(INFO, f"[transform_dataset_to_dmatrix] Labels type: {type(y)}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels shape: {y.shape if hasattr(y, 'shape') else 'N/A'}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels head: {y[:5] if hasattr(y, '__len__') and len(y) > 0 else 'N/A'}")
    else:
        log(INFO, "[transform_dataset_to_dmatrix] Labels are None.")
    # --- End Logging ---
    # Handle case where preprocess_data might return None for labels (e.g., unlabeled data)
    if y is None:
        log(INFO, "No labels found in data. Creating DMatrix without labels.")
        return xgb.DMatrix(x, missing=np.nan)
    # For validation data, log label distribution to help identify issues
    if not is_training:
        # Count occurrences of each label
        label_counts = np.bincount(y.astype(int))
        # Use the correct class names for UNSW_NB15 dataset if possible
        if hasattr(processor, 'dataset_type') and processor.dataset_type == "unsw_nb15":
            label_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'] 
        else:
            # For engineered dataset, use numeric labels as names
            label_names = [str(i) for i in range(len(label_counts))]
        log(INFO, "Label distribution in validation data:")
        for i, count in enumerate(label_counts):
            if i < len(label_names):
                class_name = label_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"  {class_name}: {count}")
    return xgb.DMatrix(x, label=y, missing=np.nan)
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into train and test sets, preprocess, and return DMatrices and the fitted processor.
    Args:
        data: Input dataset (Hugging Face Dataset or pandas DataFrame)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: 
            - Training DMatrix
            - Test DMatrix
            - Fitted FeatureProcessor instance
    """
    # Convert to pandas if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use sklearn's train_test_split with shuffle=True to ensure data is properly randomized
    log(INFO, "Original data shape before splitting: %s", data.shape)
    # Set random seed for consistency
    np.random.seed(random_state)
    # Auto-detect dataset type
    if 'attack_cat' in data.columns:
        dataset_type = "unsw_nb15"
    elif 'tcp_seq_diff' in data.columns:
        dataset_type = "engineered"
    else:
        dataset_type = "unsw_nb15"  # Default
        log(WARNING, "Could not auto-detect dataset type. Defaulting to 'unsw_nb15'.")
    # Initialize appropriate feature processor
    processor = FeatureProcessor(dataset_type=dataset_type)
    # Check if 'label' column exists in data
    label_col = 'label' if dataset_type == "engineered" else 'label' 
    if label_col not in data.columns:
        log(INFO, "Warning: No '%s' column found in data. Available columns: %s", 
            label_col, data.columns.tolist())
    else:
        # Report class distribution
        label_counts = data[label_col].value_counts().to_dict()
        log(INFO, "Class distribution in original data: %s", label_counts)
    # Check for data leakage indicators
    if 'uid' in data.columns:
        uid_label_counts = data.groupby('uid')[label_col].value_counts()
        uid_with_multiple_labels = uid_label_counts.index.get_level_values(0).duplicated(keep=False)
        if not any(uid_with_multiple_labels):
            log(WARNING, "CRITICAL: Each UID has only one label, indicating potential perfect data leakage through UIDs")
    # Generate a completely different random_state for validation split
    validation_random_state = (random_state * 17 + 3) % 10000
    log(INFO, "Using different random states for train/validation split: %d/%d", 
        random_state, validation_random_state)
    # Split data ensuring complete partition separation
    if 'uid' in data.columns:
        # If we have UIDs, use them to ensure no data leakage across train/test
        log(INFO, "Using UID-based splitting to ensure no data leakage")
        unique_uids = data['uid'].unique()
        np.random.seed(validation_random_state)
        np.random.shuffle(unique_uids)
        test_size = int(len(unique_uids) * test_fraction)
        test_uids = unique_uids[:test_size]
        train_uids = unique_uids[test_size:]
        # Split based on UIDs
        train_data = data[data['uid'].isin(train_uids)].copy()
        test_data = data[data['uid'].isin(test_uids)].copy()
        log(INFO, "Split by UIDs: %d train UIDs, %d test UIDs", len(train_uids), len(test_uids))
    else:
        # If no UIDs, use standard stratified split
        train_data, test_data = train_test_split_pandas(
            data,
            test_size=test_fraction,
            random_state=validation_random_state,
            shuffle=True,  # Ensure data is shuffled for a proper split
            stratify=data[label_col] if label_col in data.columns else None  # Use stratified split if possible
        )
    # Log the shapes to verify they're different sets
    log(INFO, "Train data shape: %s, Test data shape: %s", train_data.shape, test_data.shape)
    # Verify label distributions to ensure proper stratification
    if label_col in data.columns:
        train_label_counts = train_data[label_col].value_counts().to_dict()
        test_label_counts = test_data[label_col].value_counts().to_dict()
        log(INFO, "Class distribution in train data: %s", train_label_counts)
        log(INFO, "Class distribution in test data: %s", test_label_counts)
    # Check for unique values in both sets to verify they're actually different
    if 'uid' in data.columns:
        train_uids = set(train_data['uid'].unique())
        test_uids = set(test_data['uid'].unique())
        common_uids = train_uids.intersection(test_uids)
        if common_uids:
            log(WARNING, "WARNING: Found %d UIDs in both train and test sets! This indicates data leakage.", 
                len(common_uids))
        else:
            log(INFO, "Good: Train and test sets have completely separate UIDs (no overlap).")
    # Fit processor on training data and transform both sets
    # Note: transform calls fit implicitly if is_training=True and not fitted
    train_dmatrix = transform_dataset_to_dmatrix(train_data, processor=processor, is_training=True)
    test_dmatrix = transform_dataset_to_dmatrix(test_data, processor=processor, is_training=False)
    # Log number of examples for verification
    log(INFO, "Train DMatrix has %d rows, Test DMatrix has %d rows", 
        train_dmatrix.num_row(), test_dmatrix.num_row())
    # Return the fitted processor along with DMatrices
    return train_dmatrix, test_dmatrix, processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
def create_global_feature_processor(data_file: str, output_dir: str = "outputs") -> str:
    """
    Create a global feature processor fitted on the full training dataset.
    This ensures consistent preprocessing across Ray Tune and Federated Learning.
    Args:
        data_file (str): Path to the dataset file
        output_dir (str): Directory to save the processor
    Returns:
        str: Path to the saved processor file
    """
    log(INFO, f"Creating global feature processor from: {data_file}")
    # Load full dataset
    dataset = load_csv_data(data_file)
    train_data = dataset["train"]
    train_df = train_data.to_pandas()
    # Auto-detect dataset type
    if 'attack_cat' in train_df.columns:
        dataset_type = "unsw_nb15"
    elif 'tcp_seq_diff' in train_df.columns:
        dataset_type = "engineered"
    else:
        dataset_type = "unsw_nb15"  # Default
        log(WARNING, "Could not auto-detect dataset type. Defaulting to 'unsw_nb15'.")
    # Create and fit processor on full training data
    processor = FeatureProcessor(dataset_type=dataset_type)
    processor.fit(train_df)
    # Save processor to file
    os.makedirs(output_dir, exist_ok=True)
    processor_path = os.path.join(output_dir, "global_feature_processor.pkl")
    with open(processor_path, 'wb') as f:
        pickle.dump(processor, f)
    log(INFO, f"Global feature processor saved to: {processor_path}")
    log(INFO, f"Processor type: {dataset_type}")
    log(INFO, f"Categorical features: {len(processor.categorical_features)}")
    log(INFO, f"Numerical features: {len(processor.numerical_features)}")
    return processor_path
def load_global_feature_processor(processor_path: str) -> FeatureProcessor:
    """
    Load a pre-fitted global feature processor.
    Args:
        processor_path (str): Path to the saved processor file
    Returns:
        FeatureProcessor: The loaded processor
    """
    if not os.path.exists(processor_path):
        raise FileNotFoundError(f"Global feature processor not found at: {processor_path}")
    with open(processor_path, 'rb') as f:
        processor = pickle.load(f)
    log(INFO, f"Loaded global feature processor from: {processor_path}")
    return processor
# Comment out or remove ModelPredictor if not used or complete
# class ModelPredictor:
#     """
#     Handles model prediction and dataset labeling
#     """
#     def __init__(self, model_path: str):
#         self.model = xgb.Booster()
#         self.model.load_model(model_path)
#     def predict_and_save(
#         self,
#         input_data: Union[str, pd.DataFrame],
#         output_path: str,
#         include_confidence: bool = True
#     ):
#         """
#         Predict on new data and save labeled dataset
#         """
#         # Load/preprocess input data
#         # data = self._prepare_data(input_data) # Requires _prepare_data method
#         # Generate predictions
#         # predictions = self.model.predict(data)
#         # confidence = None
#         # if include_confidence:
#         #     confidence = self.model.predict(data, output_margin=True)
#         # Save labeled dataset
#         # self._save_output(data, predictions, confidence, output_path) # Requires _save_output method
</file>

<file path="go_to_work.sh">
#!/bin/bash
# Run Python scripts in the background and store their process IDs (PIDs)
python3 data/receiving_data.py &
PID1=$!
python3 data/livepreprocessing_socket.py &
PID2=$!
# Wait for 1 minute
sleep 30
# Kill the Python scripts
kill $PID1 $PID2
# Run Git commands and GitHub workflow
git pull
./commit.sh
gh workflow run cml.yaml
# Wait for 5 minutess
sleep 300
git pull
</file>

<file path="multi_class_implementation_plan.md">
# Multi-Class Classification Implementation Plan

## Current State
- Binary classification (benign vs malicious) using XGBoost
- Features include both categorical and numerical network traffic data
- Using federated learning with Flower framework
- Need to expand to classify specific types of malicious traffic

## Implementation Steps

### 1. Data Preprocessing Modifications (`dataset.py`)
- [ ] Update `preprocess_data()` function:
  ```python
  def preprocess_data(data):
      # ... existing code ...
      if 'label' in df.columns:
          features = df.drop(columns=['label'])
          
          # New label mapping for three classes
          label_mapping = {
              'benign': 0, 
              'dns_tunneling': 1, 
              'icmp_tunneling': 2
          }
          labels_series = df['label'].map(label_mapping)
          
          # Handle unmapped labels
          if labels_series.isnull().any():
              unmapped_labels = df['label'][labels_series.isnull()].unique()
              print(f"Warning: Unmapped labels found: {unmapped_labels}")
              labels_series = labels_series.fillna(-1)
          
          labels = labels_series.astype(int)
          return features, labels
      else:
          return df, None
  ```

### 2. Model Configuration Updates (`client_utils.py`)
- [ ] Update XGBoost parameters in `utils.py`:
  ```python
  BST_PARAMS = {
      'objective': 'multi:softmax',
      'num_class': 3,
      'eval_metric': ['mlogloss', 'merror'],
      'learning_rate': 0.1,
      'max_depth': 6,
      'min_child_weight': 1,
      'subsample': 0.8,
      'colsample_bytree': 0.8,
      'scale_pos_weight': [1.0, 2.0, 2.0]  # Adjust based on class distribution
  }
  ```

- [ ] Modify `evaluate()` method in `XgbClient` class:
  ```python
  def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
      # ... existing model loading code ...
      
      # Generate predictions - No thresholding needed for multi-class
      y_pred_proba = bst.predict(self.valid_dmatrix)
      y_pred_labels = np.argmax(y_pred_proba, axis=1)
      
      # Get ground truth labels
      y_true = self.valid_dmatrix.get_label()
      
      # Compute multi-class metrics
      precision = precision_score(y_true, y_pred_labels, average='weighted')
      recall = recall_score(y_true, y_pred_labels, average='weighted')
      f1 = f1_score(y_true, y_pred_labels, average='weighted')
      accuracy = accuracy_score(y_true, y_pred_labels)
      
      # Multi-class loss calculation
      epsilon = 1e-10
      log_likelihood = -np.log(y_pred_proba[np.arange(len(y_true)), y_true.astype(int)] + epsilon)
      loss = np.mean(log_likelihood)
      
      # Multi-class confusion matrix
      conf_matrix = confusion_matrix(y_true, y_pred_labels)
      
      metrics = {
          "precision": float(precision),
          "recall": float(recall),
          "f1": float(f1),
          "accuracy": float(accuracy),
          "loss": float(loss),
          "confusion_matrix": conf_matrix.tolist(),
          "num_predictions": self.num_val
      }
      
      return metrics
  ```

### 3. Server-Side Updates (`server_utils.py`)
- [ ] Update metrics aggregation:
  ```python
  def evaluate_metrics_aggregation(eval_metrics):
      total_num = sum([num for num, _ in eval_metrics])
      
      # Aggregate evaluation metrics
      metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy', 'loss']
      aggregated_metrics = {}
      
      for metric in metrics_to_aggregate:
          if all(metric in metrics for _, metrics in eval_metrics):
              aggregated_metrics[metric] = sum([metrics[metric] * num for num, metrics in eval_metrics]) / total_num
          else:
              aggregated_metrics[metric] = 0.0
      
      # Aggregate confusion matrix
      aggregated_conf_matrix = None
      for num, metrics in eval_metrics:
          conf_matrix = np.array(metrics.get("confusion_matrix", [[0, 0, 0], [0, 0, 0], [0, 0, 0]]))
          if aggregated_conf_matrix is None:
              aggregated_conf_matrix = conf_matrix
          else:
              aggregated_conf_matrix += conf_matrix
      
      aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix.tolist()
      aggregated_metrics["prediction_mode"] = eval_metrics[0][1].get("prediction_mode", False)
      
      return aggregated_metrics["loss"], aggregated_metrics
  ```

- [ ] Update prediction saving:
  ```python
  def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None):
      predictions_dict = {
          'predicted_label': predictions,
          'prediction_type': [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling') 
              for p in predictions
          ]
      }
      # ... rest of the function
  ```

### 4. Prediction Updates (`use_saved_model.py`)
- [ ] Update prediction saving:
  ```python
  def save_detailed_predictions(predictions, output_path):
      results_df = pd.DataFrame()
      
      if predictions.ndim > 1 and predictions.shape[1] > 1:
          results_df['raw_probabilities'] = predictions.tolist()
          predicted_labels = np.argmax(predictions, axis=1)
          results_df['predicted_label'] = predicted_labels
          
          results_df['prediction_type'] = [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling')
              for p in predicted_labels
          ]
          
          results_df['prediction_score'] = predictions[
              np.arange(len(predicted_labels)), 
              predicted_labels
          ]
      
      results_df.to_csv(output_path, index=False)
      return results_df
  ```

- [ ] Update main evaluation:
  ```python
  def main():
      # ... existing code ...
      if args.has_labels:
          y_pred_labels = np.argmax(raw_predictions, axis=1)
          accuracy = accuracy_score(y_true, y_pred_labels)
          
          cm = confusion_matrix(y_true, y_pred_labels)
          report = classification_report(y_true, y_pred_labels)
          
          log(INFO, f"Accuracy: {accuracy:.4f}")
          log(INFO, f"Confusion Matrix:\n{cm}")
          log(INFO, f"Classification Report:\n{report}")
  ```

## Testing Strategy

### 1. Unit Tests
- [ ] Test label mapping in `dataset.py`
- [ ] Test multi-class metrics calculation
- [ ] Test prediction format and class probabilities
- [ ] Test confusion matrix calculation

### 2. Integration Tests
- [ ] Test full training pipeline with three classes
- [ ] Test federated learning convergence
- [ ] Test model saving and loading
- [ ] Test prediction pipeline

### 3. System Tests
- [ ] End-to-end training and evaluation
- [ ] Performance testing with large datasets
- [ ] Class imbalance handling
- [ ] Error handling and edge cases

## Success Criteria
1. Model successfully trains on three-class data
2. All evaluation metrics properly calculated and reported
3. Predictions include class probabilities
4. Federated learning pipeline works with multiple classes
5. High accuracy in distinguishing between benign and malicious traffic
6. Accurate classification of DNS vs ICMP tunneling attacks
7. Documentation is complete and accurate
8. Test cases pass successfully

## Potential Challenges and Mitigations
1. Class imbalance
   - Solution: Use class weights in model parameters
   - Monitor class distribution in training data
   - Consider data augmentation for underrepresented classes

2. Model complexity
   - Solution: Start with simpler model and gradually increase complexity
   - Monitor training metrics for overfitting
   - Use cross-validation for parameter tuning

3. Federated learning convergence
   - Solution: Adjust learning rate and number of rounds
   - Monitor loss curves across clients
   - Consider using FedAvg with momentum

4. Performance impact
   - Solution: Profile code for bottlenecks
   - Optimize prediction pipeline
   - Consider batch processing for large datasets
</file>

<file path="original_bst_params.json">
{
  "objective": "multi:softmax",
  "num_class": 3,
  "eta": 0.05,
  "max_depth": 3,
  "min_child_weight": 10,
  "gamma": 1.0,
  "subsample": 0.7,
  "colsample_bytree": 0.6,
  "colsample_bylevel": 0.6,
  "nthread": 16,
  "tree_method": "hist",
  "eval_metric": [
    "mlogloss",
    "merror"
  ],
  "max_delta_step": 5,
  "reg_alpha": 2.0,
  "reg_lambda": 5.0,
  "base_score": 0.5,
  "scale_pos_weight": [
    1.0,
    2.0,
    1.0
  ],
  "grow_policy": "lossguide",
  "normalize_type": "tree",
  "random_state": 42
}
</file>

<file path="PREPROCESSING_CONSISTENCY_FIX.md">
# Preprocessing Consistency Fix

## Problem Fixed

Your federated learning pipeline was experiencing disconnection between two phases:

1. **Individual Client Training** (showing good learning curves with decreasing mlogloss)
2. **Federated Learning** (showing stagnant accuracy at 0.5453)

## Root Cause

The disconnection was caused by **inconsistent preprocessing** across different phases:

- **Ray Tune optimization**: Created its own `FeatureProcessor` for each trial
- **Federated Learning clients**: Each created their own `FeatureProcessor` fitted on local data
- **Server evaluation**: Created its own `FeatureProcessor` for centralized evaluation

This led to different scaling, encoding, and feature transformations across phases, making the hyperparameters learned during Ray Tune ineffective for federated learning.

## Solution Implemented

### 1. Global Feature Processor

Created a **single, global `FeatureProcessor`** that:
- Is fitted on the **full training dataset** (`data/received/final_dataset.csv`)
- Uses **temporal splitting** to prevent data leakage
- Is saved to `outputs/global_feature_processor.pkl`
- Is shared across **all phases** (Ray Tune, Server, Clients)

### 2. Updated Scripts

#### A. `run_bagging.sh` (Primary Fix)
Updated to create the global processor before starting federated learning:

```bash
# Step 1: Create global feature processor
python create_global_processor.py \
    --data-file "data/received/final_dataset.csv" \
    --output-dir "outputs" \
    --force

# Step 2: Start server (uses global processor)
python3 server.py --pool-size=5 --num-rounds=5 --num-clients-per-round=5 --centralised-eval

# Step 3: Start clients (use global processor)
python3 client.py --partition-id=0 --num-partitions=5 --partitioner-type=exponential
# ... (all clients)
```

#### B. GitHub Workflow (`.github/workflows/cml.yaml`)
Updated to use the same `final_dataset.csv` for both Ray Tune and federated learning:

```yaml
# Ray Tune step now uses the same dataset as federated learning
bash run_ray_tune.sh --data-file "data/received/final_dataset.csv" --num-samples 5

# Federated learning step uses the same dataset
./run_bagging.sh
```

#### C. `create_global_processor.py`
New script that creates the global processor:

```bash
python create_global_processor.py \
    --data-file "data/received/final_dataset.csv" \
    --output-dir "outputs" \
    --force
```

## Key Improvements

### âœ… Consistent Preprocessing
- **Same feature scaling** across all phases
- **Same categorical encoding** across all phases  
- **Same data transformations** across all phases

### âœ… Temporal Data Integrity
- **No data leakage** between train/test splits
- **Temporal splitting** based on `Stime` column
- **Proper train/validation separation**

### âœ… Phase Alignment
- **Ray Tune** uses same preprocessing as **Federated Learning**
- **Individual client training** matches **federated aggregation**
- **Hyperparameters** transfer effectively between phases

## Usage Instructions

### For GitHub Actions Workflow
Your workflow automatically runs the fixed version:

1. **Ray Tune Step**: Creates global processor and tunes hyperparameters
2. **Federated Learning Step**: Uses same global processor for training
3. **Results**: Should now show consistent performance across phases

### For Local Development

#### Option 1: Use the workflow scripts (Recommended)
```bash
# Run the complete pipeline with consistent preprocessing
source venv/bin/activate
./run_bagging.sh
```

#### Option 2: Manual step-by-step
```bash
# Step 1: Create global processor
python create_global_processor.py \
    --data-file "data/received/final_dataset.csv" \
    --output-dir "outputs"

# Step 2: Run Ray Tune (optional)
bash run_ray_tune.sh --data-file "data/received/final_dataset.csv"

# Step 3: Run federated learning
./run_bagging.sh
```

## Expected Results

After this fix, you should see:

1. **Consistent Learning Curves**: Both individual client training and federated learning should show similar learning patterns
2. **Improved Federated Performance**: Accuracy should improve beyond the previous stagnant 0.5453
3. **Effective Hyperparameter Transfer**: Parameters tuned in Ray Tune should work effectively in federated learning
4. **Reproducible Results**: Same preprocessing ensures consistent results across runs

## Files Modified

1. **`run_bagging.sh`** - Added global processor creation step
2. **`.github/workflows/cml.yaml`** - Updated to use consistent dataset
3. **`dataset.py`** - Added global processor creation/loading functions
4. **`create_global_processor.py`** - New script for processor creation
5. **`ray_tune_xgboost_updated.py`** - Updated to use global processor
6. **`client.py`** - Updated to load global processor
7. **`server.py`** - Updated to use global processor

## Validation

To verify the fix is working:

1. **Check processor creation**: Look for `âœ“ Global feature processor ready at outputs/global_feature_processor.pkl`
2. **Monitor learning curves**: Both phases should show similar learning patterns
3. **Check accuracy progression**: Federated learning accuracy should improve over rounds
4. **Verify consistency**: Individual client metrics should align with federated metrics

## Troubleshooting

### Issue: "Global feature processor not found"
**Solution**: Ensure `run_bagging.sh` is being used, which creates the processor automatically.

### Issue: "Still seeing stagnant accuracy"
**Check**: 
- Verify `outputs/global_feature_processor.pkl` exists
- Check that all clients and server are loading the same processor
- Ensure temporal splitting is working correctly

### Issue: "Ray Tune parameters not transferring"
**Check**:
- Both Ray Tune and federated learning are using `data/received/final_dataset.csv`
- Global processor is created before both phases
- `tune_results/best_params.json` exists and is being applied
</file>

<file path="pretrained_model_usage.md">
# Using the Saved Federated Learning Model

This document explains how to use the saved XGBoost model that was trained using the federated learning process.

## Overview

The federated learning process now saves the final trained model after the training process completes. The model is saved in two formats:

1. JSON format (`final_model.json`) - Human-readable format
2. Binary format (`final_model.bin`) - More efficient for loading

These files are saved in the output directory created for each training run, which follows the pattern:
```
outputs/YYYY-MM-DD/HH-MM-SS/
```

## Making Predictions with the Saved Model

We provide a utility script `use_saved_model.py` that demonstrates how to load the saved model and use it for making predictions on new data.

### Prerequisites

Make sure you have all the required dependencies installed:
- XGBoost
- Pandas
- NumPy
- Scikit-learn (for evaluation metrics)
- Flower (for logging utilities)

### Basic Usage

```bash
python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data> --output_path <path_for_predictions>
```

#### Arguments:

- `--model_path`: Path to the saved model file (.json or .bin)
- `--data_path`: Path to the data file (.csv)
- `--output_path`: Path to save the predictions (default: predictions.csv)
- `--has_labels`: Flag to indicate if the data file contains labels (for evaluation)
- `--info_only`: Display model information without making predictions

### Examples

#### Viewing model information:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --info_only
```

This will display information about the model such as the number of trees, feature importance, and model parameters.

#### Making predictions on unlabeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/unlabeled_data.csv --output_path predictions.csv
```

#### Evaluating the model on labeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/test_data.csv --output_path predictions.csv --has_labels
```

When using the `--has_labels` flag, the script will also calculate and display evaluation metrics such as accuracy, precision, recall, and F1 score.

## Troubleshooting

### Model File Not Found
Make sure the path to the model file is correct. The model files are saved in the output directory created for each training run.

### Model Loading Issues
If you encounter issues loading the model, the system will try multiple approaches:
1. Direct loading using `load_model`
2. Reading the file as bytes and loading into a Booster
3. Creating a new Booster with parameters and then loading the model

### Data Format Issues
The data should be in a format that can be converted to an XGBoost DMatrix. If you're having issues, check that your data has the same features that were used during training.

If you use the `--has_labels` flag but the system can't process the data as labeled, it will automatically fall back to processing it as unlabeled data.

### Memory Issues
If you're working with large datasets, you might encounter memory issues. Consider processing the data in batches or using a machine with more memory.

## Additional Resources

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Flower Documentation](https://flower.dev/docs/)
</file>

<file path="progress.md">
# FL-CML-Pipeline: Progress Summary

## Project Overview

This project implements a privacy-preserving machine learning solution using federated learning with the Flower framework. The system allows multiple clients to collaboratively train XGBoost models for network intrusion detection without sharing raw data, preserving privacy while achieving high model performance.

## Key Components

### 1. Federated Learning Architecture

- **Server Implementation (`server.py`)**
  - Controls the federated learning process
  - Implements both bagging and cyclic training strategies
  - Handles model aggregation and evaluation
  - Manages client selection and coordination

- **Client Implementation (`client.py`)**
  - Loads and processes local data
  - Trains local models based on server instructions
  - Participates in the federated learning process
  - Reports results back to the server

### 2. Data Pipeline

- **Data Processing (`dataset.py`)**
  - Loads CSV data from network traffic captures
  - Implements preprocessing for network traffic features
  - Provides multiple partitioning strategies (IID, Linear, Square, Exponential)
  - Handles data format conversions for XGBoost compatibility

- **Real-time Data Capture**
  - Support for live network traffic capture
  - Processing and conversion to training datasets

### 3. Utility Functions

- **Client Utilities (`client_utils.py`)**
  - XGBoost client implementation
  - Client-side helper functions

- **Server Utilities (`server_utils.py`)**
  - Server-side helper functions
  - Client management systems
  - Results handling and storage

### 4. Experiment Framework

- **Training Methods**
  - Bagging approach (`run_bagging.sh`): Aggregates models from multiple clients
  - Cyclic approach (`run_cyclic.sh`): Passes model sequentially through clients

- **Evaluation**
  - Supports both centralized and federated evaluation
  - Tracks multiple metrics (precision, recall, F1 score)

## Project Features

- âœ… **Privacy-Preserving Training** - True federated learning with data isolation
- âœ… **Flexible Configuration** - Support for various training strategies
- âœ… **Reproducible Experiments** - Automatic output organization
- âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline
- âœ… **Multiple Partitioning Strategies** - IID, Linear, Square, Exponential

## Recent Developments

### Implemented Core Functionality
- Established federated learning architecture with Flower framework
- Created data processing pipeline for network traffic data
- Implemented both bagging and cyclic training approaches
- Set up experiment scripts for reproducible testing

### Technical Improvements
- Enhanced XGBoost integration with Flower framework
- Improved data partitioning strategies
- Optimized client-server communication
- Implemented comprehensive metrics tracking

### Documentation
- Created detailed README with project structure and instructions
- Documented key components and their relationships
- Added configuration guidelines and examples

### Infrastructure
- Set up project directory structure
- Created Bash scripts for easy experiment execution
- Implemented output storage and organization

## Next Steps

### Planned Enhancements
- Improve scalability for larger numbers of clients
- Enhance privacy guarantees with differential privacy techniques
- Optimize hyperparameters for better model performance
- Add support for more model architectures

### Ongoing Research
- Comparing performance of bagging vs. cyclic approaches
- Analyzing impact of different data partitioning strategies
- Evaluating model convergence across federated strategies

## Conclusion

The FL-CML-Pipeline project has established a solid foundation for privacy-preserving machine learning using federated learning. The implemented system successfully demonstrates collaborative model training across multiple clients without sharing raw data, achieving the core goal of privacy-preserving machine learning for network intrusion detection.

The project continues to evolve with ongoing research into optimal federated learning strategies and implementation improvements to enhance performance and usability.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.4.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "xgboost-comprehensive"
version = "0.1.0"
description = "Federated XGBoost with Flower (comprehensive)"
authors = ["The Flower Authors <hello@flower.ai>"]

[tool.poetry.dependencies]
python = ">=3.8,<3.11"
flwr = { extras = ["simulation"], version = ">=1.7.0,<2.0" }
flwr-datasets = ">=0.2.0,<1.0.0"
xgboost = ">=2.0.0,<3.0.0"
</file>

<file path="ray_tune_README.md">
# Ray Tune XGBoost Hyperparameter Optimization

This guide explains how to use Ray Tune for optimizing the hyperparameters of the XGBoost classifier in our federated learning pipeline.

## Overview

Ray Tune is a powerful library for hyperparameter tuning that can efficiently find optimal parameters for machine learning models. We've integrated Ray Tune with our XGBoost implementation to achieve better model performance through automated hyperparameter search.

## Requirements

Make sure you have all the required packages installed:

```bash
pip install -r requirements.txt
```

## Hyperparameter Tuning Process

### 1. Run Ray Tune Optimization

Use the `run_ray_tune.sh` script to start the optimization process:

```bash
# Basic usage
bash run_ray_tune.sh --data-file path/to/your/data.csv

# Advanced usage with more options
bash run_ray_tune.sh \
  --data-file path/to/your/data.csv \
  --num-samples 20 \
  --cpus-per-trial 2 \
  --gpu-fraction 0.2 \
  --output-dir ./my_tuning_results
```

#### Available Options

- `--data-file`: Path to the CSV data file (required)
- `--num-samples`: Number of hyperparameter combinations to try (default: 10)
- `--cpus-per-trial`: CPUs to allocate per trial (default: 1)
- `--gpu-fraction`: Fraction of GPU to use per trial (e.g., 0.1 for 10%)
- `--output-dir`: Directory to save results (default: ./tune_results)

### 2. Apply Tuned Parameters to Federated Learning

After tuning is complete, use the `use_tuned_params.py` script to integrate the best parameters into your federated learning system:

```bash
python use_tuned_params.py --params-file ./tune_results/best_params.json
```

This will:
1. Backup the original parameters to `original_bst_params.json`
2. Create a `tuned_params.py` file with the optimized parameters
3. Provide instructions on how to use these parameters

### 3. Run Federated Learning with Tuned Parameters

The federated learning system will automatically detect and use the tuned parameters if:

1. The `tuned_params.py` file exists in the project directory
2. You haven't explicitly provided custom parameters to the XgbClient

## How It Works

The Ray Tune optimization process:

1. **Search Space Definition**: We define a search space for hyperparameters like `max_depth`, `min_child_weight`, `eta`, etc.
2. **ASHA Scheduler**: We use the Asynchronous Successive Halving Algorithm (ASHA) for early stopping of poorly performing trials
3. **Parallel Execution**: Multiple hyperparameter combinations are evaluated in parallel
4. **Best Model Selection**: The best performing model is identified based on validation metrics
5. **Model Persistence**: The best model and its parameters are saved for later use

## Parameters Being Tuned

The following XGBoost parameters are optimized:

- `max_depth`: Maximum depth of a tree
- `min_child_weight`: Minimum sum of instance weight needed in a child
- `eta`: Learning rate
- `subsample`: Subsample ratio of the training instances
- `colsample_bytree`: Subsample ratio of columns when constructing each tree
- `reg_alpha`: L1 regularization term on weights
- `reg_lambda`: L2 regularization term on weights
- `num_boost_round`: Number of boosting rounds

## GPU Support

If you have a GPU available, you can use it to speed up the tuning process by specifying a `--gpu-fraction` value. This enables XGBoost's GPU acceleration via the `gpu_hist` tree method.

## Monitoring and Results

During tuning, progress is logged to the console. After completion, you'll find these files in the output directory:

- `best_params.json`: JSON file containing the best hyperparameters
- `best_model.json`: XGBoost model trained with the best hyperparameters
- `progress.csv`: CSV file with metrics for all trials
- Detailed trial information in subdirectories

## Troubleshooting

- **Memory Issues**: If you encounter memory errors, try reducing `--num-samples` or `--cpus-per-trial`
- **GPU Errors**: If you face GPU-related errors, try removing the `--gpu-fraction` option or installing the appropriate CUDA toolkit
- **Import Errors**: Ensure all dependencies are installed with `pip install -r requirements.txt`

## Advanced Usage

### Custom Search Space

To customize the hyperparameter search space, modify the `search_space` dictionary in `ray_tune_xgboost.py`.

### Integration with Existing Code

The `client_utils.py` file has been updated to automatically use tuned parameters when available. You can control this behavior by setting the `use_tuned_params` parameter when initializing the `XgbClient`.
</file>

<file path="ray_tune_xgboost_updated.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
Recent Fixes:
- Fixed Ray Tune worker file access issues by passing data directly to trial functions
  instead of trying to reload files from worker environments
- Ensured consistent preprocessing across hyperparameter tuning and federated learning phases
- Fixed processor path issues by using absolute paths for Ray Tune workers
- Added robust label column handling to work with different feature processor behaviors
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, log_loss
import logging
from ray.air.config import RunConfig
# Import existing data processing code
from dataset import load_csv_data, transform_dataset_to_dmatrix, create_global_feature_processor, load_global_feature_processor
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info(f"Starting trial with config: {config}")
    # Load the global feature processor instead of creating a new one
    processor_path = config.get('global_processor_path', 'outputs/global_feature_processor.pkl')
    try:
        processor = load_global_feature_processor(processor_path)
        logger.info("Using global feature processor for consistent preprocessing")
    except FileNotFoundError:
        logger.warning(f"Global processor not found at {processor_path}, creating new one")
        from dataset import FeatureProcessor
        processor = FeatureProcessor()
        processor.fit(train_df)
    # Transform data using the global processor
    train_processed = processor.transform(train_df, is_training=True)
    test_processed = processor.transform(test_df, is_training=False)
    # Debug: Check what columns are available after processing
    logger.info(f"Train processed columns: {list(train_processed.columns)}")
    logger.info(f"Test processed columns: {list(test_processed.columns)}")
    # Handle label column extraction more robustly
    if 'label' in train_processed.columns:
        train_features = train_processed.drop(columns=['label'])
        train_labels = train_processed['label'].astype(int)
    else:
        # If label column doesn't exist after processing, use original data
        logger.warning("Label column not found in processed data, using original labels")
        train_features = train_processed
        train_labels = train_df['label'].astype(int) if 'label' in train_df.columns else train_df['Label'].astype(int)
    if 'label' in test_processed.columns:
        test_features = test_processed.drop(columns=['label'])
        test_labels = test_processed['label'].astype(int)
    else:
        # If label column doesn't exist after processing, use original data
        logger.warning("Label column not found in processed test data, using original labels")
        test_features = test_processed
        test_labels = test_df['label'].astype(int) if 'label' in test_df.columns else test_df['Label'].astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred_proba = bst.predict(test_data)  # Get probabilities from multi:softprob
    y_pred_labels = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to predicted labels
    y_true = test_data.get_label()
    # Compute multi-class metrics using predicted labels
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file=None, test_file=None, data_file=None, num_samples=100, cpus_per_trial=1, gpu_fraction=None, output_dir="./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune with consistent preprocessing.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Step 1: Create global feature processor for consistent preprocessing
    logger.info("Creating global feature processor for consistent preprocessing...")
    if data_file:
        processor_path = create_global_feature_processor(data_file, output_dir)
    elif train_file:
        processor_path = create_global_feature_processor(train_file, output_dir)
    else:
        raise ValueError("Either data_file or train_file must be provided to create global processor")
    # Load and prepare data
    if train_file and test_file:
        logger.info(f"Loading training data from {train_file}")
        train_data = load_csv_data(train_file)["train"].to_pandas()
        logger.info(f"Loading testing data from {test_file}")
        test_data = load_csv_data(test_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        for df in [train_data]:
            if 'label' not in df.columns and 'Label' in df.columns:
                df['label'] = df['Label']
        # Check if test data has a label column
        if 'label' not in test_data.columns and 'Label' in test_data.columns:
            test_data['label'] = test_data['Label']
        # If test data doesn't have a label column, create a dummy one
        if 'label' not in test_data.columns:
            logger.warning("Test data doesn't have a label column. Creating a dummy label column with zeros.")
            test_data['label'] = 0
        # Load the global feature processor and process data
        processor = load_global_feature_processor(processor_path)
        # Process the data using the global processor
        train_processed = processor.transform(train_data, is_training=True)
        test_processed = processor.transform(test_data, is_training=False)
        # Extract features and labels
        if 'label' in train_processed.columns:
            train_features = train_processed.drop(columns=['label'])
            train_labels = train_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed training data, using original labels")
            train_features = train_processed
            train_labels = train_data['label'].astype(int) if 'label' in train_data.columns else train_data['Label'].astype(int)
        if 'label' in test_processed.columns:
            test_features = test_processed.drop(columns=['label'])
            test_labels = test_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed test data, using original labels")
            test_features = test_processed
            test_labels = test_data['label'].astype(int) if 'label' in test_data.columns else test_data['Label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    else:
        # Fall back to original behavior with single file and splitting
        logger.info(f"Loading data from {data_file}")
        data = load_csv_data(data_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        if 'label' not in data.columns and 'Label' in data.columns:
            data['label'] = data['Label']
        # Use temporal splitting to avoid data leakage
        from sklearn.model_selection import train_test_split as sklearn_split
        # Check if Stime column exists for temporal splitting
        if 'Stime' in data.columns:
            logger.info("Using temporal splitting based on Stime to avoid data leakage")
            data_sorted = data.sort_values('Stime').reset_index(drop=True)
            train_size = int(0.8 * len(data_sorted))
            train_split_orig = data_sorted.iloc[:train_size].copy()
            test_split_orig = data_sorted.iloc[train_size:].copy()
        else:
            logger.warning("No Stime column found, using random split")
            train_split_orig, test_split_orig = sklearn_split(data, test_size=0.2, random_state=42)
        # Load the global processor and transform data
        processor = load_global_feature_processor(processor_path)
        train_processed = processor.transform(train_split_orig, is_training=True)
        test_processed = processor.transform(test_split_orig, is_training=False)
        # Extract features and labels
        if 'label' in train_processed.columns:
            train_features = train_processed.drop(columns=['label'])
            train_labels = train_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed training data, using original labels from split")
            train_features = train_processed
            # Get labels from the original split data before processing
            train_labels = train_split_orig['label'].astype(int)
        if 'label' in test_processed.columns:
            test_features = test_processed.drop(columns=['label'])
            test_labels = test_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed test data, using original labels from split")
            test_features = test_processed
            # Get labels from the original split data before processing
            test_labels = test_split_orig['label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # Modified range for FL compatibility
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the original data DataFrames and processor path
    def _train_with_data_wrapper(config):
        # Add processor path to config - ensure it's absolute for Ray Tune workers
        config['global_processor_path'] = os.path.abspath(processor_path)
        # Pass copies of the original dataframes to the training function
        if train_file and test_file:
            return train_xgboost(config, train_data.copy(), test_data.copy())
        else:
            # For single data file mode, pass the original loaded data directly
            # instead of trying to reload it in the worker
            return train_xgboost(config, data.copy(), data.copy())
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200,
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        _train_with_data_wrapper,
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=RunConfig(
            local_dir=output_dir,
            name="xgboost_tune"
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info(f"  mlogloss: {best_metrics['mlogloss']:.4f}")
    logger.info(f"  merror: {best_metrics['merror']:.4f}")
    logger.info(f"  precision: {best_metrics['precision']:.4f}")
    logger.info(f"  recall: {best_metrics['recall']:.4f}")
    logger.info(f"  f1: {best_metrics['f1']:.4f}")
    logger.info(f"  accuracy: {best_metrics['accuracy']:.4f}")
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    with open(best_params_file, 'w') as f:
        json.dump(best_config, f, indent=2)
    logger.info(f"Best parameters saved to {best_params_file}")
    # Train a final model with the best parameters
    train_final_model(best_config, train_features, train_labels, test_features, test_labels, output_dir)
    return best_config
def train_final_model(config, train_features, train_labels, test_features, test_labels, output_dir):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info(f"Final model saved to {model_path}")
    # Evaluate the model
    y_pred_proba = final_model.predict(test_data)  # Get probabilities from multi:softprob
    y_pred_labels = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to predicted labels
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Log final performance
    logger.info("Final model performance:")
    logger.info(f"  Precision: {precision:.4f}")
    logger.info(f"  Recall: {recall:.4f}")
    logger.info(f"  F1 Score: {f1:.4f}")
    logger.info(f"  Accuracy: {accuracy:.4f}")
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Ray Tune for XGBoost hyperparameter optimization")
    parser.add_argument("--data-file", type=str, help="Path to single CSV data file (optional if train and test files are provided)")
    parser.add_argument("--train-file", type=str, help="Path to training CSV data file")
    parser.add_argument("--test-file", type=str, help="Path to testing CSV data file")
    parser.add_argument("--num-samples", type=int, default=10, help="Number of hyperparameter combinations to try")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="CPUs per trial")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="GPU fraction per trial (0.1 for 10%)")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Output directory for results")
    args = parser.parse_args()
    # Validate arguments
    if not args.data_file and not (args.train_file and args.test_file):
        parser.error("Either --data-file or both --train-file and --test-file must be provided")
    # Run the hyperparameter tuning
    tune_xgboost(
        train_file=args.train_file,
        test_file=args.test_file,
        data_file=args.data_file,
        num_samples=args.num_samples,
        cpus_per_trial=args.cpus_per_trial,
        gpu_fraction=args.gpu_fraction,
        output_dir=args.output_dir
    )
if __name__ == "__main__":
    main()
</file>

<file path="ray_tune_xgboost.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import logging
from ray import air
# Import existing data processing code
from dataset import load_csv_data, preprocess_data, FeatureProcessor
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info("Starting trial with config: %s", config)
    # Use preprocess_data to get features and encoded labels
    processor = FeatureProcessor()
    train_features, train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    test_features, test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle cases where test data might be unlabeled
    if test_labels is None:
        logger.warning("Test data has no labels. Creating dummy labels for DMatrix.")
        test_labels = np.zeros(len(test_features))
    else:
        test_labels = test_labels.astype(int) # Ensure labels are integers
    # Ensure train labels are integers
    train_labels = train_labels.astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    # FeatureProcessor already handles feature types and drops unnecessary columns
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred_proba = bst.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Compute multi-class metrics using predicted labels
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file: str, test_file: str, num_samples: int = 100, cpus_per_trial: int = 1, gpu_fraction: float = None, output_dir: str = "./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune.
    Args:
        train_file (str): Path to the training CSV data file.
        test_file (str): Path to the testing CSV data file.
        num_samples (int): Number of hyperparameter combinations to try.
        cpus_per_trial (int): Number of CPUs to allocate per trial.
        gpu_fraction (float): Fraction of GPU to use per trial (if None, no GPU is used).
        output_dir (str): Directory to save results.
    Returns:
        dict: Best hyperparameters found.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Load and prepare data *once* before tuning starts
    logger.info("Loading training data from %s", train_file)
    train_df = load_csv_data(train_file)["train"].to_pandas()
    logger.info("Loading testing data from %s", test_file)
    test_df = load_csv_data(test_file)["train"].to_pandas() # Use train split of test file for validation
    # Preprocess data *once* to get features/labels for the final model training
    # and to fit the processor
    logger.info("Preprocessing data for final model training and fitting processor...")
    processor = FeatureProcessor()
    # We need the original dataframes (train_df, test_df) to pass to the trial wrapper
    # But we also need the processed features/labels for the final model training step
    final_train_features, final_train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    final_test_features, final_test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle potential missing labels in the test set for final model evaluation
    if final_test_labels is None:
        logger.warning("Test data has no labels. Using zeros for final model evaluation.")
        final_test_labels = np.zeros(len(final_test_features))
    else:
        final_test_labels = final_test_labels.astype(int) # Ensure labels are integers
    final_train_labels = final_train_labels.astype(int)
    logger.info("Training data size for final model: %d", len(final_train_features))
    logger.info("Validation data size for final model: %d", len(final_test_features))
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # Modified range for FL compatibility
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        # NOTE: Ray Tune handles GPU allocation via resources_per_trial typically
        # Setting tree_method here might be sufficient, but review Ray Tune docs if issues persist
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the *original* data DataFrames
    # The train_xgboost function inside the trial will handle preprocessing
    def _train_with_data_wrapper(config):
        # Pass copies of the original dataframes to the training function
        return train_xgboost(config, train_df.copy(), test_df.copy()) 
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200, # Corresponds roughly to num_boost_round max
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Wrap trainable with resource specification
    trainable_with_resources = tune.with_resources(
        _train_with_data_wrapper, 
        resources={"cpu": cpus_per_trial, "gpu": gpu_fraction if gpu_fraction else 0}
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        trainable_with_resources, # Use wrapped trainable
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=air.RunConfig( # Use air.RunConfig
            local_dir=output_dir,
            name="xgboost_tune",
            # resources_per_trial is handled by tune.with_resources
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info("  mlogloss: %.4f", best_metrics['mlogloss'])
    logger.info("  merror: %.4f", best_metrics['merror'])
    logger.info("  precision: %.4f", best_metrics['precision'])
    logger.info("  recall: %.4f", best_metrics['recall'])
    logger.info("  f1: %.4f", best_metrics['f1'])
    logger.info("  accuracy: %.4f", best_metrics['accuracy'])
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    # Use utf-8 encoding when writing JSON
    with open(best_params_file, 'w', encoding='utf-8') as f:
        json.dump(best_config, f, indent=2)
    logger.info("Best parameters saved to %s", best_params_file)
    # Train a final model with the best parameters using the preprocessed data
    train_final_model(best_config, final_train_features, final_train_labels, final_test_features, final_test_labels, output_dir)
    return best_config
def train_final_model(config: dict, 
                      train_features: pd.DataFrame, train_labels: pd.Series, 
                      test_features: pd.DataFrame, test_labels: pd.Series, 
                      output_dir: str):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels (encoded)
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels (encoded)
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True # Show progress for final model
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info("Final model saved to %s", model_path)
    # Evaluate the model
    y_pred_proba = final_model.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Log final performance
    logger.info("Final model performance:")
    logger.info("  Precision: %.4f", precision)
    logger.info("  Recall: %.4f", recall)
    logger.info("  F1 Score: %.4f", f1)
    logger.info("  Accuracy: %.4f", accuracy)
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="XGBoost Hyperparameter Tuning with Ray Tune")
    parser.add_argument("--train-file", type=str, required=True, help="Path to the training data CSV file.")
    parser.add_argument("--test-file", type=str, required=True, help="Path to the testing data CSV file.")
    parser.add_argument("--num-samples", type=int, default=100, help="Number of hyperparameter samples to try.")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="Number of CPUs to allocate per trial.")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="Fraction of GPU resources per trial (e.g., 0.5). Default is None (CPU only).")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Directory to save Ray Tune results and best parameters.")
    args = parser.parse_args()
    logger.info("===== XGBoost Hyperparameter Tuning with Ray Tune ====")
    logger.info("Training file: %s", args.train_file)
    logger.info("Testing file: %s", args.test_file)
    logger.info("Number of hyperparameter samples: %d", args.num_samples)
    logger.info("CPUs per trial: %d", args.cpus_per_trial)
    logger.info("GPU: %s", f"{args.gpu_fraction * 100}% per trial" if args.gpu_fraction else "Not used")
    logger.info("Output directory: %s", args.output_dir)
    logger.info("=======================================================")
    try:
        # Run the tuning process
        best_params = tune_xgboost(
            train_file=args.train_file,
            test_file=args.test_file,
            num_samples=args.num_samples,
            cpus_per_trial=args.cpus_per_trial,
            gpu_fraction=args.gpu_fraction,
            output_dir=args.output_dir
        )
        # Train the final model with the best hyperparameters
        # Load data again for final training (could be optimized if memory is a concern)
        train_df_final = load_csv_data(args.train_file)["train"].to_pandas()
        test_df_final = load_csv_data(args.test_file)["train"].to_pandas()
        # Preprocess data using the same processor logic used during tuning
        processor_final = FeatureProcessor()
        final_train_features, final_train_labels = preprocess_data(train_df_final, processor=processor_final, is_training=True)
        final_test_features, final_test_labels = preprocess_data(test_df_final, processor=processor_final, is_training=False)
        # Ensure labels are integers
        final_train_labels = final_train_labels.astype(int)
        if final_test_labels is not None:
            final_test_labels = final_test_labels.astype(int)
        else:
            # Handle case where test set might still be unlabeled after tuning run
            final_test_labels = np.zeros(len(final_test_features)) # Dummy labels if needed
        train_final_model(
            config=best_params, 
            train_features=final_train_features, train_labels=final_train_labels, 
            test_features=final_test_features, test_labels=final_test_labels, 
            output_dir=args.output_dir
        )
        logger.info("===== Hyperparameter tuning finished successfully ====")
    except Exception as e:
        logger.error("Hyperparameter tuning failed: %s", str(e), exc_info=True)
        print("===== Hyperparameter tuning failed ====") # Also print to stderr for visibility in GH Actions
if __name__ == "__main__":
    main()
</file>

<file path="README.md">
# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## ðŸ› ï¸ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture â†’ 2. Clean Capture and Convert to Dataset â†’ 3. Train/Test â†’ 4.ï¸Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## ðŸ—ºï¸ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## ðŸŽ¯ What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **ðŸ“š Table of Contents**
- [âœ¨ Features](#-features)  
- [ðŸ“‚ Project Structure](#-project-structure)  
- [ðŸš€ Getting Started](#-getting-started)  
- [âš™ï¸ Configuration](#-configuration)
- [ðŸ“‚ Output Structure](#-output-structure)
- [ðŸ§ª Running Experiments](#-running-experiments)  
- [âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## âœ¨ Features  
âœ… **Privacy-Preserving Training** - Federated learning implementation with data isolation  
âœ… **Flexible Configuration** - Hydra-powered experiment management  
âœ… **Reproducible Experiments** - âš ï¸Automatic output organization   
âœ… **CI/CD Integration** - GitHub Actions workflow with CML reporting  
âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **ðŸ“‚ Project Structure**
```bash
â”œâ”€â”€ github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cml.yaml      # CI/CD workflow definition
â”œâ”€â”€ pyecache/             # Python cache directory
â”œâ”€â”€ data/                 # Dataset files, data capture script, and data cleaning script
â”œ   â””â”€â”€ received/         # Data from Zeek/Kafka stream
â”œâ”€â”€ outputs/              # Model, Predictions, Eval; Output files
â”œâ”€â”€ results/              # Latest aggregated metrics
â”œâ”€â”€ client.py             # Flower client logic
â”œâ”€â”€ client_utils.py       # Client helper functions
â”œâ”€â”€ dataset.py            # Data loading/preprocessing
â”œâ”€â”€ poetry.lock           # Poetry dependency lockfile - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ pyproject.toml        # Poetry project configuration - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - ðŸš§ under construction (implementation phase) ðŸš§
â”œâ”€â”€ run_bagging.sh        # Bagging experiment script - runs script.py + client.py
â”œâ”€â”€ run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
â”œâ”€â”€ server.py             # Flower server logic
â”œâ”€â”€ server_utils.py       # Server helper functions
â”œâ”€â”€ sim.py                # Start simulation - âš ï¸ deprecated soon âš ï¸
â”œâ”€â”€ utils.py              # Shared utilities
â””â”€â”€ README.md             # Project documentation
```

---

## **ðŸš€ Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **âš ï¸âš™ï¸ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **âš ðŸ“‚ Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
â””â”€â”€ YYYY-MM-DD/                  # Run date
    â””â”€â”€ HH-MM-SS/                # Run time
        â”œâ”€â”€ .hydra/              # âš ï¸Config snapshots
        â”‚   â”œâ”€â”€ config.yaml
        â”‚   â””â”€â”€ hydra.yaml
        â”œâ”€â”€ results.pkl          # Training history
        â”œâ”€â”€ predictions/         # Model predictions
            â”œâ”€â”€ predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **ðŸ§ª Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## ðŸ”„ **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## ðŸŽ’ **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## ðŸ“Š Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
<div align="center">

## ðŸ”¥ **R&D Led By** ðŸ”¥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  ðŸ§‘ðŸ’» Coded with 100% chaos-driven curiosity    !
!  â˜• Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
ðŸ” Cyber Alchemy Brewing For ðŸ›ï¸ Indiana University of Pennsylvania's ARMZTA Project

ðŸ”— https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
</file>

<file path="requirements.txt">
flwr[simulation]>=1.7.0, <2.0
flwr-datasets>=0.2.0, <1.0.0
xgboost>=2.0.0, <3.0.0
ray[tune]>=2.9.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.2.0
matplotlib
seaborn
</file>

<file path="run_bagging copy.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=2 --num-rounds=20 --num-clients-per-round=2 &
sleep 30  # Sleep for 30s to give the server enough time to start
for i in `seq 0 1`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --num-partitions=2 --partitioner-type=exponential &
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_bagging.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
# Ensure the global feature processor is created before starting server and clients
# This guarantees consistent preprocessing across all clients and the server
echo "Step 1: Ensuring global feature processor is created..."
python create_global_processor.py \
    --data-file "data/received/final_dataset.csv" \
    --output-dir "outputs" \
    --force
if [ $? -ne 0 ]; then
    echo "Error creating global feature processor. Exiting."
    exit 1
fi
echo "âœ“ Global feature processor ready at outputs/global_feature_processor.pkl"
echo ""
echo "Step 2: Starting federated learning server..."
python3 server.py --pool-size=5 --num-rounds=5 --num-clients-per-round=5 --centralised-eval &
SERVER_PID=$!
sleep 30  # Sleep for 30s to give the server enough time to start
echo "Step 3: Starting federated learning clients..."
# Start regular client (partition 0)
echo "Starting regular client (partition 0)"
python3 client.py --partition-id=0 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[0]=$!
# Start regular client (partition 1)
echo "Starting regular client (partition 1)"
python3 client.py --partition-id=1 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[1]=$!
# Start regular client (partition 2)
echo "Starting regular client (partition 2)"
python3 client.py --partition-id=2 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[2]=$!
# Start regular client (partition 3)
echo "Starting regular client (partition 3)"
python3 client.py --partition-id=3 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[3]=$!
# Start regular client (partition 4)
echo "Starting regular client (partition 4)"
python3 client.py --partition-id=4 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[4]=$!
echo "All clients started. Waiting for federated learning to complete..."
# Enable CTRL+C to stop all background processes
trap "echo 'Stopping all processes...'; kill -- -$$" SIGINT SIGTERM
# Wait for server to complete (it will finish after all rounds)
wait $SERVER_PID
# Wait for all client processes to complete
for pid in "${CLIENT_PIDS[@]}"; do
    if kill -0 $pid 2>/dev/null; then
        wait $pid
    fi
done
echo ""
echo "âœ“ Federated learning completed successfully!"
echo "Results saved to: outputs/"
</file>

<file path="run_cyclic.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --train-method=cyclic --pool-size=20 --num-rounds=10 &
sleep 30  # Sleep for 15s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --train-method=cyclic --num-partitions=5 --partitioner-type=uniform &
    sleep 5
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_ray_tune.sh">
#!/bin/bash
# Script to run Ray Tune for XGBoost hyperparameter optimization
# with consistent preprocessing to fix disconnection between tuning and FL phases
# Default values
DATA_FILE="data/received/final_dataset.csv"
NUM_SAMPLES=50
CPUS_PER_TRIAL=2
OUTPUT_DIR="./tune_results"
TRAIN_FILE=""
TEST_FILE=""
# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --data-file)
            DATA_FILE="$2"
            shift 2
            ;;
        --train-file)
            TRAIN_FILE="$2"
            shift 2
            ;;
        --test-file)
            TEST_FILE="$2"
            shift 2
            ;;
        --num-samples)
            NUM_SAMPLES="$2"
            shift 2
            ;;
        --cpus-per-trial)
            CPUS_PER_TRIAL="$2"
            shift 2
            ;;
        --output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        *)
            echo "Unknown argument: $1"
            shift
            ;;
    esac
done
echo "Starting Ray Tune XGBoost hyperparameter optimization with consistent preprocessing..."
# Step 1: Create global feature processor for consistent preprocessing
echo "Step 1: Creating global feature processor..."
# Determine which data file to use for the global processor
PROCESSOR_DATA_FILE="$DATA_FILE"
if [[ -n "$TRAIN_FILE" && -f "$TRAIN_FILE" ]]; then
    PROCESSOR_DATA_FILE="$TRAIN_FILE"
fi
python create_global_processor.py \
    --data-file "$PROCESSOR_DATA_FILE" \
    --output-dir "outputs" \
    --force
if [ $? -ne 0 ]; then
    echo "Failed to create global feature processor. Exiting."
    exit 1
fi
echo "Global feature processor created successfully."
# Step 2: Run Ray Tune with the updated script
echo "Step 2: Running Ray Tune optimization..."
# Build the Python command based on available files
PYTHON_CMD="python ray_tune_xgboost_updated.py"
PYTHON_CMD="$PYTHON_CMD --num-samples $NUM_SAMPLES"
PYTHON_CMD="$PYTHON_CMD --cpus-per-trial $CPUS_PER_TRIAL"
PYTHON_CMD="$PYTHON_CMD --output-dir \"$OUTPUT_DIR\""
if [[ -n "$TRAIN_FILE" && -n "$TEST_FILE" && -f "$TRAIN_FILE" && -f "$TEST_FILE" ]]; then
    echo "Using separate train and test files"
    PYTHON_CMD="$PYTHON_CMD --train-file \"$TRAIN_FILE\" --test-file \"$TEST_FILE\""
else
    echo "Using single data file: $DATA_FILE"
    PYTHON_CMD="$PYTHON_CMD --data-file \"$DATA_FILE\""
fi
echo "Executing: $PYTHON_CMD"
eval $PYTHON_CMD
if [ $? -ne 0 ]; then
    echo "Ray Tune optimization failed. Exiting."
    exit 1
fi
echo "Ray Tune optimization completed successfully."
# Step 3: Generate updated parameters file
echo "Step 3: Generating tuned parameters file..."
python use_tuned_params.py
if [ $? -ne 0 ]; then
    echo "Failed to generate tuned parameters. Continuing anyway."
fi
echo "Ray Tune with consistent preprocessing completed!"
echo "Global feature processor is available at: outputs/global_feature_processor.pkl"
echo "Tuned parameters are available at: $OUTPUT_DIR/best_params.json"
</file>

<file path="run.py">
#!/usr/bin/env python3
"""
Main script to run the federated learning pipeline with consistent preprocessing.
This script ensures that:
1. A global feature processor is created for consistent preprocessing
2. Ray Tune hyperparameter optimization uses this global processor
3. Federated learning uses the same global processor
4. All phases maintain preprocessing consistency
"""
import subprocess
import sys
from flwr.common.logger import log
from logging import INFO
def run_command(command, description):
    """Run a command and handle errors."""
    log(INFO, "Running: %s", description)
    log(INFO, "Command: %s", " ".join(command))
    try:
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        log(INFO, "âœ“ %s completed successfully", description)
        if result.stdout:
            log(INFO, "Output: %s", result.stdout.strip())
        return True
    except subprocess.CalledProcessError as e:
        log(INFO, "âœ— %s failed with exit code %d", description, e.returncode)
        if e.stdout:
            log(INFO, "Stdout: %s", e.stdout.strip())
        if e.stderr:
            log(INFO, "Stderr: %s", e.stderr.strip())
        return False
def main():
    """Main execution pipeline."""
    log(INFO, "Starting Federated Learning Pipeline with Consistent Preprocessing")
    log(INFO, "=" * 80)
    # Step 1: Create global feature processor
    log(INFO, "Step 1: Creating global feature processor for consistent preprocessing")
    if not run_command([
        "python", "create_global_processor.py",
        "--data-file", "data/received/final_dataset.csv",
        "--output-dir", "outputs",
        "--force"
    ], "Global feature processor creation"):
        log(INFO, "Failed to create global feature processor. Exiting.")
        sys.exit(1)
    # Step 2: Run hyperparameter tuning with consistent preprocessing
    log(INFO, "Step 2: Running hyperparameter tuning with consistent preprocessing")
    if not run_command([
        "python", "ray_tune_xgboost_updated.py",
        "--data-file", "data/received/final_dataset.csv",
        "--num-samples", "30",
        "--cpus-per-trial", "2",
        "--output-dir", "./tune_results"
    ], "Ray Tune hyperparameter optimization"):
        log(INFO, "Hyperparameter tuning failed. Continuing with default parameters.")
    # Step 3: Generate tuned parameters file
    log(INFO, "Step 3: Generating tuned parameters file")
    if not run_command([
        "python", "use_tuned_params.py"
    ], "Tuned parameters generation"):
        log(INFO, "Failed to generate tuned parameters. Using default parameters.")
    # Step 4: Run federated learning simulation
    log(INFO, "Step 4: Starting federated learning simulation")
    if not run_command([
        "python", "sim.py",
        "--train-method", "bagging",
        "--pool-size", "5",
        "--num-rounds", "5",
        "--num-clients-per-round", "5",
        "--centralised-eval",
        "--csv-file", "data/received/final_dataset.csv"
    ], "Federated learning simulation"):
        log(INFO, "Federated learning simulation failed.")
        sys.exit(1)
    log(INFO, "=" * 80)
    log(INFO, "Federated Learning Pipeline completed successfully!")
    log(INFO, "Key improvements:")
    log(INFO, "âœ“ Consistent preprocessing across all phases")
    log(INFO, "âœ“ Temporal splitting to prevent data leakage")
    log(INFO, "âœ“ Global feature processor for uniform data representation")
    log(INFO, "âœ“ Tuned hyperparameters applied to federated learning")
if __name__ == "__main__":
    main()
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, log_loss, accuracy_score
from logging import INFO, WARNING
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
import numpy as np
# Assuming visualization_utils.py is in the same directory or accessible via PYTHONPATH
from visualization_utils import (
    plot_confusion_matrix,
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution
)
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    # aggregated_metrics["loss"] = loss  # REMOVED - Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["mlogloss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    if not (isinstance(loss, (int, float)) and isinstance(aggregated_metrics, dict)):
        log(INFO, "[ERROR] Output of evaluate_metrics_aggregation is not (loss, dict): %s, %s", type(loss), type(aggregated_metrics))
        raise TypeError("evaluate_metrics_aggregation must return (loss, dict)")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices or array of probabilities)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'Normal', 'Reconnaissance', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Check if predictions is a 2D array (multi-class probabilities)
    if isinstance(predictions, np.ndarray) and len(predictions.shape) > 1:
        log(INFO, "Detected multi-class probability predictions with shape: %s", predictions.shape)
        # Convert probabilities to class labels
        predicted_labels = np.argmax(predictions, axis=1)
    else:
        # Already a list of class indices
        predicted_labels = predictions
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predicted_labels,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Default mapping for UNSW_NB15 multi-class predictions
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic',
            10: 'Class_10'  # Added for the engineered dataset which has 11 classes
        }
        # Safely convert predictions to integers
        predictions_dict['prediction_type'] = [label_mapping.get(int(label), f'unknown_{label}') for label in predicted_labels]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
        # Generate and save visualizations if we have true labels to compare with
        try:
            class_names = list(label_mapping.values())
            num_classes = len(class_names)
            # Convert to numpy arrays if they're not already
            y_true = np.array(true_labels) if not isinstance(true_labels, np.ndarray) else true_labels
            y_pred = np.array(predicted_labels) if not isinstance(predicted_labels, np.ndarray) else predicted_labels
            # Create confusion matrix
            cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{round_num}.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{round_num}.png")
            plot_class_distribution(y_true, y_pred, class_names, dist_path)
            log(INFO, f"Visualizations saved for round {round_num}")
        except Exception as e:
            log(WARNING, f"Error generating visualizations: {e}")
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions shape: %s", raw_predictions.shape if hasattr(raw_predictions, 'shape') else 'scalar')
    # Log distribution of scores
    if hasattr(raw_predictions, 'shape'):
        log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
            np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # For multi-class with multi:softprob, the raw predictions will be probabilities for each class
    if hasattr(raw_predictions, 'shape') and len(raw_predictions.shape) > 1:
        log(INFO, "Processing multi-class probability predictions with shape: %s", raw_predictions.shape)
        predicted_labels = np.argmax(raw_predictions, axis=1)
        # Default mapping for the engineered dataset
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic',
            10: 'Class_10'  # Added for the engineered dataset which has 11 classes
        }
        # Save predictions to CSV with class names
        predictions_df = pd.DataFrame({
            'predicted_label': predicted_labels,
            'prediction_type': [label_mapping.get(int(p), f'unknown_{p}') for p in predicted_labels],
        })
        # Add probability columns for each class
        for i in range(raw_predictions.shape[1]):
            predictions_df[f'prob_class_{i}'] = raw_predictions[:, i]
    elif len(raw_predictions.shape) == 1:  # Binary case or multi:softmax
        # Check if this is binary classification or multi-class with direct labels
        if np.max(raw_predictions) <= 1.0 and np.min(raw_predictions) >= 0.0:
            # Binary case
            probabilities = raw_predictions  # Already probabilities
            predicted_labels = (probabilities >= 0.5).astype(int)
            # Save predictions to CSV
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
                'prediction_score': probabilities
            })
        else:
            # Likely multi:softmax with direct class labels
            predicted_labels = np.round(raw_predictions).astype(int)
            # Default mapping for the engineered dataset
            label_mapping = {
                0: 'Normal', 
                1: 'Reconnaissance', 
                2: 'Backdoor', 
                3: 'DoS', 
                4: 'Exploits', 
                5: 'Analysis', 
                6: 'Fuzzers', 
                7: 'Worms', 
                8: 'Shellcode', 
                9: 'Generic',
                10: 'Class_10'  # Added for the engineered dataset which has 11 classes
            }
            # Save predictions to CSV with class names
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': [label_mapping.get(int(p), f'unknown_{p}') for p in predicted_labels],
            })
    else:
        # Fallback for unexpected prediction format
        log(WARNING, "Unexpected prediction format. Creating basic predictions DataFrame.")
        predictions_df = pd.DataFrame({
            'raw_prediction': raw_predictions
        })
    # Log predicted class distribution
    if 'predicted_label' in predictions_df.columns:
        unique, counts = np.unique(predictions_df['predicted_label'], return_counts=True)
        log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Generate visualizations if true labels are available
    try:
        true_labels = dmatrix.get_label()
        if true_labels is not None and 'predicted_label' in predictions_df.columns:
            output_dir = os.path.dirname(output_path)
            num_classes = max(11, np.max(true_labels) + 1)  # Ensure at least 11 classes for the engineered dataset
            class_names = [label_mapping.get(i, f'Class_{i}') for i in range(num_classes)]
            predicted_labels = predictions_df['predicted_label'].values
            # Create confusion matrix
            cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))
            cm_path = os.path.join(output_dir, "final_confusion_matrix.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, "final_class_distribution.png")
            plot_class_distribution(true_labels, predicted_labels, class_names, dist_path)
            # Plot ROC and Precision-Recall Curves (for multi-class)
            if len(raw_predictions.shape) > 1 and raw_predictions.shape[1] >= num_classes:
                roc_path = os.path.join(output_dir, "final_roc_curves.png")
                plot_roc_curves(true_labels, raw_predictions, class_names, roc_path)
                pr_path = os.path.join(output_dir, "final_precision_recall_curves.png")
                plot_precision_recall_curves(true_labels, raw_predictions, class_names, pr_path)
            log(INFO, "Visualizations saved with final model predictions")
    except Exception as e:
        log(WARNING, f"Error generating visualizations: {e}")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_model(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Get predictions
            y_pred_proba = bst.predict(test_data)
            # For multi:softprob, we get probabilities for each class
            # Convert to labels by taking argmax if predictions are probabilities
            if isinstance(y_pred_proba, np.ndarray) and len(y_pred_proba.shape) > 1:
                y_pred_labels = np.argmax(y_pred_proba, axis=1)
                log(INFO, "Converting probability predictions to labels (argmax), shape: %s", y_pred_proba.shape)
            else:
                y_pred_labels = y_pred_proba  # Already labels
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_proba, server_round, "results", y_true)
            # Compute metrics using the predictions
            predictions = y_pred_labels  # Use the converted labels for metrics
            pred_proba = y_pred_proba    # The original probabilities for plots that need them
            # Ensure pred_proba has the correct shape for multi-class
            if len(pred_proba.shape) == 1 or pred_proba.shape[1] == 1:
                 # If predict gives labels or single class proba, try predict_proba if available
                 try:
                     # Note: XGBoost predict() with multi:softmax directly gives labels.
                     # To get probabilities, the objective might need to be multi:softprob
                     log(WARNING, "Predict output seems 1D, attempting to handle for multi-class probability plots...")
                     if BST_PARAMS.get('objective') == 'multi:softmax':
                         # Create dummy probabilities centered around the predicted class
                         num_classes = BST_PARAMS.get('num_class', 11) # Default to 11 if not set
                         pred_proba = np.zeros((len(predictions), num_classes))
                         for i, label in enumerate(predictions):
                             if 0 <= int(label) < num_classes: # Check bounds
                                pred_proba[i, int(label)] = 0.9 # Assign high prob to predicted
                                other_prob = 0.1 / max(1, (num_classes - 1))
                                for j in range(num_classes):
                                    if j != int(label):
                                        pred_proba[i,j] = other_prob
                             else:
                                 log(WARNING, f"Prediction label {label} out of bounds [0, {num_classes-1}]")
                                 # Assign uniform probability as fallback if label is invalid
                                 pred_proba[i, :] = 1.0 / num_classes
                         log(WARNING, "Reconstructed dummy probabilities for multi:softmax. Plots may be inaccurate. Consider using 'multi:softprob' objective for better probability estimates.")
                     else: # Cannot determine probabilities
                         pred_proba = None
                 except AttributeError:
                     log(WARNING, "Could not get probabilities, ROC and PR curves will not be generated.")
                     pred_proba = None
                 except Exception as e:
                     log(WARNING, f"Error processing probabilities: {e}. ROC/PR plots skipped.")
                     pred_proba = None
            elif pred_proba.shape[1] != BST_PARAMS.get('num_class', 11):
                 log(WARNING, f"Probability shape mismatch ({pred_proba.shape[1]} columns vs {BST_PARAMS.get('num_class', 11)} classes). Plots may fail.")
                 # Attempt to proceed, but plots requiring probabilities might error out
            # Calculate metrics
            # Ensure y_test is integer type for log_loss if using one-hot encoding
            y_test_int = y_true.astype(int)
            num_classes_actual = BST_PARAMS.get('num_class', 11)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                 try:
                     loss = log_loss(y_test_int, pred_proba, eps=1e-15, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"pred_proba shape: {pred_proba.shape}")
                     log(WARNING, f"pred_proba sample: {pred_proba[:5]}")
            else:
                 log(WARNING, "Calculating log_loss using one-hot encoding due to missing/invalid probabilities.")
                 try:
                     predictions_int = np.array(predictions).astype(int)
                     loss = log_loss(y_test_int, np.eye(num_classes_actual)[predictions_int], eps=1e-15, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during one-hot log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"predictions unique: {np.unique(predictions)}, shape: {predictions.shape}")
            accuracy = accuracy_score(y_true, predictions)
            precision = precision_score(y_true, predictions, average='weighted', zero_division=0)
            recall = recall_score(y_true, predictions, average='weighted', zero_division=0)
            f1 = f1_score(y_true, predictions, average='weighted', zero_division=0)
            cm = confusion_matrix(y_true, predictions, labels=range(num_classes_actual)) # Ensure labels match num_classes
            log(INFO, f"Centralized eval round {server_round} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            # --- Generate and Save Plots ---
            output_dir = config.get("output_dir", "results") # Get output dir from config or default
            # Instead of creating a separate plots directory, save directly in output_dir
            log(INFO, f"Saving evaluation plots to: {output_dir}")
            # Update class names to include the 11th class
            class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic', 'Class_10'] 
            # Plot Confusion Matrix
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{server_round}.png")
            plot_confusion_matrix(cm, class_names[:num_classes_actual], cm_path) # Use actual num_classes
            # Plot Class Distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{server_round}.png")
            plot_class_distribution(y_test_int, predictions.astype(int), class_names[:num_classes_actual], dist_path)
            # Plot ROC and Precision-Recall Curves (only if probabilities are available and valid)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                roc_path = os.path.join(output_dir, f"roc_curves_round_{server_round}.png")
                plot_roc_curves(y_test_int, pred_proba, class_names[:num_classes_actual], roc_path)
                pr_path = os.path.join(output_dir, f"precision_recall_curves_round_{server_round}.png")
                plot_precision_recall_curves(y_test_int, pred_proba, class_names[:num_classes_actual], pr_path)
            else:
                 log(WARNING, f"Skipping ROC and PR curve generation due to unavailable/invalid probabilities (shape: {pred_proba.shape if pred_proba is not None else 'None'}).")
            # --- End Plot Generation ---
            # Return metrics
            return loss, {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}
    return evaluate_model
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO, WARNING
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data, FeatureProcessor, create_global_feature_processor, load_global_feature_processor
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Create global feature processor before starting federated learning
log(INFO, "Creating global feature processor for consistent preprocessing across all clients...")
test_csv_path = "data/received/final_dataset.csv"
global_processor_path = create_global_feature_processor(test_csv_path, output_dir)
global_processor = load_global_feature_processor(global_processor_path)
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    # Use the engineered dataset for testing
    log(INFO, "Using original final dataset with temporal splitting for centralized evaluation: %s", test_csv_path)
    test_set = load_csv_data(test_csv_path)["test"]
    test_set.set_format("pandas")
    test_df = test_set.to_pandas()
    # Use the global processor for consistent evaluation
    log(INFO, "Using global feature processor for centralized evaluation")
    # Transform to DMatrix with the global processor
    test_dmatrix = transform_dataset_to_dmatrix(
        test_df, 
        processor=global_processor,
        is_training=False
    )
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
class CustomFedXgbBagging(FedXgbBagging):
    def aggregate_evaluate(self, server_round, results, failures):
        if self.evaluate_metrics_aggregation_fn is not None:
            eval_metrics = []
            for r in results:
                # Case 1: Object with num_examples and metrics
                if hasattr(r, "num_examples") and hasattr(r, "metrics"):
                    eval_metrics.append((r.num_examples, r.metrics))
                # Case 2: Tuple of (num_examples, metrics_dict)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and isinstance(r[0], (int, float))
                    and isinstance(r[1], dict)
                ):
                    eval_metrics.append(r)
                # Case 3: Tuple of (client_proxy, EvaluateRes)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and hasattr(r[1], "num_examples")
                    and hasattr(r[1], "metrics")
                ):
                    eval_metrics.append((r[1].num_examples, r[1].metrics))
                else:
                    raise TypeError(
                        f"aggregate_evaluate: Unexpected result format: {type(r)}, value: {r}"
                    )
            aggregated_result = self.evaluate_metrics_aggregation_fn(eval_metrics)
            if not (isinstance(aggregated_result, tuple) and len(aggregated_result) == 2):
                raise TypeError("aggregate_evaluate must return (loss, dict)")
            loss, metrics = aggregated_result
            if not isinstance(metrics, dict):
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        return super().aggregate_evaluate(server_round, results, failures)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = CustomFedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        log(INFO, "[DEBUG] aggregate_evaluate received aggregated_result type: %s, value: %s", type(aggregated_result), aggregated_result)
        # Expect (loss, metrics_dict)
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "[ERROR] Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        log(INFO, "[ERROR] Unexpected format from aggregate_evaluate: %s", type(aggregated_result))
        raise TypeError("aggregate_evaluate must return (loss, dict)")
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add distributed losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["losses_distributed"] = history.losses_distributed
else:
    results["losses_distributed"] = []
    log(INFO, "No distributed losses found in history")
# Add centralized losses if available
if hasattr(history, 'losses_centralized') and history.losses_centralized:
    results["losses_centralized"] = history.losses_centralized
else:
    results["losses_centralized"] = []
    log(INFO, "No centralized losses found in history")
# Add distributed metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics_distributed"] = history.metrics_distributed
else:
    results["metrics_distributed"] = {}
    log(INFO, "No distributed metrics found in history")
# Add centralized metrics if available
if hasattr(history, 'metrics_centralized') and history.metrics_centralized:
    results["metrics_centralized"] = history.metrics_centralized
else:
    results["metrics_centralized"] = {}
    log(INFO, "No centralized metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
log(INFO, "Generating additional visualizations...")
# Define CLASS_NAMES based on the engineered dataset labels found during preprocessing
# This is a placeholder - we'll use generic labels since engineered dataset has numeric classes
CLASS_NAMES = [str(i) for i in range(11)]  # Using generic labels 0-10 as a fallback
# Check if we can get real class names from the processor
if centralised_eval and hasattr(global_processor, 'unique_labels'):
    CLASS_NAMES = [str(label) for label in global_processor.unique_labels]
    log(INFO, "Using class names from engineered dataset: %s", CLASS_NAMES)
# Import visualization functions and other necessary modules
from visualization_utils import (
    plot_learning_curves,
    plot_confusion_matrix as vis_plot_confusion_matrix, # Alias to avoid conflict
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution,
    plot_per_class_metrics,
    plot_prediction_probability_distributions
)
from sklearn.metrics import confusion_matrix
import numpy as np
# 1. Plot Learning Curves (Loss and Metrics over rounds)
try:
    metrics_for_learning_curve = ['accuracy', 'precision', 'recall', 'f1', 'mlogloss'] # Common metrics
    results_pkl_path = os.path.join(output_dir, "results.pkl")
    if os.path.exists(results_pkl_path):
        plot_learning_curves(results_pkl_path, metrics_for_learning_curve, output_dir)
    else:
        log(WARNING, "results.pkl not found at %s, skipping learning curve plots.", results_pkl_path)
except Exception as e:
    log(WARNING, "Failed to generate learning curve plots: %s", e)
# 2. Generate other plots if centralised evaluation was performed and model is available
if centralised_eval and hasattr(strategy, 'global_model') and strategy.global_model is not None and 'test_dmatrix' in globals():
    log(INFO, "Performing final evaluation on centralised test set for detailed visualizations...")
    try:
        # Reconstruct the final model (Booster)
        final_bst = xgb.Booster(params=BST_PARAMS)
        if isinstance(strategy.global_model, (bytes, bytearray)):
            final_bst.load_model(bytearray(strategy.global_model))
        elif isinstance(strategy.global_model, xgb.Booster): # if it was already a booster (e.g. from a custom strategy)
            final_bst = strategy.global_model
        else:
            raise TypeError("Unsupported global_model type in strategy for visualization.")
        y_true = test_dmatrix.get_label()
        y_pred_proba = final_bst.predict(test_dmatrix)
        y_pred = np.argmax(y_pred_proba, axis=1)
        # Plot Confusion Matrix
        conf_matrix_data = confusion_matrix(y_true, y_pred)
        vis_plot_confusion_matrix(conf_matrix_data, CLASS_NAMES, os.path.join(output_dir, "final_confusion_matrix.png"))
        # Plot ROC Curves
        plot_roc_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_roc_curves.png"))
        # Plot Precision-Recall Curves
        plot_precision_recall_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_pr_curves.png"))
        # Plot Class Distribution (True vs Predicted on test set)
        plot_class_distribution(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_class_distribution.png"))
        # Plot Per-Class Metrics (Precision, Recall, F1)
        plot_per_class_metrics(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_per_class_metrics.png"))
        # Plot Prediction Probability Distributions
        # This function saves to output_dir/prediction_probability_distributions.png by default
        plot_prediction_probability_distributions(y_true, y_pred_proba, CLASS_NAMES, output_dir)
        log(INFO, "Successfully generated all detailed visualizations for the final model.")
    except Exception as e:
        log(WARNING, "Failed to generate final model visualizations: %s", e)
elif not centralised_eval:
    log(INFO, "Centralised evaluation was not enabled. Skipping final model detailed visualizations.")
elif not (hasattr(strategy, 'global_model') and strategy.global_model is not None):
    log(INFO, "No final global model available in strategy. Skipping final model detailed visualizations.")
elif 'test_dmatrix' not in globals():
    log(INFO, "Centralised test_dmatrix not available. Skipping final model detailed visualizations.")
log(INFO, "Server process finished.")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    BST_PARAMS,
)
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="tuned_params.py">
"""
tuned_params.py
This file contains tuned hyperparameters for the XGBoost model when using the UNSW_NB15 dataset.
These parameters are automatically loaded by client_utils.py when available.
Note: These are starter parameters that should be refined using ray_tune_xgboost.py
"""
# Tuned parameters for UNSW-NB15 multi-class classification
TUNED_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 11,  # Classes: 0-10 (Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic, plus class 10)
    "eta": 0.03,
    "max_depth": 5,
    "min_child_weight": 5,
    "gamma": 0.8,
    "subsample": 0.8,
    "colsample_bytree": 0.7,
    "colsample_bylevel": 0.7,
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 3,
    "reg_alpha": 1.5,
    "reg_lambda": 3.0,
    "base_score": 0.5,
    "scale_pos_weight": 1.0,
    "grow_policy": "lossguide",
    "normalize_type": "tree",
    "random_state": 42
}
</file>

<file path="update_ray_tune_xgboost.py">
#!/usr/bin/env python3
"""
Script to update ray_tune_xgboost.py for use with UNSW_NB15 dataset
"""
import re
import sys
def update_file(input_file, output_file):
    with open(input_file, 'r') as f:
        content = f.read()
    # Update num_class in train_xgboost function
    content = re.sub(
        r"'num_class': 3,  # benign \(0\), dns_tunneling \(1\), icmp_tunneling \(2\)",
        "'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)",
        content
    )
    # Update num_class in train_final_model function
    content = re.sub(
        r"'num_class': 3,",
        "'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)",
        content
    )
    with open(output_file, 'w') as f:
        f.write(content)
    print(f"Successfully updated {input_file} and saved to {output_file}")
if __name__ == "__main__":
    input_file = "ray_tune_xgboost.py"
    output_file = "ray_tune_xgboost_updated.py"
    update_file(input_file, output_file)
</file>

<file path="use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the
federated learning process to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data>
    --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json
    --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from server_utils import load_saved_model
from dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get feature importance: %s", str(error))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get model parameters: %s", str(error))
def clean_data_for_xgboost(data_frame):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        data_frame (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = data_frame.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # Use max() to set minimum threshold
        threshold = max(threshold, 1e6)
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path):
    """
    Save detailed prediction information to CSV for multi-class classification.
    Args:
        predictions (np.ndarray): Raw predictions from the model
        output_path (str): Path to save the predictions
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are multi-dimensional (one-hot encoded)
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        # Store raw probabilities
        results_df['raw_probabilities'] = predictions.tolist()
        # Get predicted class (argmax)
        predicted_labels = np.argmax(predictions, axis=1)
        results_df['predicted_label'] = predicted_labels
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predicted_labels
        ]
        # Store confidence scores (probability of predicted class)
        results_df['prediction_score'] = predictions[
            np.arange(len(predicted_labels)),
            predicted_labels
        ]
    else:
        # For single class predictions
        results_df['predicted_label'] = predictions.astype(int)
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predictions
        ]
        # Default confidence score of 1.0 for direct class predictions
        results_df['prediction_score'] = 1.0
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    label_counts = results_df['predicted_label'].value_counts()
    log(INFO, "Prediction counts by class:")
    for label, count in label_counts.items():
        class_name = label_mapping.get(int(label), f'unknown_{label}')
        log(INFO, "  %s: %d", class_name, count)
    if 'prediction_score' in results_df.columns:
        log(INFO, "Confidence score statistics: min=%.6f, max=%.6f, mean=%.6f",
            results_df['prediction_score'].min(),
            results_df['prediction_score'].max(),
            results_df['prediction_score'].mean())
    return results_df
def evaluate_labeled_data(model, dataset, output_path):
    """Handle evaluation of labeled data."""
    # Convert to DMatrix
    dmatrix = transform_dataset_to_dmatrix(dataset)
    # Get true labels for evaluation
    y_true = dmatrix.get_label()
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save detailed predictions
    _ = save_detailed_predictions(raw_predictions, output_path)
    # Evaluate if data has labels
    if raw_predictions.ndim > 1:
        y_pred_labels = np.argmax(raw_predictions, axis=1)
    else:
        y_pred_labels = raw_predictions.astype(int)
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_labels)
    precision = precision_score(y_true, y_pred_labels, average='weighted')
    recall = recall_score(y_true, y_pred_labels, average='weighted')
    f1_score_val = f1_score(y_true, y_pred_labels, average='weighted')
    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_labels)
    # Generate classification report
    class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
    report = classification_report(y_true, y_pred_labels, target_names=class_names)
    # Log evaluation results
    log(INFO, "Evaluation Results:")
    log(INFO, "  Accuracy: %.4f", accuracy)
    log(INFO, "  Precision (weighted): %.4f", precision)
    log(INFO, "  Recall (weighted): %.4f", recall)
    log(INFO, "  F1 Score (weighted): %.4f", f1_score_val)
    log(INFO, "Confusion Matrix:\n%s", conf_matrix)
    log(INFO, "Classification Report:\n%s", report)
def predict_unlabeled_data(model, data_path, output_path):
    """Handle prediction of unlabeled data."""
    # Load unlabeled data
    data = pd.read_csv(data_path)
    # Clean data
    data = clean_data_for_xgboost(data)
    # Convert to DMatrix
    dmatrix = xgb.DMatrix(data)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save predictions
    save_detailed_predictions(raw_predictions, output_path)
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Process data based on whether it has labels
        if args.has_labels:
            try:
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                evaluate_labeled_data(model, dataset, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during evaluation: %s", str(error))
                raise
        else:
            try:
                predict_unlabeled_data(model, args.data_path, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during prediction: %s", str(error))
                raise
    except Exception as error:  # pylint: disable=broad-except
        log(INFO, "Error: %s", str(error))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="use_tuned_params.py">
"""
use_tuned_params.py
This script loads the optimized hyperparameters found by Ray Tune and integrates them
into the existing federated learning system. It replaces the default XGBoost parameters
in both client_utils.py and utils.py with the optimized ones.
Usage:
    python use_tuned_params.py --params-file ./tune_results/best_params.json
"""
import os
import json
import argparse
import logging
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def load_tuned_params(params_file):
    """
    Load the optimized hyperparameters from a JSON file.
    Args:
        params_file (str): Path to the JSON file containing the optimized parameters
    Returns:
        dict: Optimized hyperparameters
    """
    if not os.path.exists(params_file):
        raise FileNotFoundError(f"Parameters file not found: {params_file}")
    logger.info("Loading optimized parameters from %s", params_file)
    with open(params_file, 'r', encoding='utf-8') as f:
        params = json.load(f)
    return params
def create_xgboost_params(tuned_params):
    """
    Create XGBoost parameters dictionary from the tuned parameters.
    Args:
        tuned_params (dict): Optimized hyperparameters from Ray Tune
    Returns:
        dict: XGBoost parameters dictionary for use in the existing system
    """
    # Start with the base parameters
    xgb_params = BST_PARAMS.copy()
    # Update with tuned parameters - convert float values to ints for integer parameters
    xgb_params.update({
        'max_depth': int(tuned_params['max_depth']),  # HyperOpt returns float, convert to int
        'min_child_weight': int(tuned_params['min_child_weight']),  # HyperOpt returns float, convert to int
        'eta': tuned_params['eta'],  # Learning rate
        'subsample': tuned_params['subsample'],
        'colsample_bytree': tuned_params['colsample_bytree'],
        'reg_alpha': tuned_params['reg_alpha'],
        'reg_lambda': tuned_params['reg_lambda']
    })
    # Add num_boost_round if it exists in tuned_params
    if 'num_boost_round' in tuned_params:
        xgb_params['num_boost_round'] = int(tuned_params['num_boost_round'])  # Convert to int
    # Add GPU support if specified in tuned parameters
    if 'tree_method' in tuned_params:
        if isinstance(tuned_params['tree_method'], list) and len(tuned_params['tree_method']) > 0:
            # If it's from hp.choice, it will be a list
            xgb_params['tree_method'] = tuned_params['tree_method'][0]
        else:
            xgb_params['tree_method'] = tuned_params['tree_method']
    return xgb_params
def save_updated_params(params, output_file):
    """
    Save updated parameters to a Python file that can be imported by client_utils.py
    Args:
        params (dict): Updated XGBoost parameters
        output_file (str): Path to save the updated parameters
    """
    # Extract num_boost_round if present to use as NUM_LOCAL_ROUND
    num_local_round = None
    if 'num_boost_round' in params:
        num_local_round = int(params['num_boost_round'])
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# This file is generated automatically by use_tuned_params.py\n")
        f.write("# It contains optimized XGBoost parameters found by Ray Tune\n\n")
        # Add NUM_LOCAL_ROUND if it was extracted from num_boost_round
        if num_local_round is not None:
            f.write(f"NUM_LOCAL_ROUND = {num_local_round}\n\n")
        f.write("TUNED_PARAMS = {\n")
        for key, value in params.items():
            if isinstance(value, str):
                f.write(f"    '{key}': '{value}',\n")
            elif isinstance(value, list):
                f.write(f"    '{key}': {value},\n")
            else:
                f.write(f"    '{key}': {value},\n")
        f.write("}\n")
    logger.info("Updated XGBoost parameters saved to %s", output_file)
    if num_local_round is not None:
        logger.info("NUM_LOCAL_ROUND set to %d based on tuned num_boost_round", num_local_round)
def backup_original_params():
    """
    Backup the original parameters to a Python file for reference.
    Returns:
        str: Path to the backup file
    """
    backup_file = "original_bst_params.json"
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(BST_PARAMS, f, indent=2)
    logger.info("Original parameters backed up to %s", backup_file)
    return backup_file
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Use tuned hyperparameters with the existing XGBoost client")
    parser.add_argument("--params-file", type=str, required=True, help="Path to the tuned parameters JSON file")
    parser.add_argument("--output-file", type=str, default="tuned_params.py", help="Output Python file for updated parameters")
    args = parser.parse_args()
    # Backup original parameters
    backup_file = backup_original_params()
    # Load tuned parameters
    tuned_params = load_tuned_params(args.params_file)
    logger.info("Loaded the following optimized parameters:")
    for key, value in tuned_params.items():
        logger.info("  %s: %s", key, value)
    # Create updated XGBoost parameters
    updated_params = create_xgboost_params(tuned_params)
    # Save updated parameters
    save_updated_params(updated_params, args.output_file)
    # Success message
    logger.info("Optimized parameters saved to %s", args.output_file)
    logger.info("Original parameters backed up to %s", backup_file)
    logger.info("These parameters will be automatically used by the XGBoost client")
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 2
BST_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 11,  # Updated for engineered dataset which has 11 classes (0-10)
    "eta": 0.05,  # Reduced learning rate to prevent overfitting
    "max_depth": 6,  # Increased from 3 to allow more complex trees
    "min_child_weight": 10,  # Increased to prevent fitting to small samples
    "gamma": 1.0,  # Increased minimum loss reduction for split
    "subsample": 0.7,  # Sample fewer rows per iteration
    "colsample_bytree": 0.6,  # Sample fewer features per tree
    "colsample_bylevel": 0.6,  # Sample fewer features per level
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 5,
    "reg_alpha": 0.8,  # Decreased L1 regularization from 2.0
    "reg_lambda": 0.8,  # Decreased L2 regularization from 5.0
    "base_score": 0.5,  # Neutral starting point
    "scale_pos_weight": 1.0,  # Simplified for multi-class with >3 classes
    "grow_policy": "lossguide",  # Alternative tree growing policy
    "normalize_type": "tree",  # Helps with interpretability
    "random_state": 42  # Fixed seed for reproducibility
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

<file path="visualization_utils.py">
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import pickle
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
from itertools import cycle
from flwr.common.logger import log
from logging import INFO, WARNING
def plot_confusion_matrix(conf_matrix, class_names, output_path):
    """Generates and saves a heatmap visualization of the confusion matrix."""
    try:
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names, ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Confusion matrix plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate confusion matrix plot: %s", e)
def plot_roc_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves ROC curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        fpr = {}
        tpr = {}
        roc_auc = {}
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(fpr[i], tpr[i], color=color, lw=2,
                     label='ROC curve of class {0} (area = {1:0.2f})'
                     ''.format(class_names[i], roc_auc[i]))
        ax.plot([0, 1], [0, 1], 'k--', lw=2)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('Receiver Operating Characteristic (ROC) - One-vs-Rest')
        ax.legend(loc="lower right")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "ROC curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate ROC curve plot: %s", e)
def plot_precision_recall_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves Precision-Recall curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        precision = {}
        recall = {}
        average_precision = {}
        for i in range(n_classes):
            precision[i], recall[i], _ = precision_recall_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            average_precision[i] = auc(recall[i], precision[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(recall[i], precision[i], color=color, lw=2,
                     label='PR curve of class {0} (AP = {1:0.2f})'
                     ''.format(class_names[i], average_precision[i]))
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_ylim([0.0, 1.05])
        ax.set_xlim([0.0, 1.0])
        ax.set_title('Precision-Recall Curve - One-vs-Rest')
        ax.legend(loc="lower left")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Precision-Recall curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate Precision-Recall curve plot: %s", e)
def plot_class_distribution(y_true, y_pred, class_names, output_path):
    """Generates and saves bar plots comparing true and predicted class distributions."""
    try:
        n_classes = len(class_names)
        true_counts = np.bincount(y_true.astype(int), minlength=n_classes)
        pred_counts = np.bincount(y_pred.astype(int), minlength=n_classes)
        x = np.arange(n_classes)
        width = 0.35
        fig, ax = plt.subplots(figsize=(12, 6))
        rects1 = ax.bar(x - width/2, true_counts, width, label='True Labels')
        rects2 = ax.bar(x + width/2, pred_counts, width, label='Predicted Labels')
        ax.set_ylabel('Count')
        ax.set_title('True vs Predicted Class Distribution')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.bar_label(rects1, padding=3)
        ax.bar_label(rects2, padding=3)
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Class distribution plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate class distribution plot: %s", e)
def plot_per_class_metrics(y_true, y_pred, class_names, output_path):
    """Generates and saves a bar chart of per-class precision, recall, and F1-score."""
    try:
        from sklearn.metrics import classification_report
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
        metrics_to_plot = ['precision', 'recall', 'f1-score']
        class_metrics = {class_name: [] for class_name in class_names}
        for class_name in class_names:
            if class_name in report:
                for metric in metrics_to_plot:
                    class_metrics[class_name].append(report[class_name][metric])
            else: 
                for _ in metrics_to_plot:
                    class_metrics[class_name].append(0)
        x = np.arange(len(class_names)) 
        width = 0.2  
        multiplier = 0
        fig, ax = plt.subplots(figsize=(max(12, len(class_names) * 1.5), 6))
        for i, metric_name in enumerate(metrics_to_plot):
            metric_values = [class_metrics[cn][i] for cn in class_names]
            offset = width * multiplier
            rects = ax.bar(x + offset, metric_values, width, label=metric_name.capitalize())
            ax.bar_label(rects, padding=3, fmt='%.2f')
            multiplier += 1
        ax.set_ylabel('Score')
        ax.set_title('Per-Class Precision, Recall, and F1-Score')
        ax.set_xticks(x + width, class_names, rotation=45, ha="right")
        ax.legend(loc='lower right', ncols=len(metrics_to_plot))
        ax.set_ylim(0, 1.1) 
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Per-class metrics plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate per-class metrics plot: %s", e)
def _plot_single_metric_curve(ax, history_attr, metric_key, label_prefix, marker):
    """Helper function to plot a single metric curve on a given axis."""
    plot_successful = False
    if hasattr(history_attr, '__contains__') and metric_key in history_attr: # Flower 1.x format
        if isinstance(history_attr[metric_key], list) and history_attr[metric_key]:
            rounds, values = zip(*history_attr[metric_key])
            ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
            plot_successful = True
    # Flower 2.x format: history_attr is a list of (round, {dict_of_metrics})
    elif isinstance(history_attr, list) and history_attr:
        if all(isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict) for item in history_attr):
            rounds = [item[0] for item in history_attr if metric_key in item[1]]
            values = [item[1][metric_key] for item in history_attr if metric_key in item[1]]
            if rounds and values:
                ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
                plot_successful = True
    return plot_successful
def plot_learning_curves(results_pkl_path, metrics_to_plot, output_dir):
    """Generates and saves learning curve plots (loss and specified metrics vs. round)
       from a pickled Flower history object.
    Args:
        results_pkl_path (str): Path to the results.pkl file.
        metrics_to_plot (list): A list of metric keys (str) to plot from the history object.
        output_dir (str): Directory to save the plots.
    """
    try:
        if not os.path.exists(results_pkl_path):
            log(WARNING, "Results file not found: %s", results_pkl_path)
            return
        with open(results_pkl_path, 'rb') as f:
            history_data = pickle.load(f) # history_data is now a dict
        # Plot Loss
        fig_loss, ax_loss = plt.subplots(figsize=(12, 6))
        loss_plotted = False
        # Access as dictionary keys
        if "losses_distributed" in history_data and history_data["losses_distributed"]:
            rounds_dist, losses_dist = zip(*history_data["losses_distributed"])
            ax_loss.plot(rounds_dist, losses_dist, label='Distributed Loss', marker='o')
            loss_plotted = True
        if "losses_centralized" in history_data and history_data["losses_centralized"]:
            rounds_cent, losses_cent = zip(*history_data["losses_centralized"])
            ax_loss.plot(rounds_cent, losses_cent, label='Centralized Loss', marker='x')
            loss_plotted = True
        if loss_plotted:
            ax_loss.set_title('Loss Over Federated Learning Rounds')
            ax_loss.set_xlabel('Server Round')
            ax_loss.set_ylabel('Loss')
            ax_loss.legend()
            ax_loss.grid(True)
            fig_loss.tight_layout()
            loss_plot_path = os.path.join(output_dir, "learning_curve_loss.png")
            fig_loss.savefig(loss_plot_path)
            log(INFO, "Loss learning curve plot saved to %s", loss_plot_path)
        else:
            log(INFO, "No loss data found to plot.")
        plt.close(fig_loss)
        # Plot Specified Metrics
        if not metrics_to_plot:
            log(INFO, "No metrics specified for plotting learning curves.")
            return
        num_metrics = len(metrics_to_plot)
        fig_metrics, axes_metrics = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=True)
        if num_metrics == 1:
            axes_metrics = [axes_metrics] 
        any_metric_plotted = False
        for i, metric_key in enumerate(metrics_to_plot):
            ax = axes_metrics[i]
            dist_plotted = False
            cent_plotted = False
            # Access as dictionary keys
            if "metrics_distributed" in history_data and history_data["metrics_distributed"]:
                dist_plotted = _plot_single_metric_curve(ax, history_data["metrics_distributed"], metric_key, 'Distributed', 'o')
            if "metrics_centralized" in history_data and history_data["metrics_centralized"]:
                cent_plotted = _plot_single_metric_curve(ax, history_data["metrics_centralized"], metric_key, 'Centralized', 'x')
            if dist_plotted or cent_plotted:
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} Over Federated Learning Rounds')
                ax.set_ylabel(metric_key.capitalize())
                ax.legend()
                ax.grid(True)
                any_metric_plotted = True
            else:
                log(WARNING, "Could not plot metric '%s' from history. Data not found or in unexpected format.", metric_key)
                ax.text(0.5, 0.5, f"Data for '{metric_key}' not found \nor in unexpected format.", 
                        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} (Data Unavailable)')
        if any_metric_plotted:
            axes_metrics[-1].set_xlabel('Server Round') # Set x-label only for the bottom-most plot
            fig_metrics.tight_layout()
            metrics_plot_path = os.path.join(output_dir, "learning_curves_metrics.png")
            fig_metrics.savefig(metrics_plot_path)
            log(INFO, "Metrics learning curves plot saved to %s", metrics_plot_path)
        else:
            log(INFO, "No data found for any of the specified metrics to plot.")
        plt.close(fig_metrics)
    except FileNotFoundError:
        log(WARNING, "Results file not found: %s", results_pkl_path)
    except pickle.UnpicklingError:
        log(WARNING, "Error unpickling results file: %s", results_pkl_path)
    except Exception as e:
        log(WARNING, "Could not generate learning curve plots: %s", e)
def plot_prediction_probability_distributions(y_true, y_pred_proba, class_names, output_dir, bins=50):
    """Generates and saves histograms of prediction probabilities for each true class.
    For each class, this plot shows the distribution of the predicted probabilities 
    assigned to that class, for samples that actually belong to that class.
    High probabilities bunched towards 1.0 are desirable.
    Args:
        y_true (np.array): Array of true labels (integers).
        y_pred_proba (np.array): Array of predicted probabilities, shape (n_samples, n_classes).
        class_names (list): List of class names (strings).
        output_dir (str): Directory to save the plot.
        bins (int): Number of bins for the histograms.
    """
    try:
        n_classes = len(class_names)
        if y_pred_proba.shape[1] != n_classes:
            log(WARNING, "Number of classes in y_pred_proba does not match len(class_names).")
            return
        # Determine the number of rows and columns for subplots
        n_cols = 3 
        n_rows = (n_classes + n_cols - 1) // n_cols # Ceiling division
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=True, sharey=False)
        axes = axes.flatten() # Flatten to easily iterate regardless of shape
        for i in range(n_classes):
            ax = axes[i]
            # Get probabilities for the current class where the true label is this class
            true_class_indices = np.where(y_true == i)[0]
            if len(true_class_indices) == 0:
                ax.text(0.5, 0.5, "No true samples for this class", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{class_names[i]} (No true samples)')
                ax.set_xlabel('Predicted Probability')
                ax.set_ylabel('Frequency')
                continue
            class_probabilities = y_pred_proba[true_class_indices, i]
            ax.hist(class_probabilities, bins=bins, range=(0,1), edgecolor='black', alpha=0.7)
            ax.set_title(f'Class: {class_names[i]}')
            ax.set_xlabel('Predicted Probability for this Class')
            ax.set_ylabel('Frequency')
            ax.grid(True, axis='y', linestyle='--', alpha=0.7)
            mean_proba = np.mean(class_probabilities)
            ax.axvline(mean_proba, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_proba:.2f}')
            ax.legend(fontsize='small')
        # Hide any unused subplots
        for j in range(n_classes, n_rows * n_cols):
            fig.delaxes(axes[j])
        fig.suptitle('Distribution of Predicted Probabilities for True Classes', fontsize=16, y=1.02)
        fig.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for suptitle
        plot_path = os.path.join(output_dir, "prediction_probability_distributions.png")
        fig.savefig(plot_path)
        plt.close(fig)
        log(INFO, "Prediction probability distribution plot saved to %s", plot_path)
    except Exception as e:
        log(WARNING, "Could not generate prediction probability distribution plot: %s", e)
</file>

</files>
