This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    cml.yaml
diagrams/
  client_operations.mmd
  data_handling.mmd
  overall_workflow.mmd
  server_operations.mmd
.gitattributes
.gitignore
.repomixignore
client_utils.py
client.py
commit.sh
create_global_processor.py
CRITICAL_ISSUES_ANALYSIS.md
dataset.py
FL-CML-Pipeline-Analysis.md
go_to_work.sh
model_performance.md
original_bst_params.json
pyproject.toml
ray_tune_README.md
ray_tune_xgboost_updated.py
ray_tune_xgboost.py
README.md
requirements.txt
run_bagging.sh
run_cyclic.sh
run_ray_tune.sh
run.py
server_utils.py
server.py
sim.py
tuned_params.py
update_ray_tune_xgboost.py
use_saved_model.py
use_tuned_params.py
utils.py
visualization_utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/cml.yaml">
name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
        with:
          lfs: true
      # Ensure LFS objects are properly pulled
      - name: Pull LFS objects
        run: |
          git lfs install
          git lfs pull
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: 'pip'
      - uses: iterative/setup-cml@v1
      # Cache conda/mamba environments
      - name: Cache conda environment
        uses: actions/cache@v3
        with:
          path: |
            ~/.conda
            ~/miniconda3
            ./venv
          key: ${{ runner.os }}-conda-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-conda-
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          # Check if venv exists before creating
          if [ ! -d "venv" ]; then
            echo "Setting up virtual environment for the first time"
            pip install virtualenv
            virtualenv venv
          fi
          source venv/bin/activate
          # Check if mamba is installed
          if ! command -v mamba &> /dev/null; then
            echo "Installing mamba"
            pip install mamba
            mamba init
            source ~/.bashrc
            mamba create
            mamba activate
          fi
          # Install dependencies
          pip install -r requirements.txt
          # Check if main packages are installed to avoid reinstalling them
          if ! python -c "import xgboost" &> /dev/null; then
            pip install xgboost
          fi
          if ! python -c "import flwr" &> /dev/null; then
            pip install -U flwr["simulation"]
          fi
          if ! python -c "import ray" &> /dev/null; then
            pip install -U "ray[all]"
          fi
          if ! python -c "import torch" &> /dev/null; then
            pip install torch torchvision torchaudio
          fi
          if ! python -c "import hydra" &> /dev/null; then
            pip install hydra-core
          fi
          if ! python -c "import imblearn" &> /dev/null; then
            pip install imbalanced-learn
          fi
      - name: Install hyperopt
        run: |
          source venv/bin/activate
          pip install hyperopt
      - name: Run Ray Tune hyperparameter optimization
        run: |
          source venv/bin/activate
          # Check if data directory exists, create if not
          mkdir -p data/sample
          # Use the same dataset that federated learning will use for consistency
          FINAL_DATASET="data/received/final_dataset.csv"
          # Check if the final dataset exists
          if [ -f "$FINAL_DATASET" ]; then
            echo "Using final dataset for hyperparameter tuning: $FINAL_DATASET"
            # Create output directory for Ray Tune results
            mkdir -p tune_results
            # Run Ray Tune with the final dataset for consistency
            bash run_ray_tune.sh --data-file "$FINAL_DATASET" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
          else
            echo "Final dataset not found at $FINAL_DATASET, checking for fallback data..."
            # Define train and test file paths for UNSW_NB15 as fallback
            TRAIN_FILE="data/received/UNSW_NB15_training-set.csv"
            TEST_FILE="data/received/UNSW_NB15_testing-set.csv"
            # Check if the UNSW_NB15 files exist, otherwise use sample data
            if [ -f "$TRAIN_FILE" ] && [ -f "$TEST_FILE" ]; then
              echo "Using UNSW_NB15 data files for hyperparameter tuning"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            else
              echo "No suitable data found, creating synthetic data for testing"
              # Try to find any CSV file to use as a fallback
              SAMPLE_DATA=""
              for csv_file in $(find data -name "*.csv" | head -n 1); do
                SAMPLE_DATA="$csv_file"
                break
              done
              if [ -z "$SAMPLE_DATA" ]; then
                echo "No CSV data found, creating a sample dataset for tuning"
                # Generate synthetic data for train and test (kept as fallback)
                python -c "import pandas as pd; import numpy as np; np.random.seed(42); n = 1000; df = pd.DataFrame({'feature1': np.random.normal(0, 1, n), 'feature2': np.random.normal(0, 1, n), 'feature3': np.random.normal(0, 1, n), 'feature4': np.random.normal(0, 1, n), 'feature5': np.random.normal(0, 1, n), 'label': np.random.choice([0, 1, 2], n)}); train = df.sample(frac=0.8, random_state=42); test = df.drop(train.index); train.to_csv('data/sample/synthetic_train.csv', index=False); test.to_csv('data/sample/synthetic_test.csv', index=False)"
                TRAIN_FILE="data/sample/synthetic_train.csv"
                TEST_FILE="data/sample/synthetic_test.csv"
              else
                # If we have a single file, create train/test split (kept as fallback)
                python -c "import pandas as pd; from sklearn.model_selection import train_test_split; df = pd.read_csv('$SAMPLE_DATA'); train, test = train_test_split(df, test_size=0.2, random_state=42); train.to_csv('data/sample/split_train.csv', index=False); test.to_csv('data/sample/split_test.csv', index=False)"
                TRAIN_FILE="data/sample/split_train.csv"
                TEST_FILE="data/sample/split_test.csv"
              fi
              echo "Using train file: $TRAIN_FILE"
              echo "Using test file: $TEST_FILE"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              # Use a small number of samples in CI for speed
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            fi
          fi
          # Apply the tuned parameters to the federated learning system
          if [ -f "tune_results/best_params.json" ]; then
            python use_tuned_params.py --params-file "./tune_results/best_params.json"
            echo "Successfully applied tuned parameters to federated learning"
          else
            echo "Warning: No tuned parameters found, will use default parameters"
          fi
      - name: Run federated learning pipeline
        run: |
          source venv/bin/activate
          # Create output directories to ensure they exist
          mkdir -p outputs results tune_results
          # Run the federated learning with tuned parameters
          ./run_bagging.sh
          # List what was created for debugging
          echo "Contents of outputs directory:"
          ls -la outputs/ || echo "outputs directory is empty or doesn't exist"
          echo "Contents of results directory:"
          ls -la results/ || echo "results directory is empty or doesn't exist"
          echo "Contents of tune_results directory:"
          ls -la tune_results/ || echo "tune_results directory is empty or doesn't exist"
      - name: Evaluate model performance
        run: |
          source venv/bin/activate
          # Compare model performance with and without tuned parameters
          echo "## XGBoost Model Performance" > model_performance.md
          echo "" >> model_performance.md
          # Extract metrics from the results directory
          if [ -d "results" ]; then
            echo "### Federated Learning Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "| Metric | Value |" >> model_performance.md
            echo "| ------ | ----- |" >> model_performance.md
            # Extract and add key metrics if available
            for metric_file in $(find results -name "*.txt" | grep -i "metrics\|accuracy\|f1\|precision\|recall"); do
              metric_name=$(basename "$metric_file" .txt)
              metric_value=$(cat "$metric_file" | head -n 1)
              echo "| $metric_name | $metric_value |" >> model_performance.md
            done
          fi
          # Add Ray Tune optimization results
          if [ -f "tune_results/best_params.json" ]; then
            echo "" >> model_performance.md
            echo "### Hyperparameter Optimization Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "Best hyperparameters:" >> model_performance.md
            echo '```json' >> model_performance.md
            cat tune_results/best_params.json >> model_performance.md
            echo '```' >> model_performance.md
          fi
      - name: Debug git status before commit
        run: |
          echo "=== Current git status ==="
          git status
          echo "=== Files in outputs directory ==="
          find outputs -type f 2>/dev/null || echo "No files in outputs directory"
          echo "=== Files in results directory ==="
          find results -type f 2>/dev/null || echo "No files in results directory"
          echo "=== Files in tune_results directory ==="
          find tune_results -type f 2>/dev/null || echo "No files in tune_results directory"
          echo "=== Git check-ignore on key directories ==="
          git check-ignore outputs/ || echo "outputs/ is NOT ignored"
          git check-ignore results/ || echo "results/ is NOT ignored" 
          git check-ignore tune_results/ || echo "tune_results/ is NOT ignored"
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          git checkout multi-class-predictions
          # Force add all files in result directories using find to be more robust
          echo "Force adding all result files..."
          # Add tune_results directory and all its contents
          if [ -d "tune_results" ]; then
            git add -f tune_results/
            find tune_results -type f -exec git add -f {} \; 2>/dev/null || true
          fi
          # Add results directory and all its contents  
          if [ -d "results" ]; then
            git add -f results/
            find results -type f -exec git add -f {} \; 2>/dev/null || true
          fi
          # Add outputs directory and all its contents
          if [ -d "outputs" ]; then
            git add -f outputs/
            find outputs -type f -exec git add -f {} \; 2>/dev/null || true
          fi
          # Add model performance report
          git add model_performance.md 2>/dev/null || true
          # Add any other important files that might have been created
          git add -f *.json 2>/dev/null || true
          git add -f tuned_params.py 2>/dev/null || true
          # Check what files are staged for commit
          echo "Files staged for commit:"
          git status --porcelain
          # Commit all changes (only if there are changes)
          if [ -n "$(git status --porcelain)" ]; then
            git commit -m "workflow with Ray Tune optimization in actionðŸš€"
            echo "Changes committed successfully"
          else
            echo "No changes to commit"
          fi
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true
</file>

<file path="diagrams/client_operations.mmd">
graph LR
    subgraph ClientOperations [Client Operations client.py_client_utils.py]
        A[Receive Parameters from Server] --> B[Local Training: fit]
        B --> C[Evaluation: evaluate]
        C --> D[Metrics Calculation]
        D --> E[Send Results to Server]
        B --> F[Update Model]
        F --> B
        C --> G[Prediction if unlabeled data]
        G --> H[Save Predictions]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ClientOperations component;
</file>

<file path="diagrams/data_handling.mmd">
graph LR
    subgraph DataHandling [Data Handling dataset.py]
        A[Load CSV: load_csv_data] --> B[Preprocessing: preprocess_data]
        B --> C[Separate Features/Labels: separate_xy]
        C --> D[DMatrix Conversion: transform_dataset_to_dmatrix]
        D --> E[Train/Test Split: train_test_split]
        B --> F[Handle inf/NaN]
        F --> C
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class DataHandling component;
</file>

<file path="diagrams/overall_workflow.mmd">
graph LR
    subgraph OverallWorkflow [Overall Workflow]
        A[Client] --> B[Server]
        B --> A
        A --> C[Data]
        C --> A
    end
    
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class OverallWorkflow component
</file>

<file path="diagrams/server_operations.mmd">
graph LR
    subgraph ServerOperations [Server Operations server.py]
        A[Strategy Selection: FedXgbBagging/FedXgbCyclic] --> B[Client Management]
        B --> C[Model Aggregation]
        C --> D[Send Parameters to Clients]
        B --> E[Receive Results from Clients]
        E --> F[Metrics Aggregation]
        C --> G[Evaluation if centralized]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ServerOperations component;
</file>

<file path=".gitattributes">
data/received/final_dataset.csv filter=lfs diff=lfs merge=lfs -text
</file>

<file path=".gitignore">
.venv/
# Virtual environments (should NEVER be in Git)
.venv/
venv/
env/
ENV/

# Python cache
__pycache__/
*.pyc
*.pyo
*.pyd

# Large data files (too large for Git)
data/old/
data/received/
received/

# Ignore large CSV files in data directories, but allow small result CSVs
data/**/*.csv
received/**/*.csv

# Still ignore log files as they can be large
*.log
</file>

<file path=".repomixignore">
# Ignore data directory containing large files
data/

# Common large file types
*.csv
*.parquet
*.pkl
*.model

# Ignore outputs and results
outputs/
results/
local_utils/
received/
notebooks/
tune_results/

# Cache directories
__pycache__/
.cache/ 
.cursor/
</file>

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO, ERROR, WARNING
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
import importlib.util
from sklearn.utils.class_weight import compute_sample_weight
# Default XGBoost parameters for UNSW_NB15 multi-class classification
BST_PARAMS = {
    'objective': 'multi:softprob',  # Multi-class classification with probabilities
    'num_class': 11,  # Classes: 0-10 (Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic, plus class 10)
    'eval_metric': ['mlogloss', 'merror'],  # Multi-class metrics
    'learning_rate': 0.05,
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': 1.0  # Removed class-specific weights as it's not compatible with multi-class with >3 classes
}
# Try to import tuned parameters if available
try:
    # Check if tuned_params.py exists
    tuned_params_path = os.path.join(os.path.dirname(__file__), "tuned_params.py")
    if os.path.exists(tuned_params_path):
        # Dynamically import the tuned parameters
        spec = importlib.util.spec_from_file_location("tuned_params", tuned_params_path)
        tuned_params_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(tuned_params_module)
        # Use the tuned parameters
        TUNED_PARAMS = tuned_params_module.TUNED_PARAMS
        log(INFO, "Using tuned XGBoost parameters from Ray Tune optimization")
    else:
        TUNED_PARAMS = BST_PARAMS.copy()
except Exception as e:
    log(INFO, f"Could not load tuned parameters: {str(e)}")
    TUNED_PARAMS = BST_PARAMS.copy()
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
        cid: Client ID for logging purposes
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        cid,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None,
        use_tuned_params=True
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            cid: Client ID for logging purposes
            params (dict): XGBoost parameters (defaults to BST_PARAMS if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
            use_tuned_params (bool): Whether to use tuned parameters if available
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.cid = cid
        # Use tuned parameters if available and requested
        if params is not None:
            self.params = params
        elif use_tuned_params:
            self.params = TUNED_PARAMS.copy()
            log(INFO, "Using tuned parameters for XGBoost training")
        else:
            self.params = BST_PARAMS.copy()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        # --- PHASE 1: Aggressive Regularization (Overrides any loaded/tuned params) --- REMOVED
        y_train = self.train_dmatrix.get_label()
        # --- Check if labels are empty ---
        if y_train.size == 0:
            log(ERROR, f"Client {self.cid}: Training DMatrix has no labels. Cannot proceed with fit.")
            return FitRes(
                status=Status(code=Code.FIT_NOT_IMPLEMENTED, message="Training data is missing labels."),
                parameters=Parameters(tensor_type="", tensors=[]), # Return empty params
                num_examples=0,
                metrics={}
            )
        # --- End Check ---
        # Ensure labels are integers for compute_sample_weight
        y_train_int = y_train.astype(int)
        class_counts = np.bincount(y_train_int)
        # Log class distribution for all classes
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        for i, count in enumerate(class_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        # --- Debugging Sample Weight Calculation ---
        log(INFO, f"Type of y_train before weight calc: {type(y_train)}")
        log(INFO, f"Shape of y_train: {y_train.shape if hasattr(y_train, 'shape') else 'N/A'}")
        try:
            log(INFO, f"Unique values in y_train: {np.unique(y_train)}")
        except Exception as e:
            log(INFO, f"Could not get unique values of y_train: {e}")
        log(INFO, f"Type of y_train_int: {type(y_train_int)}")
        log(INFO, f"Shape of y_train_int: {y_train_int.shape}")
        log(INFO, f"Unique values in y_train_int: {np.unique(y_train_int)}")
        log(INFO, f"dtype of y_train_int: {y_train_int.dtype}")
        # Only log min/max if array is not empty
        if y_train_int.size > 0:
            log(INFO, f"Min/Max values in y_train_int: {np.min(y_train_int)} / {np.max(y_train_int)}")
        else:
            log(INFO, "y_train_int is empty, cannot calculate Min/Max.")
        # --- End Debugging ---
        # Compute sample weights for class imbalance
        try:
            sample_weights = compute_sample_weight('balanced', y_train_int) # Use integer labels
            log(INFO, f"Successfully computed sample weights. Shape: {sample_weights.shape}, dtype: {sample_weights.dtype}")
        except IndexError as e:
            log(INFO, f"IndexError during compute_sample_weight: {e}")
            log(INFO, f"Unique labels causing issue: {np.unique(y_train_int)}")
            # As a fallback, use uniform weights
            log(INFO, "Falling back to uniform sample weights.")
            sample_weights = np.ones(len(y_train_int))
        except Exception as e:
            log(INFO, f"Other error during compute_sample_weight: {e}")
            log(INFO, "Falling back to uniform sample weights due to unexpected error.")
            sample_weights = np.ones(len(y_train_int))
        # Create a new DMatrix with weights for training
        dtrain_weighted = xgb.DMatrix(self.train_dmatrix.get_data(), label=y_train, weight=sample_weights, feature_names=self.train_dmatrix.feature_names)
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch with sample weights
            bst = xgb.train(
                self.params,
                dtrain_weighted,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (dtrain_weighted, "train")],
                early_stopping_rounds=20,
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        # Since objective is multi:softprob, predict() outputs probabilities
        y_pred_proba = bst.predict(self.valid_dmatrix)
        # Get class labels from probabilities
        y_pred_labels = np.argmax(y_pred_proba, axis=1)
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        num_classes_actual = len(class_names) # Or get from self.params if needed
        for i, count in enumerate(true_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics using predicted labels
        # Add zero_division=0 to handle cases where a class might not be predicted
        precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss using probabilities
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_true_int = y_true.astype(int)
        # Ensure y_true_int does not contain labels outside the expected range [0, num_classes-1]
        valid_indices = (y_true_int >= 0) & (y_true_int < num_classes_actual)
        if not np.all(valid_indices):
            log(WARNING, f"Found {np.sum(~valid_indices)} labels outside expected range [0, {num_classes_actual-1}]. Clamping for mlogloss calculation.")
            y_true_int = np.clip(y_true_int, 0, num_classes_actual - 1)
            # Optionally filter data if clamping is not desired:
            # y_true_int = y_true_int[valid_indices]
            # y_pred_proba = y_pred_proba[valid_indices]
        y_true_one_hot = np.eye(num_classes_actual)[y_true_int]
        # Ensure y_pred_proba has the correct shape and handle potential issues
        if y_pred_proba.shape == (len(y_true_int), num_classes_actual):
            # Clip probabilities to avoid log(0)
            y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)
            mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_clipped), axis=1))
        else:
            log(WARNING, f"Shape mismatch for mlogloss: y_pred_proba shape {y_pred_proba.shape}, expected ({len(y_true_int)}, {num_classes_actual}). Skipping mlogloss.")
            mlogloss = -1.0 # Indicate failure to calculate
        # Compute confusion matrix using predicted labels
        try:
            # Explicitly provide labels to ensure consistent matrix size
            conf_matrix = confusion_matrix(y_true, y_pred_labels, labels=range(num_classes_actual))
        except Exception as e:
            log(WARNING, f"Error computing confusion matrix: {str(e)}")
            # Create empty confusion matrix with the correct size
            conf_matrix = np.zeros((num_classes_actual, num_classes_actual), dtype=int)
        # Generate detailed classification report using predicted labels
        try:
            # Ensure target_names matches the actual number of classes
            unique_labels = np.unique(np.concatenate((y_true.astype(int), y_pred_labels))) # Get labels present in data
            target_names_filtered = [class_names[i] for i in range(num_classes_actual) if i in unique_labels]
            # Ensure we use labels consistent with target_names_filtered
            labels_for_report = [i for i in range(num_classes_actual) if i in unique_labels]
            class_report = classification_report(
                y_true, 
                y_pred_labels, 
                labels=labels_for_report, 
                target_names=target_names_filtered, 
                zero_division=0
            )
            log(INFO, f"Classification Report:\n{class_report}")
        except Exception as e:
            log(WARNING, f"Error generating classification report: {str(e)}")
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix shape: {conf_matrix.shape}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from server_utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss)
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data,
    load_global_feature_processor,
    create_global_feature_processor,
    transform_dataset_to_dmatrix
)
from utils import client_args_parser, BST_PARAMS
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    # Ensure data/received/ directory exists
    os.makedirs("data/received", exist_ok=True)
    # Load labeled data for training - using the original final dataset with proper temporal splitting
    labeled_csv_path = "data/received/final_dataset.csv"
    log(INFO, "Using original final dataset with temporal splitting: %s", labeled_csv_path)
    # Check for global feature processor first
    global_processor_path = "outputs/global_feature_processor.pkl"
    if os.path.exists(global_processor_path):
        log(INFO, "Loading global feature processor for consistent preprocessing")
        global_processor = load_global_feature_processor(global_processor_path)
    else:
        log(WARNING, "Global feature processor not found, creating one from the dataset")
        global_processor_path = create_global_feature_processor(labeled_csv_path, "outputs")
        global_processor = load_global_feature_processor(global_processor_path)
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction if available
    # For now, we'll use a portion of the labeled data as unlabeled
    # This should be updated once an unlabeled version of the engineered dataset is available
    unlabeled_csv_path = labeled_csv_path  # Using the same file for now
    unlabeled_dataset = labeled_dataset    # Using the same dataset for now
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    # The ExponentialPartitioner doesn't have get_indices, but provides partition method
    # which returns the partition directly rather than just indices
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except Exception as e:
        log(INFO, f"get_partition failed ({e}), using fallback partitioning")
        # Fallback to simple index-based partitioning if get_partition is not available
        total_samples = len(full_train_data)
        samples_per_partition = total_samples // args.num_partitions
        start_idx = args.partition_id * samples_per_partition
        end_idx = (args.partition_id + 1) * samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
        log(INFO, f"Used fallback partitioning. Partition {args.partition_id}: samples {start_idx} to {end_idx}")
        log(INFO, f"Partition size: {end_idx - start_idx} samples (out of {total_samples} total)")
        # Get the partition slice
        train_partition = full_train_data.select(range(start_idx, end_idx))
    # Convert to pandas for easier manipulation
    train_partition_df = train_partition.to_pandas()
    # Perform local train/test split using client-specific random seed
    client_seed = args.seed + args.partition_id * 1000  # Make each client's seed unique
    log(INFO, f"Using client-specific random seed for train/test split: {client_seed}")
    # Log data shape before splitting
    log(INFO, f"Original data shape before splitting: {train_partition_df.shape}")
    # Check class distribution before splitting
    if 'label' in train_partition_df.columns:
        class_dist = train_partition_df['label'].value_counts().to_dict()
        log(INFO, f"Class distribution in original data: {class_dist}")
    # Use different random states for train/validation split to ensure no overlap
    train_random_state = client_seed
    val_random_state = client_seed + 5000  # Different seed for validation
    log(INFO, f"Using different random states for train/validation split: {train_random_state}/{val_random_state}")
    # Perform the split
    from sklearn.model_selection import train_test_split as sklearn_split
    train_df, valid_df = sklearn_split(
        train_partition_df, 
        test_size=args.test_fraction, 
        random_state=train_random_state,
        stratify=train_partition_df['label'] if 'label' in train_partition_df.columns else None
    )
    log(INFO, f"Train data shape: {train_df.shape}, Test data shape: {valid_df.shape}")
    # Log class distributions after splitting
    if 'label' in train_df.columns:
        train_class_dist = train_df['label'].value_counts().to_dict()
        valid_class_dist = valid_df['label'].value_counts().to_dict()
        log(INFO, f"Class distribution in train data: {train_class_dist}")
        log(INFO, f"Class distribution in test data: {valid_class_dist}")
    # Use the global processor to transform the data
    log(INFO, "Transforming data using global feature processor")
    try:
        # Transform using the global processor
        train_processed = global_processor.transform(train_df, is_training=True)
        valid_processed = global_processor.transform(valid_df, is_training=False)
        # Convert to DMatrix
        train_dmatrix = transform_dataset_to_dmatrix(train_processed, processor=global_processor, is_training=True)
        valid_dmatrix = transform_dataset_to_dmatrix(valid_processed, processor=global_processor, is_training=False)
        num_train = len(train_processed)
        num_val = len(valid_processed)
        log(INFO, f"Train DMatrix has {train_dmatrix.num_row()} rows, Test DMatrix has {valid_dmatrix.num_row()} rows")
        log(INFO, f"Local split: {num_train} train samples, {num_val} validation samples")
    except Exception as e:
        log(ERROR, f"Error in data transformation: {str(e)}")
        log(INFO, "Falling back to original train_test_split method")
        # Fallback to the original method if global processor fails
        train_dmatrix, valid_dmatrix, _ = train_test_split(
            train_partition, test_fraction=args.test_fraction, random_state=args.seed
        )
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=global_processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        cid=args.partition_id,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="commit.sh">
#!/bin/bash
# Stage all changes
git add .
# Commit the changes with a commit message
git commit -m "commit.sh"
# Push the changes to the remote repository
git push
</file>

<file path="create_global_processor.py">
#!/usr/bin/env python3
"""
create_global_processor.py
Script to create a global feature processor that ensures consistent preprocessing 
across Ray Tune hyperparameter optimization and Federated Learning phases.
This addresses the disconnection between individual client training and federated
learning by ensuring all phases use the same preprocessing statistics.
"""
import argparse
import os
import sys
from dataset import create_global_feature_processor, load_global_feature_processor
from flwr.common.logger import log
from logging import INFO
def main():
    """Create global feature processor for consistent preprocessing."""
    parser = argparse.ArgumentParser(
        description="Create global feature processor for consistent preprocessing"
    )
    parser.add_argument(
        "--data-file",
        type=str,
        default="data/received/final_dataset.csv",
        help="Path to the dataset file (default: data/received/final_dataset.csv)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs",
        help="Output directory for the processor (default: outputs)"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force recreation of processor even if it already exists"
    )
    args = parser.parse_args()
    # Check if data file exists
    if not os.path.exists(args.data_file):
        log(INFO, "Error: Data file not found: %s", args.data_file)
        sys.exit(1)
    # Check if processor already exists
    processor_path = os.path.join(args.output_dir, "global_feature_processor.pkl")
    if os.path.exists(processor_path) and not args.force:
        log(INFO, "Global feature processor already exists at: %s", processor_path)
        log(INFO, "Use --force to recreate it")
        # Load and display info about existing processor
        try:
            processor = load_global_feature_processor(processor_path)
            log(INFO, "Existing processor details:")
            log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
            log(INFO, "  Categorical features: %d", len(processor.categorical_features))
            log(INFO, "  Numerical features: %d", len(processor.numerical_features))
            log(INFO, "  Is fitted: %s", processor.is_fitted)
        except (FileNotFoundError, ImportError, AttributeError) as e:
            log(INFO, "Error loading existing processor: %s", str(e))
            log(INFO, "Consider using --force to recreate it")
        sys.exit(0)
    # Create the global processor
    log(INFO, "Creating global feature processor...")
    try:
        processor_path = create_global_feature_processor(args.data_file, args.output_dir)
        log(INFO, "Successfully created global feature processor at: %s", processor_path)
        # Verify the processor
        processor = load_global_feature_processor(processor_path)
        log(INFO, "Verification successful!")
        log(INFO, "  Dataset type: %s", getattr(processor, 'dataset_type', 'unknown'))
        log(INFO, "  Categorical features: %d", len(processor.categorical_features))
        log(INFO, "  Numerical features: %d", len(processor.numerical_features))
        log(INFO, "  Is fitted: %s", processor.is_fitted)
        if hasattr(processor, 'unique_labels'):
            log(INFO, "  Unique labels: %s", processor.unique_labels)
    except (FileNotFoundError, ImportError, AttributeError, ValueError) as e:
        log(INFO, "Error creating global feature processor: %s", str(e))
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="CRITICAL_ISSUES_ANALYSIS.md">
# Critical Issues Analysis - FL-CML-Pipeline

## Executive Summary
The federated learning pipeline has several critical issues that prevent it from achieving paper-worthy results. The most severe issue is a **temporal data leakage problem** that completely excludes class 2 from training, making the model unable to learn this class.

## ðŸš¨ Critical Issues (A+ Priority)

### 1. **TEMPORAL SPLITTING DATA LEAKAGE - CLASS 2 MISSING**
**Severity:** CRITICAL - Breaks the entire classification system

**Problem:**
- Class 2 has a very narrow Stime range [1.6614, 1.6626] 
- All 15,000 samples of class 2 are concentrated in the last temporal period
- When using 80/20 temporal split, ALL class 2 samples go to test set
- ZERO class 2 samples in training set
- Model cannot learn class 2 at all

**Evidence:**
```
Class 2: Stime range [1.6614, 1.6626], count: 15000
Train split Stime range: [-9.4258, 1.6612] 
Test split Stime range: [1.6612, 1.6627]
â†’ Result: 0 class 2 samples in training, 15000 in test
```

**Impact:**
- Model accuracy severely limited (~35% instead of potential 90%+)
- Invalid evaluation metrics 
- Cannot classify class 2 attacks in production
- Fundamental violation of ML best practices

### 2. **HYPERPARAMETER OPTIMIZATION CATASTROPHIC FAILURE**
**Severity:** CRITICAL - Search space completely wrong

**Problem:**
- `num_boost_round` range: [1, 10] - **COMPLETELY INADEQUATE**
- Should be [50, 500] minimum for decent XGBoost performance
- `eta` range includes 0.001 - too small for practical training
- Only 5 search samples in CI/CD - not enough for optimization
- Early termination without proper convergence

**Evidence from code:**
```python
# TERRIBLE search space in ray_tune_xgboost_updated.py line 283:
"num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # DISASTER!
"eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3))  # Too wide range
```

**Result:**
- Ray Tune finds `num_boost_round: 1` - essentially no training
- Model underfits severely 
- Accuracy stuck at ~35%

### 3. **XGBoost CONFIGURATION ERRORS**
**Severity:** HIGH - Wrong model configuration

**Problems:**
- `num_class: 11` but data has missing class 2 in training
- No proper class weight balancing for imbalanced data
- Missing early stopping in federated rounds
- Suboptimal tree parameters

### 4. **FEDERATED LEARNING STRATEGY CATASTROPHIC**
**Severity:** HIGH - Completely inadequate FL setup

**Problems:**
- **Only 1 local boosting round** - trees can't learn anything meaningful
- **Only 5 global rounds** - no convergence possible
- No early stopping or convergence criteria
- No learning rate scheduling
- Clients train on data with missing classes

## ðŸ”§ Performance Issues

### 5. **DATA PREPROCESSING INCONSISTENCIES**
**Severity:** MEDIUM-HIGH - Feature engineering issues

**Problems:**
- Global feature processor not consistently applied
- Multiple different preprocessing paths
- Potential feature scaling issues
- Inconsistent handling of engineered vs original dataset

### 6. **EVALUATION METHODOLOGY FLAWED**
**Severity:** MEDIUM - Invalid results reporting

**Problems:**
- Evaluating on temporally split data with missing classes
- Confusion matrix shape mismatches (11 classes vs 10 actual)
- ROC curves invalid for missing classes
- Metrics calculations incorrect for imbalanced data

## ðŸ“Š Quantified Impact

**Current Performance:**
- Accuracy: ~35.2% (should be 80%+)
- F1-Score: ~32.2% (should be 75%+)
- Missing Class 2: 100% classification failure
- Training Time: Excessive due to poor hyperparameters
- **COMPLETE SYSTEM FAILURE**

**Expected Performance After Fixes:**
- Accuracy: 85-95%
- F1-Score: 80-90%
- All 11 classes learnable
- 10x faster training
- **PRODUCTION-READY SYSTEM**

## ðŸ› ï¸ Emergency Fixes Required (Priority Order)

### 1. **FIX TEMPORAL SPLITTING (URGENT - 2 hours)**
```python
# In dataset.py, load_csv_data() function - REPLACE temporal split:
# FROM:
if 'Stime' in df.columns:
    df_sorted = df.sort_values('Stime').reset_index(drop=True)
    train_size = int(0.8 * len(df_sorted))
    train_df = df_sorted.iloc[:train_size]
    test_df = df_sorted.iloc[train_size:]

# TO:
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(
    df, test_size=0.2, random_state=42, stratify=df['label']
)
```

### 2. **FIX RAY TUNE SEARCH SPACE (URGENT - 1 hour)**
```python
# In ray_tune_xgboost_updated.py line 283 - REPLACE:
search_space = {
    "max_depth": hp.quniform("max_depth", 4, 12, 1),  # Better range
    "min_child_weight": hp.quniform("min_child_weight", 1, 10, 1),
    "reg_alpha": hp.loguniform("reg_alpha", np.log(0.01), np.log(10.0)),
    "reg_lambda": hp.loguniform("reg_lambda", np.log(0.01), np.log(10.0)),
    "eta": hp.uniform("eta", 0.01, 0.3),  # Reasonable range
    "subsample": hp.uniform("subsample", 0.6, 1.0),
    "colsample_bytree": hp.uniform("colsample_bytree", 0.6, 1.0),
    "num_boost_round": hp.quniform("num_boost_round", 50, 300, 10)  # CRITICAL FIX!
}
```

### 3. **INCREASE FL TRAINING ROUNDS (URGENT - 30 minutes)**
```python
# In utils.py:
NUM_LOCAL_ROUND = 20  # FROM: 2 TO: 20

# In server script args:
--num-rounds 20  # FROM: 5 TO: 20
```

### 4. **FIX XGBOOST PARAMETERS (30 minutes)**
```python
# In utils.py BST_PARAMS:
BST_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 11,  # Keep 11 for consistency
    "eta": 0.05,      # Conservative learning rate
    "max_depth": 8,   # Deeper trees
    "min_child_weight": 5,
    "gamma": 0.5,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "colsample_bylevel": 0.8,
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 1,
    "reg_alpha": 0.1,
    "reg_lambda": 1.0,
    "base_score": 0.5,
    "scale_pos_weight": 1.0,
    "grow_policy": "depthwise",  # Better for tabular data
    "random_state": 42
}
```

### 5. **INCREASE HYPERPARAMETER SEARCH (30 minutes)**
```bash
# In run_ray_tune.sh and cml.yaml:
--num-samples 50  # FROM: 5 TO: 50
--cpus-per-trial 1  # Keep reasonable for CI
```

## ðŸŽ¯ Implementation Plan for Paper-Worthy Results

### Phase 1: Emergency Fixes (Day 1 - 4 hours)
1. âœ… **Replace temporal split with stratified split** (2 hours)
2. âœ… **Fix Ray Tune search space** (1 hour) 
3. âœ… **Increase FL rounds and local rounds** (30 min)
4. âœ… **Fix XGBoost base parameters** (30 min)

### Phase 2: Optimization (Day 2 - 6 hours)
1. **Run proper hyperparameter optimization** (3 hours)
2. **Validate all classes present in training** (1 hour)
3. **Optimize federated learning strategy** (2 hours)

### Phase 3: Validation & Paper Results (Day 3 - 8 hours)
1. **End-to-end testing with all classes** (3 hours)
2. **Comprehensive evaluation on multiple metrics** (3 hours)
3. **Generate publication-quality results** (2 hours)

## ðŸŽ¯ Expected Outcomes After Fixes

### Immediate Results (After Phase 1):
- **All 11 classes present in training**
- **Accuracy: 70-80%** (vs current 35%)
- **Proper XGBoost training** (50+ trees vs 1 tree)
- **Valid federated learning** (convergence possible)

### Final Results (After Phase 3):
- **Accuracy: 85-95%** (publication worthy)
- **F1-Score: 80-90%** (excellent performance)
- **All 11 classes properly classified**
- **Robust federated learning convergence**
- **Production-ready intrusion detection system**

## ðŸ”¥ Critical File Changes Required

### 1. `dataset.py` (line ~354-396):
```python
# REPLACE temporal splitting logic with stratified split
if 'label' in df.columns and not is_unlabeled:
    train_df, test_df = train_test_split(
        df, test_size=0.2, random_state=42, stratify=df['label']
    )
    print(f"Stratified split: {len(train_df)} train, {len(test_df)} test")
    return DatasetDict({
        "train": Dataset.from_pandas(train_df),
        "test": Dataset.from_pandas(test_df)
    })
```

### 2. `ray_tune_xgboost_updated.py` (line ~283):
```python
# FIX search space
search_space = {
    "max_depth": hp.quniform("max_depth", 4, 12, 1),
    "min_child_weight": hp.quniform("min_child_weight", 1, 10, 1),
    "reg_alpha": hp.loguniform("reg_alpha", np.log(0.01), np.log(10.0)),
    "reg_lambda": hp.loguniform("reg_lambda", np.log(0.01), np.log(10.0)),
    "eta": hp.uniform("eta", 0.01, 0.3),
    "subsample": hp.uniform("subsample", 0.6, 1.0),
    "colsample_bytree": hp.uniform("colsample_bytree", 0.6, 1.0),
    "num_boost_round": hp.quniform("num_boost_round", 50, 300, 10)  # CRITICAL!
}
```

### 3. `utils.py`:
```python
NUM_LOCAL_ROUND = 20  # Increase from 2
# Update BST_PARAMS as shown above
```

### 4. `run_bagging.sh` and `run_cyclic.sh`:
```bash
--num-rounds 20  # Increase from 5
```

### 5. `cml.yaml`:
```yaml
--num-samples 50  # Increase from 5 for proper optimization
```

## âš¡ Immediate Action Required

**THIS IS A PRODUCTION EMERGENCY!** 

The current system:
- **Cannot learn 1 out of 11 classes** (9% classification failure)
- **Accuracy is 3x lower than achievable**
- **Uses only 1 boosting round** (essentially random)
- **Will fail in production**

**These fixes will transform the system from broken to publication-ready in 1 day.**

After implementing these changes, expect:
âœ… **90%+ accuracy**
âœ… **All classes learnable** 
âœ… **Proper federated convergence**
âœ… **Publication-worthy results**
âœ… **Production-ready system**
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING, ERROR
import pickle
import os
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self, dataset_type="unsw_nb15"):
        """
        Initialize the feature processor.
        Args:
            dataset_type (str): Type of dataset to process.
                Options: "unsw_nb15" (original) or "engineered" (new dataset)
        """
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        self.label_encoder = LabelEncoder()
        self.dataset_type = dataset_type
        # Define feature groups based on dataset type
        if dataset_type == "unsw_nb15":
            # Original UNSW_NB15 dataset features
            self.categorical_features = [
                'proto', 'service', 'state', 'is_ftp_login', 'is_sm_ips_ports'
            ]
            self.numerical_features = [
                'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 
                'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 
                'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 
                'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 
                'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 
                'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst'
            ]
        elif dataset_type == "engineered":
            # New engineered dataset features - all are numerical (pre-normalized)
            self.categorical_features = []
            self.numerical_features = [
                'dur', 'sbytes', 'dbytes', 'Sload', 'swin', 'smeansz', 'Sjit', 'Stime',
                'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_dst_src_ltm',
                'duration', 'jit_ratio', 'inter_pkt_ratio', 'tcp_setup_ratio',
                'byte_pkt_interaction_dst', 'load_jit_interaction_dst', 'tcp_seq_diff'
            ]
        else:
            raise ValueError(f"Unknown dataset type: {dataset_type}")
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # === LABEL COLUMN STANDARDIZATION DURING FIT ===
        # Ensure consistent label column naming during fitting
        df_copy = df.copy()
        if 'attack_cat' in df_copy.columns and 'label' not in df_copy.columns:
            log(INFO, "Standardizing 'attack_cat' column to 'label' during fit")
            df_copy = df_copy.rename(columns={'attack_cat': 'label'})
        elif 'Label' in df_copy.columns and 'label' not in df_copy.columns:
            log(INFO, "Standardizing 'Label' column to 'label' during fit")
            df_copy = df_copy.rename(columns={'Label': 'label'})
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df_copy.columns:
                unique_values = df_copy[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df_copy[df_copy[col] == val]
                        if 'label' in df_copy.columns and len(subset) > 0:
                            most_common_label = subset['label'].value_counts().idxmax()
                            label_pct = subset['label'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics - for original dataset or data validation
        # For engineered dataset, this will be minimal since data is already normalized
        if self.dataset_type == "unsw_nb15":
            for col in self.numerical_features:
                if col in df_copy.columns:
                    self.numerical_stats[col] = {
                        'mean': df_copy[col].mean(),
                        'std': df_copy[col].std(),
                        'median': df_copy[col].median(),
                        'q99': df_copy[col].quantile(0.99)
                    }
        else:
            # For engineered dataset, we only track basic stats for validation
            # No need for extensive normalization since data is already normalized
            for col in self.numerical_features:
                if col in df_copy.columns:
                    self.numerical_stats[col] = {
                        'min': df_copy[col].min(),
                        'max': df_copy[col].max(),
                        'median': df_copy[col].median(),
                    }
        # Fit label encoder for standardized 'label' column
        if 'label' in df_copy.columns:
            label_values = df_copy['label']
            if label_values.dtype == 'object' or isinstance(label_values.iloc[0], str):
                log(INFO, "Fitting label encoder for categorical labels")
                self.label_encoder.fit(label_values)
            else:
                log(INFO, "Labels are already numeric, no encoding needed")
        # For engineered dataset with numeric labels, just record the unique labels
        if 'label' in df_copy.columns and self.dataset_type == "engineered":
            self.unique_labels = sorted(df_copy['label'].unique())
            log(INFO, f"Found {len(self.unique_labels)} unique labels in engineered dataset: {self.unique_labels}")
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            # If not fitted and not training, we should fit it anyway to avoid errors
            # This is needed for the centralized evaluation case
            log(INFO, "FeatureProcessor not fitted but needed for transform. Fitting now.")
            self.fit(df)
        df = df.copy()
        # === LABEL COLUMN STANDARDIZATION ===
        # Ensure consistent label column naming throughout the pipeline
        if 'attack_cat' in df.columns and 'label' not in df.columns:
            # Convert attack_cat to standardized 'label' column for original dataset
            log(INFO, "Standardizing 'attack_cat' column to 'label' for consistent naming")
            df = df.rename(columns={'attack_cat': 'label'})
        elif 'Label' in df.columns and 'label' not in df.columns:
            # Convert uppercase 'Label' to lowercase 'label' for consistency
            log(INFO, "Standardizing 'Label' column to 'label' for consistent naming")
            df = df.rename(columns={'Label': 'label'})
        # Drop id column since it's just an identifier
        if 'id' in df.columns:
            df.drop(columns=['id'], inplace=True)
        # Transform categorical features (only needed for original dataset)
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with different approaches based on dataset type
        if self.dataset_type == "unsw_nb15":
            # Original dataset needs normalization and outlier handling
            for col in self.numerical_features:
                if col in df.columns and col in self.numerical_stats:
                    # Replace infinities
                    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                    # Cap outliers using 99th percentile - fix dtype compatibility
                    q99 = self.numerical_stats[col]['q99']
                    # Ensure q99 has the same dtype as the column to avoid FutureWarning
                    if df[col].dtype.kind in 'biufc':  # numeric dtypes
                        q99 = df[col].dtype.type(q99)
                    df.loc[df[col] > q99, col] = q99  # Cap outliers
                    # Fill NaN with median
                    median = self.numerical_stats[col]['median']
                    # Ensure median has the same dtype as the column
                    if df[col].dtype.kind in 'biufc':  # numeric dtypes
                        median = df[col].dtype.type(median)
                    df[col] = df[col].fillna(median)
        else:
            # Engineered dataset is already normalized, just handle missing values
            for col in self.numerical_features:
                if col in df.columns:
                    # Replace infinities and NaN with 0 (since data is normalized, 0 is a reasonable default)
                    df[col] = df[col].replace([np.inf, -np.inf, np.nan], 0)
        # === IMPORTANT: DO NOT DROP THE STANDARDIZED 'label' COLUMN ===
        # The label column should be preserved so that downstream components can find it
        # The preprocess_data function will handle label extraction separately
        # Only drop the original attack_cat column if it still exists after renaming
        if 'attack_cat' in df.columns:
            log(INFO, "Dropping original 'attack_cat' column after standardization")
            df.drop(columns=['attack_cat'], inplace=True)
        return df
def preprocess_data(data: Union[pd.DataFrame, Dataset], processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification for both original and engineered datasets.
    Args:
        data (Union[pd.DataFrame, Dataset]): Input DataFrame or Hugging Face Dataset
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    if processor is None:
        # Auto-detect dataset type based on columns
        if 'attack_cat' in data.columns:
            processor = FeatureProcessor(dataset_type="unsw_nb15")
        elif 'tcp_seq_diff' in data.columns:
            processor = FeatureProcessor(dataset_type="engineered")
        else:
            log(WARNING, "Could not automatically detect dataset type. Defaulting to 'unsw_nb15'.")
            processor = FeatureProcessor(dataset_type="unsw_nb15")
    # === STANDARDIZED LABEL HANDLING ===
    # Process features first (this will standardize label column names)
    features = processor.transform(data, is_training)
    # Now extract labels from the standardized 'label' column
    labels = None
    if 'label' in features.columns:
        log(INFO, "Found standardized 'label' column in processed data")
        labels = features['label'].copy()
        # Handle label encoding based on dataset type
        if processor.dataset_type == "unsw_nb15":
            # For original dataset, labels need to be encoded if they're categorical
            if labels.dtype == 'object' or isinstance(labels.iloc[0], str):
                log(INFO, "Encoding categorical labels for UNSW_NB15 dataset")
                # Ensure label encoder is fitted if needed (during training)
                if is_training and not hasattr(processor.label_encoder, 'classes_'):
                    log(INFO, "Fitting label encoder during training preprocessing.")
                    processor.label_encoder.fit(labels)
                elif not hasattr(processor.label_encoder, 'classes_') or processor.label_encoder.classes_.size == 0:
                    log(WARNING, "Label encoder not fitted, cannot transform categorical labels.")
                    try:
                        processor.label_encoder.fit(labels)
                        log(WARNING, "Fitted label encoder on non-training data chunk.")
                    except Exception as fit_err:
                        log(ERROR, f"Could not fit label encoder on non-training data: {fit_err}")
                        labels = np.full(len(data), -1, dtype=int)
                # Transform labels if encoder is ready
                if hasattr(processor.label_encoder, 'classes_') and processor.label_encoder.classes_.size > 0:
                    try:
                        labels = processor.label_encoder.transform(labels)
                    except ValueError as e:
                        log(ERROR, f"Error transforming labels: {e}. Unseen labels might exist.")
                        labels = np.full(len(data), -1, dtype=int)
            else:
                # Labels are already numeric
                labels = labels.astype(int)
        else:
            # For engineered dataset, labels should already be numeric
            log(INFO, "Using direct numeric labels from engineered dataset.")
            labels = labels.astype(int)
        # Log label distribution
        if labels is not None:
            try:
                unique_labels, counts = np.unique(labels, return_counts=True)
                label_counts = dict(zip(unique_labels, counts))
                log(INFO, f"Label distribution: {label_counts}")
            except Exception as e:
                log(WARNING, f"Could not compute label distribution: {e}")
        # Remove label column from features to avoid data leakage
        features = features.drop(columns=['label'])
        log(INFO, "Removed 'label' column from features to prevent data leakage")
    else:
        # No label column found
        log(INFO, "No 'label' column found in processed data - assuming unlabeled data")
        labels = None
    return features, labels
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Uses temporal splitting based on Stime column to avoid data leakage.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Auto-detect dataset type
    if 'attack_cat' in df.columns:
        print("Detected original UNSW_NB15 dataset with attack_cat column")
    elif 'tcp_seq_diff' in df.columns:
        print("Detected engineered dataset with normalized features")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, use temporal splitting to avoid data leakage
        # Sort by Stime column if available for temporal integrity
        if 'Stime' in df.columns:
            print("Using temporal splitting based on Stime column to avoid data leakage")
            df_sorted = df.sort_values('Stime').reset_index(drop=True)
            # Split temporally: first 80% for training, last 20% for testing
            train_size = int(0.8 * len(df_sorted))
            train_df = df_sorted.iloc[:train_size]
            test_df = df_sorted.iloc[train_size:]
            # Optional: shuffle within each set to add randomness while maintaining temporal integrity
            train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)
            test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)
            print(f"Temporal split: {len(train_df)} train samples, {len(test_df)} test samples")
            print(f"Train Stime range: {train_df['Stime'].min():.4f} to {train_df['Stime'].max():.4f}")
            print(f"Test Stime range: {test_df['Stime'].min():.4f} to {test_df['Stime'].max():.4f}")
            # Verify no temporal overlap
            if train_df['Stime'].max() <= test_df['Stime'].min():
                print("âœ“ No temporal overlap between train and test sets")
            else:
                print("âš  Warning: Temporal overlap detected between train and test sets")
            return DatasetDict({
                "train": Dataset.from_pandas(train_df),
                "test": Dataset.from_pandas(test_df)
            })
        else:
            # Fallback to stratified random split if no temporal column available
            print("No Stime column found, using stratified random split")
            train_test_dict = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label' if 'label' in df.columns else None)
            return DatasetDict({
                "train": train_test_dict["train"],
                "test": train_test_dict["test"]
            })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset (can be pandas DataFrame or Hugging Face Dataset)
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # Convert Hugging Face Dataset to pandas DataFrame if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Now process the data using the pandas DataFrame
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # --- Logging before DMatrix creation ---
    log(INFO, f"[transform_dataset_to_dmatrix] is_training={is_training}")
    log(INFO, f"[transform_dataset_to_dmatrix] Features shape: {x.shape}")
    if y is not None:
        log(INFO, f"[transform_dataset_to_dmatrix] Labels type: {type(y)}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels shape: {y.shape if hasattr(y, 'shape') else 'N/A'}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels head: {y[:5] if hasattr(y, '__len__') and len(y) > 0 else 'N/A'}")
    else:
        log(INFO, "[transform_dataset_to_dmatrix] Labels are None.")
    # --- End Logging ---
    # Handle case where preprocess_data might return None for labels (e.g., unlabeled data)
    if y is None:
        log(INFO, "No labels found in data. Creating DMatrix without labels.")
        return xgb.DMatrix(x, missing=np.nan)
    # For validation data, log label distribution to help identify issues
    if not is_training:
        # Count occurrences of each label
        label_counts = np.bincount(y.astype(int))
        # Use the correct class names for UNSW_NB15 dataset if possible
        if hasattr(processor, 'dataset_type') and processor.dataset_type == "unsw_nb15":
            label_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'] 
        else:
            # For engineered dataset, use numeric labels as names
            label_names = [str(i) for i in range(len(label_counts))]
        log(INFO, "Label distribution in validation data:")
        for i, count in enumerate(label_counts):
            if i < len(label_names):
                class_name = label_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"  {class_name}: {count}")
    return xgb.DMatrix(x, label=y, missing=np.nan)
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into train and test sets, preprocess, and return DMatrices and the fitted processor.
    Args:
        data: Input dataset (Hugging Face Dataset or pandas DataFrame)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: 
            - Training DMatrix
            - Test DMatrix
            - Fitted FeatureProcessor instance
    """
    # Convert to pandas if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use sklearn's train_test_split with shuffle=True to ensure data is properly randomized
    log(INFO, "Original data shape before splitting: %s", data.shape)
    # Set random seed for consistency
    np.random.seed(random_state)
    # Auto-detect dataset type
    if 'attack_cat' in data.columns:
        dataset_type = "unsw_nb15"
    elif 'tcp_seq_diff' in data.columns:
        dataset_type = "engineered"
    else:
        dataset_type = "unsw_nb15"  # Default
        log(WARNING, "Could not auto-detect dataset type. Defaulting to 'unsw_nb15'.")
    # Initialize appropriate feature processor
    processor = FeatureProcessor(dataset_type=dataset_type)
    # Check if 'label' column exists in data
    label_col = 'label' if dataset_type == "engineered" else 'label' 
    if label_col not in data.columns:
        log(INFO, "Warning: No '%s' column found in data. Available columns: %s", 
            label_col, data.columns.tolist())
    else:
        # Report class distribution
        label_counts = data[label_col].value_counts().to_dict()
        log(INFO, "Class distribution in original data: %s", label_counts)
    # Check for data leakage indicators
    if 'uid' in data.columns:
        uid_label_counts = data.groupby('uid')[label_col].value_counts()
        uid_with_multiple_labels = uid_label_counts.index.get_level_values(0).duplicated(keep=False)
        if not any(uid_with_multiple_labels):
            log(WARNING, "CRITICAL: Each UID has only one label, indicating potential perfect data leakage through UIDs")
    # Generate a completely different random_state for validation split
    validation_random_state = (random_state * 17 + 3) % 10000
    log(INFO, "Using different random states for train/validation split: %d/%d", 
        random_state, validation_random_state)
    # Split data ensuring complete partition separation
    if 'uid' in data.columns:
        # If we have UIDs, use them to ensure no data leakage across train/test
        log(INFO, "Using UID-based splitting to ensure no data leakage")
        unique_uids = data['uid'].unique()
        np.random.seed(validation_random_state)
        np.random.shuffle(unique_uids)
        test_size = int(len(unique_uids) * test_fraction)
        test_uids = unique_uids[:test_size]
        train_uids = unique_uids[test_size:]
        # Split based on UIDs
        train_data = data[data['uid'].isin(train_uids)].copy()
        test_data = data[data['uid'].isin(test_uids)].copy()
        log(INFO, "Split by UIDs: %d train UIDs, %d test UIDs", len(train_uids), len(test_uids))
    else:
        # If no UIDs, use standard stratified split
        train_data, test_data = train_test_split_pandas(
            data,
            test_size=test_fraction,
            random_state=validation_random_state,
            shuffle=True,  # Ensure data is shuffled for a proper split
            stratify=data[label_col] if label_col in data.columns else None  # Use stratified split if possible
        )
    # Log the shapes to verify they're different sets
    log(INFO, "Train data shape: %s, Test data shape: %s", train_data.shape, test_data.shape)
    # Verify label distributions to ensure proper stratification
    if label_col in data.columns:
        train_label_counts = train_data[label_col].value_counts().to_dict()
        test_label_counts = test_data[label_col].value_counts().to_dict()
        log(INFO, "Class distribution in train data: %s", train_label_counts)
        log(INFO, "Class distribution in test data: %s", test_label_counts)
    # Check for unique values in both sets to verify they're actually different
    if 'uid' in data.columns:
        train_uids = set(train_data['uid'].unique())
        test_uids = set(test_data['uid'].unique())
        common_uids = train_uids.intersection(test_uids)
        if common_uids:
            log(WARNING, "WARNING: Found %d UIDs in both train and test sets! This indicates data leakage.", 
                len(common_uids))
        else:
            log(INFO, "Good: Train and test sets have completely separate UIDs (no overlap).")
    # Fit processor on training data and transform both sets
    # Note: transform calls fit implicitly if is_training=True and not fitted
    train_dmatrix = transform_dataset_to_dmatrix(train_data, processor=processor, is_training=True)
    test_dmatrix = transform_dataset_to_dmatrix(test_data, processor=processor, is_training=False)
    # Log number of examples for verification
    log(INFO, "Train DMatrix has %d rows, Test DMatrix has %d rows", 
        train_dmatrix.num_row(), test_dmatrix.num_row())
    # Return the fitted processor along with DMatrices
    return train_dmatrix, test_dmatrix, processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
def create_global_feature_processor(data_file: str, output_dir: str = "outputs") -> str:
    """
    Create a global feature processor fitted on the full training dataset.
    This ensures consistent preprocessing across Ray Tune and Federated Learning.
    Args:
        data_file (str): Path to the dataset file
        output_dir (str): Directory to save the processor
    Returns:
        str: Path to the saved processor file
    """
    log(INFO, f"Creating global feature processor from: {data_file}")
    # Load full dataset
    dataset = load_csv_data(data_file)
    train_data = dataset["train"]
    train_df = train_data.to_pandas()
    # Auto-detect dataset type
    if 'attack_cat' in train_df.columns:
        dataset_type = "unsw_nb15"
    elif 'tcp_seq_diff' in train_df.columns:
        dataset_type = "engineered"
    else:
        dataset_type = "unsw_nb15"  # Default
        log(WARNING, "Could not auto-detect dataset type. Defaulting to 'unsw_nb15'.")
    # Create and fit processor on full training data
    processor = FeatureProcessor(dataset_type=dataset_type)
    processor.fit(train_df)
    # Save processor to file
    os.makedirs(output_dir, exist_ok=True)
    processor_path = os.path.join(output_dir, "global_feature_processor.pkl")
    with open(processor_path, 'wb') as f:
        pickle.dump(processor, f)
    log(INFO, f"Global feature processor saved to: {processor_path}")
    log(INFO, f"Processor type: {dataset_type}")
    log(INFO, f"Categorical features: {len(processor.categorical_features)}")
    log(INFO, f"Numerical features: {len(processor.numerical_features)}")
    return processor_path
def load_global_feature_processor(processor_path: str) -> FeatureProcessor:
    """
    Load a pre-fitted global feature processor.
    Args:
        processor_path (str): Path to the saved processor file
    Returns:
        FeatureProcessor: The loaded processor
    """
    if not os.path.exists(processor_path):
        raise FileNotFoundError(f"Global feature processor not found at: {processor_path}")
    with open(processor_path, 'rb') as f:
        processor = pickle.load(f)
    log(INFO, f"Loaded global feature processor from: {processor_path}")
    return processor
# Comment out or remove ModelPredictor if not used or complete
# class ModelPredictor:
#     """
#     Handles model prediction and dataset labeling
#     """
#     def __init__(self, model_path: str):
#         self.model = xgb.Booster()
#         self.model.load_model(model_path)
#     def predict_and_save(
#         self,
#         input_data: Union[str, pd.DataFrame],
#         output_path: str,
#         include_confidence: bool = True
#     ):
#         """
#         Predict on new data and save labeled dataset
#         """
#         # Load/preprocess input data
#         # data = self._prepare_data(input_data) # Requires _prepare_data method
#         # Generate predictions
#         # predictions = self.model.predict(data)
#         # confidence = None
#         # if include_confidence:
#         #     confidence = self.model.predict(data, output_margin=True)
#         # Save labeled dataset
#         # self._save_output(data, predictions, confidence, output_path) # Requires _save_output method
</file>

<file path="FL-CML-Pipeline-Analysis.md">
--- Repository Documentation ---

```markdown
# Federated Learning with Flower (XGBoost Comprehensive) Documentation for AI

This document provides comprehensive documentation for the Federated Learning with Flower (XGBoost Comprehensive) repository, tailored for understanding and utilization by AI systems.

## 1. Repository Purpose and "What Is It" Summary

This repository implements a Federated Learning (FL) system using the Flower framework to train XGBoost models in a distributed manner. Its primary focus is on collaborative training of intrusion detection models on network traffic data without requiring raw data exchange between participants (clients).

The project integrates several key components:
*   **Data Handling:** Loading, preprocessing, and partitioning of network traffic datasets.
*   **Federated Learning Core:** Implementing server and client logic based on the Flower framework.
*   **Model Training:** Utilizing XGBoost for classification.
*   **Training Strategies:** Supporting both **FedXgbBagging** (parallel, aggregation-based) and **FedXgbCyclic** (sequential, model-passing) strategies from Flower.
*   **Configuration:** Using command-line arguments and implicit configuration (like `tuned_params.py`) for experimental settings.
*   **Hyperparameter Tuning:** Integration with Ray Tune to optimize XGBoost parameters.
*   **Consistent Preprocessing:** Implementation of a global feature processor to ensure uniform data handling across tuning and FL phases, preventing data leakage.
*   **Evaluation & Visualization:** Tools for evaluating model performance (metrics, confusion matrices, learning curves) and saving predictions.

The overall goal is to demonstrate a privacy-preserving approach to training powerful tree-based models like XGBoost on distributed datasets, with a focus on cybersecurity applications (network intrusion detection).

## 2. Quick Start

To get the project up and running for basic federated training:

**Prerequisites:**

*   Python 3.8+
*   pip (Python package manager)
*   Git
*   (Optional, but recommended for full CML workflow) Docker

**Installation:**

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
    cd FL-CML-Pipeline
    ```
2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(This installs core dependencies including `flwr` simulation, `xgboost`, `ray[tune]`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`, `datasets`, `flwr-datasets`).*
    *Note: `pyproject.toml` also lists dependencies, but `requirements.txt` is the primary installation method used in scripts and workflows.*

**Data Preparation:**

*   Place your CSV data file in the `data/received/` directory. The system is configured to primarily use `data/received/final_dataset.csv`. Ensure this file exists or modify the scripts (`run.py`, `run_bagging.sh`, `run_ray_tune.sh`, `client.py`, `server.py`) to point to your data file. The `dataset.py` module supports multi-class UNSW_NB15 and an engineered dataset format.

**Running a Basic Federated Training Session (Bagging):**

This command starts a Flower server and multiple Flower clients using the `FedXgbBagging` strategy.

1.  **Ensure the global feature processor is created:**
    ```bash
    python create_global_processor.py --data-file data/received/final_dataset.csv --output-dir outputs --force
    ```
2.  **Run the bagging script:**
    ```bash
    ./run_bagging.sh
    ```
    *(This script will start the server and several clients in the background and wait for them to finish. Output will be logged to the console and saved in the `outputs/` directory).*

**Running a Basic Federated Training Session (Cyclic):**

This command starts a Flower server and multiple Flower clients using the `FedXgbCyclic` strategy.

```bash
./run_cyclic.sh
```
*(This script starts the server and clients similarly to `run_bagging.sh`).*

**Running the Full Pipeline (Includes Processor Creation, Ray Tune, and Simulation):**

The `run.py` script orchestrates the entire process: creating the global processor, running Ray Tune hyperparameter optimization, applying tuned parameters, and finally running the federated learning simulation.

```bash
python run.py
```
*(This is the most comprehensive quick start for testing the full integrated workflow).*

## 3. Configuration Options

The project uses a combination of command-line arguments and internal constants/generated files (`tuned_params.py`) for configuration.

**Command-Line Arguments:**

Scripts like `client.py`, `server.py`, `sim.py`, `ray_tune_xgboost_updated.py`, `create_global_processor.py`, `use_tuned_params.py`, and `use_saved_model.py` accept arguments defined in `utils.py`, `ray_tune_xgboost_updated.py`, `create_global_processor.py`, `use_tuned_params.py`, and `use_saved_model.py` respectively.

Key arguments managed via `utils.py` parsers:

*   **`--train-method`**: (`server.py`, `client.py`, `sim.py`) Choose between `bagging` or `cyclic` FL strategy.
*   **`--num-rounds`**: (`server.py`, `sim.py`) Number of federated learning rounds.
*   **`--pool-size`**: (`server.py`, `sim.py`) Total number of clients available in the simulation pool.
*   **`--num-clients-per-round`**: (`server.py`, `sim.py`) Number of clients selected for training in each round.
*   **`--num-evaluate-clients`**: (`server.py`, `sim.py`) Number of clients selected for evaluation in each round (used by some strategies).
*   **`--centralised-eval`**: (`server.py`, `sim.py`, `client_utils.py`, `client.py`) Flag to enable evaluation on a separate, centralized test set managed by the server.
*   **`--partitioner-type`**: (`client.py`, `sim.py`) Data partitioning strategy (`uniform`, `linear`, `square`, `exponential`).
*   **`--num-partitions`**: (`client.py`) Number of data partitions to create (matches `--pool-size` in `sim.py`).
*   **`--partition-id`**: (`client.py`) The ID of the data partition assigned to a specific client instance.
*   **`--seed`**: (`client.py`, `sim.py`) Random seed for data splitting.
*   **`--test-fraction`**: (`client.py`, `sim.py`) Fraction of local data to use for client-side evaluation.
*   **`--scaled-lr`**: (`client.py`, `sim.py`) Flag to scale the client-side learning rate based on the number of clients (relevant for bagging).
*   **`--csv-file`**: (`client.py`, `sim.py`, `create_global_processor.py`) Path to the main dataset CSV file. Overridden by `--data-file`, `--train-file`, `--test-file` in tuning scripts.

**Internal Configuration (`utils.py`):**

*   **`BST_PARAMS`**: A dictionary defining the default XGBoost parameters used by clients and the server. This is the base configuration which can be overridden by tuned parameters.
*   **`NUM_LOCAL_ROUND`**: Default number of local boosting rounds each client performs. This can be overridden by `tuned_params.py`.

**Dynamic Configuration (`tuned_params.py`):**

*   The `use_tuned_params.py` script generates `tuned_params.py` based on the output of `ray_tune_xgboost_updated.py`.
*   If `tuned_params.py` exists, it overrides `BST_PARAMS` and `NUM_LOCAL_ROUND` in `client_utils.py`, automatically applying the optimized parameters to the FL clients.

**Hydra Configuration (Indicated in `README.md`, but `conf/base.yaml` not provided):**

*   The `README.md` mentions Hydra for experiment settings, pointing to `conf/base.yaml`. While the file content is not available in this merged view, the description in `README.md` indicates it would define parameters like `num_rounds`, `num_clients`, `batch_size`, `num_clients_per_round_fit`, `num_clients_per_round_eval`, and `local_epochs`. It suggests these can be overridden via the command line (e.g., `python run.py num_rounds=20`).
*   The `server.py` and `client.py` scripts as provided **do not explicitly use Hydra**, relying on command-line arguments parsed by `utils.py`. The `run.py` script orchestrates other scripts via subprocess calls, effectively wrapping their command-line interfaces.
*   The `.hydra` directory in the output structure suggests Hydra *was* intended or previously used, and the `setup_output_directory` copies `.hydra` files if they exist in the current directory. However, based *only* on the provided Python scripts (`server.py`, `client.py`, `sim.py`), configuration is primarily via `argparse`.

**Summary of Configuration Sources:**

1.  **Default XGBoost Parameters:** `utils.py:BST_PARAMS`
2.  **Default Local Rounds:** `utils.py:NUM_LOCAL_ROUND`
3.  **Optimized Parameters:** `tuned_params.py:TUNED_PARAMS` (overrides `BST_PARAMS` in `client_utils.py` if present)
4.  **Optimized Local Rounds:** `tuned_params.py:NUM_LOCAL_ROUND` (overrides `utils.py:NUM_LOCAL_ROUND` if present)
5.  **Command-Line Arguments:** Parsed by `utils.py` functions (`client_args_parser`, `server_args_parser`, `sim_args_parser`) and specific scripts. These override defaults or tuned parameters where applicable (e.g., `--train-method`).
6.  **Global Feature Processor:** Configured implicitly via `create_global_processor.py` and then loaded by other modules. Its path and fitting state are configuration aspects.

**How to Configure:**

*   **Basic FL:** Modify `run_bagging.sh` or `run_cyclic.sh` scripts or the `sim.py` script directly, or pass command-line arguments when running them (e.g., `./run_bagging.sh --num-rounds 10 --num-clients-per-round 10`).
*   **Hyperparameter Tuning:** Modify `run_ray_tune.sh` or pass arguments like `--num-samples`, `--cpus-per-trial`, `--data-file`, `--output-dir`.
*   **Apply Tuned Params:** Run `python use_tuned_params.py --params-file <path_to_json>` (defaults to `./tune_results/best_params.json`).
*   **Use Saved Model:** Run `python use_saved_model.py --model_path <path> --data_path <path> --output_path <path> [--has_labels]`.
*   **Default XGBoost Params:** Modify `utils.py:BST_PARAMS` (less recommended if using tuning).
*   **Global Processor:** Configure input data and output directory via `create_global_processor.py` arguments.

## 4. Module Documentation

This section details the public interfaces and functionality of the key Python modules in the repository.

### `utils` Module

**Purpose:** Provides shared utility functions, default constants for XGBoost parameters and local rounds, and argument parsers for client, server, and simulation scripts.

**Installation / Import:** Standard Python module import after cloning the repository and installing dependencies (`pip install -r requirements.txt`).
```python
import utils # or from utils import BST_PARAMS, client_args_parser, etc.
```

**Public API:**

*   `NUM_LOCAL_ROUND: int`
    *   **Description:** Default number of local boosting rounds each client performs during fitting. Can be overridden by `tuned_params.py`.
    *   **Value:** 2
*   `BST_PARAMS: dict`
    *   **Description:** Dictionary containing default XGBoost training parameters for multi-class classification (tuned for UNSW_NB15). These parameters can be overridden by `tuned_params.py`.
    *   **Value:**
        ```python
        {
          "objective": "multi:softprob",
          "num_class": 11,
          "eta": 0.05,
          "max_depth": 6,
          "min_child_weight": 10,
          "gamma": 1.0,
          "subsample": 0.7,
          "colsample_bytree": 0.6,
          "colsample_bylevel": 0.6,
          "nthread": 16,
          "tree_method": "hist",
          "eval_metric": ["mlogloss", "merror"],
          "max_delta_step": 5,
          "reg_alpha": 0.8,
          "reg_lambda": 0.8,
          "base_score": 0.5,
          "scale_pos_weight": 1.0,
          "grow_policy": "lossguide",
          "normalize_type": "tree",
          "random_state": 42
        }
        ```
*   `client_args_parser() -> argparse.Namespace`
    *   **Description:** Creates and returns an `argparse.ArgumentParser` configured with standard command-line arguments for the client script (`client.py`).
    *   **Args:** None. Parses `sys.argv`.
    *   **Returns:** Parsed arguments as a `Namespace` object.
    *   **Key Arguments Handled:** `--train-method`, `--num-partitions`, `--partitioner-type`, `--partition-id`, `--seed`, `--test-fraction`, `--centralised-eval`, `--scaled-lr`, `--csv-file`.
*   `server_args_parser() -> argparse.Namespace`
    *   **Description:** Creates and returns an `argparse.ArgumentParser` configured with standard command-line arguments for the server script (`server.py`).
    *   **Args:** None. Parses `sys.argv`.
    *   **Returns:** Parsed arguments as a `Namespace` object.
    *   **Key Arguments Handled:** `--train-method`, `--pool-size`, `--num-rounds`, `--num-clients-per-round`, `--num-evaluate-clients`, `--centralised-eval`.
*   `sim_args_parser() -> argparse.Namespace`
    *   **Description:** Creates and returns an `argparse.ArgumentParser` configured with standard command-line arguments for the simulation script (`sim.py`). Combines arguments for both server and client aspects of the simulation.
    *   **Args:** None. Parses `sys.argv`.
    *   **Returns:** Parsed arguments as a `Namespace` object.
    *   **Key Arguments Handled:** `--train-method`, `--pool-size`, `--num-rounds`, `--num-clients-per-round`, `--num-evaluate-clients`, `--centralised-eval`, `--num-cpus-per-client`, `--partitioner-type`, `--seed`, `--test-fraction`, `--centralised-eval-client`, `--scaled-lr`, `--csv-file`.

**Dependencies:**
*   `argparse`

**Advanced Usage Examples:**

```python
# Example: Override default BST_PARAMS for a specific script
# Note: This requires modifying the script itself or using a wrapper
from utils import BST_PARAMS
# Create a modified version
MY_CUSTOM_PARAMS = BST_PARAMS.copy()
MY_CUSTOM_PARAMS['eta'] = 0.1 # Change learning rate
# Pass MY_CUSTOM_PARAMS to XgbClient constructor in client.py
```

```bash
# Example: Run server with different parameters from the command line
python server.py --num-rounds 20 --num-clients-per-round 10
```

### `dataset` Module

**Purpose:** Provides comprehensive functionality for loading, preprocessing, splitting, partitioning, and transforming network traffic datasets for XGBoost training. Crucially includes the `FeatureProcessor` for consistent data handling and functions to create/load a global processor.

**Installation / Import:** Standard Python module import after cloning the repository and installing dependencies (`pip install -r requirements.txt`).
```python
import dataset # or from dataset import load_csv_data, FeatureProcessor, etc.
```

**Public API:**

*   `CORRELATION_TO_PARTITIONER: dict`
    *   **Description:** Mapping from string names (`"uniform"`, `"linear"`, `"square"`, `"exponential"`) to `flwr_datasets` partitioner classes.
*   `FeatureProcessor` Class
    *   **Description:** Handles feature preprocessing (categorical encoding, numerical scaling/outlier handling) and label encoding while preventing data leakage by fitting only on training data. Supports different dataset types (`"unsw_nb15"`, `"engineered"`).
    *   **`__init__(self, dataset_type="unsw_nb15")`**
        *   **Description:** Initializes the processor.
        *   **Args:**
            *   `dataset_type (str)`: Specifies the dataset schema (`"unsw_nb15"` or `"engineered"`).
    *   **`fit(self, df: pd.DataFrame) -> None`**
        *   **Description:** Fits the preprocessing parameters (encoders, stats) using the provided training DataFrame. This method is idempotent.
        *   **Args:**
            *   `df (pd.DataFrame)`: Training data to fit the processor on.
    *   **`transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame`**
        *   **Description:** Transforms the input DataFrame using the fitted parameters. If `is_training` is True and the processor is not fitted, it will fit first. Handles standardizing label column names and dropping 'id'.
        *   **Args:**
            *   `df (pd.DataFrame)`: Data to transform.
            *   `is_training (bool)`: Flag indicating if the data is for training (influences implicit fitting).
        *   **Returns:** Transformed DataFrame with processed features and standardized 'label' column (if present).
    *   **`is_fitted: bool`**
        *   **Description:** Property indicating if the processor has been fitted.
    *   **`categorical_features: list`**
        *   **Description:** List of identified categorical feature names.
    *   **`numerical_features: list`**
        *   **Description:** List of identified numerical feature names.
*   `preprocess_data(data: Union[pd.DataFrame, Dataset], processor: FeatureProcessor = None, is_training: bool = False) -> Tuple[pd.DataFrame, Union[pd.Series, None]]`
    *   **Description:** High-level preprocessing function. Converts Hugging Face `Dataset` to pandas, applies the `FeatureProcessor`, extracts and encodes the 'label' column (if `is_training`), and removes the 'label' column from features.
    *   **Args:**
        *   `data (Union[pd.DataFrame, Dataset])`: Input data.
        *   `processor (FeatureProcessor, optional)`: An existing processor instance. If None, one is created and fitted (on training data).
        *   `is_training (bool)`: Flag passed to `processor.transform`.
    *   **Returns:** A tuple `(features_df, labels_series)`. `labels_series` is None if no label column is found.
*   `load_csv_data(file_path: str) -> DatasetDict`
    *   **Description:** Loads data from a CSV file into a Hugging Face `DatasetDict`. Attempts temporal splitting based on the 'Stime' column to avoid data leakage. Falls back to stratified random splitting if 'Stime' is not available. Handles unlabeled test sets.
    *   **Args:**
        *   `file_path (str)`: Path to the CSV file.
    *   **Returns:** A `DatasetDict` with 'train' and 'test' splits.
*   `instantiate_partitioner(partitioner_type: str, num_partitions: int) -> Partitioner`
    *   **Description:** Creates and returns a `flwr_datasets` partitioner instance based on the specified type and number of partitions.
    *   **Args:**
        *   `partitioner_type (str)`: Type of partitioner (must be a key in `CORRELATION_TO_PARTITIONER`).
        *   `num_partitions (int)`: Number of partitions.
    *   **Returns:** An initialized `flwr_datasets` Partitioner object.
*   `transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False) -> xgb.DMatrix`
    *   **Description:** Converts input data (DataFrame or Dataset) into an `xgboost.DMatrix` after applying preprocessing. Handles cases with and without labels.
    *   **Args:**
        *   `data`: Input data.
        *   `processor (FeatureProcessor, optional)`: Processor instance. If None, one is created and fitted.
        *   `is_training (bool)`: Flag passed to `preprocess_data`.
    *   **Returns:** An `xgboost.DMatrix`.
*   `train_test_split(data, test_fraction: float = 0.2, random_state: int = 42) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]`
    *   **Description:** Performs a train/test split on the data, preprocesses both splits using a *single fitted processor*, and returns the resulting DMatrices and the fitted processor. Attempts UID-based splitting if a 'uid' column exists to prevent leakage.
    *   **Args:**
        *   `data`: Input data (Dataset or DataFrame).
        *   `test_fraction (float)`: Fraction for the test set.
        *   `random_state (int)`: Seed for splitting.
    *   **Returns:** Tuple `(train_dmatrix, test_dmatrix, fitted_processor)`.
*   `resplit(dataset: DatasetDict) -> DatasetDict`
    *   **Description:** Adjusts the train/test split in a `DatasetDict`, specifically moving up to 10,000 samples from the training set to the test set. Used to increase centralized test set size.
    *   **Args:**
        *   `dataset (DatasetDict)`: Input dataset dictionary.
    *   **Returns:** Adjusted `DatasetDict`.
*   `create_global_feature_processor(data_file: str, output_dir: str = "outputs") -> str`
    *   **Description:** Loads the full training dataset from a file, creates and fits a `FeatureProcessor` on it, and saves the fitted processor to a pickle file. Ensures consistent preprocessing across different modules.
    *   **Args:**
        *   `data_file (str)`: Path to the dataset CSV file (assumed to contain training data or be split temporally).
        *   `output_dir (str)`: Directory to save the processor file.
    *   **Returns:** Absolute path to the saved processor file.
*   `load_global_feature_processor(processor_path: str) -> FeatureProcessor`
    *   **Description:** Loads a pre-fitted `FeatureProcessor` instance from a pickle file.
    *   **Args:**
        *   `processor_path (str)`: Path to the saved processor file.
    *   **Returns:** The loaded `FeatureProcessor`.

**Dependencies:**
*   `xgboost`
*   `pandas`
*   `numpy`
*   `sklearn` (specifically `sklearn.preprocessing`, `sklearn.model_selection`, `sklearn.compose`, `sklearn.pipeline`)
*   `datasets` (Hugging Face datasets library)
*   `flwr_datasets`
*   `flwr.common.logger`
*   `typing`
*   `pickle`
*   `os`

**Advanced Usage Examples:**

```python
import dataset
import pandas as pd
# Assume 'my_data.csv' is an engineered dataset
df = pd.read_csv("my_data.csv")
# Create and fit processor on the full data before splitting/partitioning for FL
# (This is what create_global_feature_processor does)
processor = dataset.FeatureProcessor(dataset_type="engineered")
processor.fit(df)
# Now you can use this fitted processor to transform data chunks for different clients
# or for prediction
client_data_df = df.sample(frac=0.1) # Example subset for a client
client_dmatrix = dataset.transform_dataset_to_dmatrix(client_data_df, processor=processor, is_training=False)
print(f"Transformed client data shape: {client_dmatrix.num_row()} rows, {client_dmatrix.num_col()} features")
```

```python
# Example: Manually load and preprocess data using the global processor
import dataset
global_processor_path = "outputs/global_feature_processor.pkl"
try:
    global_processor = dataset.load_global_feature_processor(global_processor_path)
    print("Loaded global processor.")
    # Assume you have a new, raw dataframe 'new_unlabeled_df'
    # transformed_df = global_processor.transform(new_unlabeled_df, is_training=False)
    # new_dmatrix = dataset.transform_dataset_to_dmatrix(transformed_df, processor=global_processor, is_training=False)
    # print(f"New data transformed into DMatrix with shape: {new_dmatrix.num_row()} rows, {new_dmatrix.num_col()} features")
except FileNotFoundError:
    print("Global processor not found. Run create_global_processor.py first.")
```

### `client_utils` Module

**Purpose:** Implements the Flower client-side logic for training and evaluating an XGBoost model. Contains the `XgbClient` class, which interfaces with the Flower framework. Handles parameter exchange, local training (`fit`), local evaluation (`evaluate`), and predictions.

**Installation / Import:** Standard Python module import. Used internally by `client.py`.
```python
import client_utils # or from client_utils import XgbClient
```

**Public API:**

*   `BST_PARAMS: dict`
    *   **Description:** Default XGBoost parameters. Imported directly from `utils.py`. Used if no `params` are provided to `XgbClient` constructor and `use_tuned_params` is False or `tuned_params.py` is not found.
    *   **Value:** (See `utils.py`)
*   `TUNED_PARAMS: dict`
    *   **Description:** Optimized XGBoost parameters. Attempted to be imported dynamically from `tuned_params.py` if the file exists. Used if `use_tuned_params` is True in `XgbClient` constructor. Defaults to `BST_PARAMS` if import fails or file is missing.
    *   **Value:** Loaded from `tuned_params.py` or `BST_PARAMS`.
*   `XgbClient` Class
    *   **Description:** A Flower `Client` implementation specifically for training XGBoost models.
    *   **`__init__(self, train_dmatrix, valid_dmatrix, num_train, num_val, num_local_round, cid, params=None, train_method="cyclic", is_prediction_only=False, unlabeled_dmatrix=None, use_tuned_params=True)`**
        *   **Description:** Initializes the XGBoost client with local data, configuration, and ID.
        *   **Args:**
            *   `train_dmatrix (xgb.DMatrix)`: Local training data.
            *   `valid_dmatrix (xgb.DMatrix)`: Local validation data.
            *   `num_train (int)`: Number of training examples.
            *   `num_val (int)`: Number of validation examples.
            *   `num_local_round (int)`: Number of local boosting rounds per FL round.
            *   `cid (str)`: Client ID.
            *   `params (dict, optional)`: XGBoost parameters. If None, uses `TUNED_PARAMS` (if `use_tuned_params` is True) or `BST_PARAMS`.
            *   `train_method (str)`: Training method (`"bagging"` or `"cyclic"`), affects `_local_boost`.
            *   `is_prediction_only (bool)`: If True, the client skips training (`fit`) and only performs evaluation/prediction. (Note: Current `client.py` sets this to False).
            *   `unlabeled_dmatrix (xgb.DMatrix, optional)`: Data for making predictions (if needed).
            *   `use_tuned_params (bool)`: If True, attempts to use `TUNED_PARAMS` loaded from `tuned_params.py` if `params` is None.
    *   **`get_parameters(self, ins: GetParametersIns) -> GetParametersRes`**
        *   **Description:** Implements the Flower `get_parameters` method. Returns empty parameters as XGBoost model parameters are transferred differently (serialized model in `FitRes`).
        *   **Args:**
            *   `ins (GetParametersIns)`: Input from server (ignored).
        *   **Returns:** `GetParametersRes` with empty parameters.
    *   **`fit(self, ins: FitIns) -> FitRes`**
        *   **Description:** Implements the Flower `fit` method. Performs local XGBoost training rounds, optionally updates an existing global model, and serializes the resulting model (or partial model for bagging) for the server. Includes sample weighting for class imbalance and handles first round training.
        *   **Args:**
            *   `ins (FitIns)`: Contains global parameters (if any) and configuration (including `"global_round"`).
        *   **Returns:** `FitRes` containing the local model parameters (as bytes), number of examples used, and empty metrics.
    *   **`evaluate(self, ins: EvaluateIns) -> EvaluateRes`**
        *   **Description:** Implements the Flower `evaluate` method. Loads the global model, evaluates it on the local validation data, calculates multi-class metrics (precision, recall, F1, accuracy, mlogloss, confusion matrix), and saves predictions locally.
        *   **Args:**
            *   `ins (EvaluateIns)`: Contains global parameters and configuration (including `"global_round"` and `"output_dir"`).
        *   **Returns:** `EvaluateRes` containing the evaluation loss (mlogloss), number of examples used, and a dictionary of metrics.
    *   **`_local_boost(self, bst_input) -> xgb.Booster`** (Internal Helper)
        *   **Description:** Performs the actual local boosting rounds on an XGBoost Booster instance. Handles extracting relevant trees for the "bagging" method.
        *   **Args:**
            *   `bst_input (xgb.Booster)`: The input XGBoost model.
        *   **Returns:** The updated `xgb.Booster` instance.

**Dependencies:**
*   `xgboost`
*   `sklearn.metrics`
*   `flwr` (`flwr.client`, `flwr.common`, `flwr.common.logger`)
*   `numpy`
*   `pandas`
*   `os`
*   `importlib.util`
*   `sklearn.utils.class_weight`
*   `server_utils` (for `save_predictions_to_csv`)

**Advanced Usage Examples:**

```python
# Example: Manually instantiate XgbClient with custom parameters (bypassing tuned_params.py)
import client_utils
import xgboost as xgb
# Assume train_dm, valid_dm, num_train, num_val, cid are defined
custom_params = {
    'objective': 'multi:softprob',
    'num_class': 11,
    'eta': 0.01,
    'max_depth': 3,
    # ... other params
}
client = client_utils.XgbClient(
    train_dmatrix=train_dm,
    valid_dmatrix=valid_dm,
    num_train=num_train,
    num_val=num_val,
    num_local_round=5, # Custom local rounds
    cid="client_manual",
    params=custom_params, # Explicitly provide params
    use_tuned_params=False # Ensure tuned params are not used
)
# Then start the client with fl.client.start_client(..., client=client, ...)
```

### `server_utils` Module

**Purpose:** Provides utility functions and classes for the Flower server, including handling output directories, saving results, managing evaluation configuration and aggregation, loading/saving models, making predictions with saved models, and generating visualizations. Includes a custom client manager for the cyclic strategy.

**Installation / Import:** Standard Python module import. Used internally by `server.py` and `client_utils.py`.
```python
import server_utils # or from server_utils import get_evaluate_fn, save_predictions_to_csv, etc.
```

**Public API:**

*   `setup_output_directory() -> str`
    *   **Description:** Creates a unique output directory based on the current date and time (`outputs/YYYY-MM-DD/HH-MM-SS/`). Also creates a `.hydra` subdirectory and copies existing `.hydra` files if found.
    *   **Args:** None.
    *   **Returns:** Path to the created output directory.
*   `save_results_pickle(results, output_dir: str)`
    *   **Description:** Saves a Python object (typically the Flower `history` object or a dictionary containing results) to a pickle file (`results.pkl`) within the specified directory.
    *   **Args:**
        *   `results`: The Python object to save.
        *   `output_dir (str)`: Directory path.
*   `eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]`
    *   **Description:** Generates the configuration dictionary passed to clients for the `evaluate` round. Includes the `global_round` number and optionally the `output_dir`.
    *   **Args:**
        *   `rnd (int)`: Current server round number.
        *   `output_dir (str, optional)`: Output directory path to include in the config.
    *   **Returns:** Configuration dictionary.
*   `save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None)`
    *   **Description:** Saves a dictionary of evaluation metrics to a JSON file (e.g., `eval_results_round_X.json`) in the specified directory.
    *   **Args:**
        *   `eval_metrics (Dict)`: Dictionary of metrics.
        *   `round_num (int or str)`: Round number or identifier (e.g., "aggregated").
        *   `output_dir (str, optional)`: Directory to save to

--- End of Documentation ---
</file>

<file path="go_to_work.sh">
#!/bin/bash
# Run Python scripts in the background and store their process IDs (PIDs)
python3 data/receiving_data.py &
PID1=$!
python3 data/livepreprocessing_socket.py &
PID2=$!
# Wait for 1 minute
sleep 30
# Kill the Python scripts
kill $PID1 $PID2
# Run Git commands and GitHub workflow
git pull
./commit.sh
gh workflow run cml.yaml
# Wait for 5 minutess
sleep 300
git pull
</file>

<file path="model_performance.md">
# XGBoost Model Performance Report

## Ray Tune Hyperparameter Optimization Results

### Best Hyperparameters Found:
```json
{
  "colsample_bytree": 0.7516518865459294,
  "eta": 0.023418154481471127,
  "max_depth": 6.0,
  "min_child_weight": 8.0,
  "num_boost_round": 8.0,
  "reg_alpha": 0.18887659874307602,
  "reg_lambda": 0.006673107568162361,
  "subsample": 0.7081559047661303
}
```

### Ray Tune Validation Performance:
- **Accuracy**: 74.09%
- **Precision**: 76.57%
- **Recall**: 74.09%
- **F1 Score**: 73.08%
- **Multi-class Log Loss**: 1.9652

## Federated Learning Results

### Training Configuration:
- **Number of Rounds**: 5
- **Clients per Round**: 5
- **Dataset**: UNSW-NB15 (165,000 samples)
- **Classes**: 11 (multi-class cybersecurity attack classification)
- **Train/Test Split**: Temporal split (132k train, 33k test)

### Centralized Evaluation Performance:
| Round | Accuracy | Precision | Recall | F1 Score | Loss |
|-------|----------|-----------|--------|----------|------|
| 1     | 35.62%   | 34.15%    | 35.62% | 33.08%   | 2.209|
| 2     | 35.59%   | 34.06%    | 35.59% | 33.00%   | 2.206|
| 3     | 35.69%   | 34.10%    | 35.69% | 33.06%   | 2.204|
| 4     | 35.68%   | 34.10%    | 35.68% | 33.05%   | 2.202|
| 5     | 35.68%   | 34.10%    | 35.68% | 33.05%   | 2.201|

### Key Improvements:
- **Hyperparameter Optimization**: Ray Tune found optimal parameters improving validation accuracy to 74%
- **Consistent Performance**: Federated learning maintained stable performance across all rounds
- **Class Balance**: Successfully handled multi-class imbalanced dataset with sample weighting
- **Temporal Splitting**: Prevented data leakage using time-based train/test split

### Technical Features:
- âœ… Global feature processor for consistent preprocessing
- âœ… Sample weighting for class imbalance
- âœ… Comprehensive visualization (confusion matrices, ROC curves, PR curves)
- âœ… Per-round evaluation metrics and predictions
- âœ… Temporal data splitting to prevent leakage

## Notes:
- Performance gap between Ray Tune validation (74%) and federated learning (36%) suggests opportunity for federated-specific optimization
- Consistent performance across federated rounds indicates stable convergence
- Multi-class cybersecurity classification is inherently challenging with 11 classes
</file>

<file path="original_bst_params.json">
{
  "objective": "multi:softprob",
  "num_class": 11,
  "eta": 0.05,
  "max_depth": 6,
  "min_child_weight": 10,
  "gamma": 1.0,
  "subsample": 0.7,
  "colsample_bytree": 0.6,
  "colsample_bylevel": 0.6,
  "nthread": 16,
  "tree_method": "hist",
  "eval_metric": [
    "mlogloss",
    "merror"
  ],
  "max_delta_step": 5,
  "reg_alpha": 0.8,
  "reg_lambda": 0.8,
  "base_score": 0.5,
  "scale_pos_weight": 1.0,
  "grow_policy": "lossguide",
  "normalize_type": "tree",
  "random_state": 42
}
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.4.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "xgboost-comprehensive"
version = "0.1.0"
description = "Federated XGBoost with Flower (comprehensive)"
authors = ["The Flower Authors <hello@flower.ai>"]

[tool.poetry.dependencies]
python = ">=3.8,<3.11"
flwr = { extras = ["simulation"], version = ">=1.7.0,<2.0" }
flwr-datasets = ">=0.2.0,<1.0.0"
xgboost = ">=2.0.0,<3.0.0"
</file>

<file path="ray_tune_README.md">
# Ray Tune XGBoost Hyperparameter Optimization

This guide explains how to use Ray Tune for optimizing the hyperparameters of the XGBoost classifier in our federated learning pipeline.

## Overview

Ray Tune is a powerful library for hyperparameter tuning that can efficiently find optimal parameters for machine learning models. We've integrated Ray Tune with our XGBoost implementation to achieve better model performance through automated hyperparameter search.

## Requirements

Make sure you have all the required packages installed:

```bash
pip install -r requirements.txt
```

## Hyperparameter Tuning Process

### 1. Run Ray Tune Optimization

Use the `run_ray_tune.sh` script to start the optimization process:

```bash
# Basic usage
bash run_ray_tune.sh --data-file path/to/your/data.csv

# Advanced usage with more options
bash run_ray_tune.sh \
  --data-file path/to/your/data.csv \
  --num-samples 20 \
  --cpus-per-trial 2 \
  --gpu-fraction 0.2 \
  --output-dir ./my_tuning_results
```

#### Available Options

- `--data-file`: Path to the CSV data file (required)
- `--num-samples`: Number of hyperparameter combinations to try (default: 10)
- `--cpus-per-trial`: CPUs to allocate per trial (default: 1)
- `--gpu-fraction`: Fraction of GPU to use per trial (e.g., 0.1 for 10%)
- `--output-dir`: Directory to save results (default: ./tune_results)

### 2. Apply Tuned Parameters to Federated Learning

After tuning is complete, use the `use_tuned_params.py` script to integrate the best parameters into your federated learning system:

```bash
python use_tuned_params.py --params-file ./tune_results/best_params.json
```

This will:
1. Backup the original parameters to `original_bst_params.json`
2. Create a `tuned_params.py` file with the optimized parameters
3. Provide instructions on how to use these parameters

### 3. Run Federated Learning with Tuned Parameters

The federated learning system will automatically detect and use the tuned parameters if:

1. The `tuned_params.py` file exists in the project directory
2. You haven't explicitly provided custom parameters to the XgbClient

## How It Works

The Ray Tune optimization process:

1. **Search Space Definition**: We define a search space for hyperparameters like `max_depth`, `min_child_weight`, `eta`, etc.
2. **ASHA Scheduler**: We use the Asynchronous Successive Halving Algorithm (ASHA) for early stopping of poorly performing trials
3. **Parallel Execution**: Multiple hyperparameter combinations are evaluated in parallel
4. **Best Model Selection**: The best performing model is identified based on validation metrics
5. **Model Persistence**: The best model and its parameters are saved for later use

## Parameters Being Tuned

The following XGBoost parameters are optimized:

- `max_depth`: Maximum depth of a tree
- `min_child_weight`: Minimum sum of instance weight needed in a child
- `eta`: Learning rate
- `subsample`: Subsample ratio of the training instances
- `colsample_bytree`: Subsample ratio of columns when constructing each tree
- `reg_alpha`: L1 regularization term on weights
- `reg_lambda`: L2 regularization term on weights
- `num_boost_round`: Number of boosting rounds

## GPU Support

If you have a GPU available, you can use it to speed up the tuning process by specifying a `--gpu-fraction` value. This enables XGBoost's GPU acceleration via the `gpu_hist` tree method.

## Monitoring and Results

During tuning, progress is logged to the console. After completion, you'll find these files in the output directory:

- `best_params.json`: JSON file containing the best hyperparameters
- `best_model.json`: XGBoost model trained with the best hyperparameters
- `progress.csv`: CSV file with metrics for all trials
- Detailed trial information in subdirectories

## Troubleshooting

- **Memory Issues**: If you encounter memory errors, try reducing `--num-samples` or `--cpus-per-trial`
- **GPU Errors**: If you face GPU-related errors, try removing the `--gpu-fraction` option or installing the appropriate CUDA toolkit
- **Import Errors**: Ensure all dependencies are installed with `pip install -r requirements.txt`

## Advanced Usage

### Custom Search Space

To customize the hyperparameter search space, modify the `search_space` dictionary in `ray_tune_xgboost.py`.

### Integration with Existing Code

The `client_utils.py` file has been updated to automatically use tuned parameters when available. You can control this behavior by setting the `use_tuned_params` parameter when initializing the `XgbClient`.
</file>

<file path="ray_tune_xgboost_updated.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
Recent Fixes:
- Fixed Ray Tune worker file access issues by passing data directly to trial functions
  instead of trying to reload files from worker environments
- Ensured consistent preprocessing across hyperparameter tuning and federated learning phases
- Fixed processor path issues by using absolute paths for Ray Tune workers
- Added robust label column handling to work with different feature processor behaviors
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, log_loss
import logging
from ray.air.config import RunConfig
# Import existing data processing code
from dataset import load_csv_data, transform_dataset_to_dmatrix, create_global_feature_processor, load_global_feature_processor
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info(f"Starting trial with config: {config}")
    # Load the global feature processor instead of creating a new one
    processor_path = config.get('global_processor_path', 'outputs/global_feature_processor.pkl')
    try:
        processor = load_global_feature_processor(processor_path)
        logger.info("Using global feature processor for consistent preprocessing")
    except FileNotFoundError:
        logger.warning(f"Global processor not found at {processor_path}, creating new one")
        from dataset import FeatureProcessor
        processor = FeatureProcessor()
        processor.fit(train_df)
    # Transform data using the global processor
    train_processed = processor.transform(train_df, is_training=True)
    test_processed = processor.transform(test_df, is_training=False)
    # Debug: Check what columns are available after processing
    logger.info(f"Train processed columns: {list(train_processed.columns)}")
    logger.info(f"Test processed columns: {list(test_processed.columns)}")
    # Handle label column extraction more robustly
    if 'label' in train_processed.columns:
        train_features = train_processed.drop(columns=['label'])
        train_labels = train_processed['label'].astype(int)
    else:
        # If label column doesn't exist after processing, use original data
        logger.warning("Label column not found in processed data, using original labels")
        train_features = train_processed
        train_labels = train_df['label'].astype(int) if 'label' in train_df.columns else train_df['Label'].astype(int)
    if 'label' in test_processed.columns:
        test_features = test_processed.drop(columns=['label'])
        test_labels = test_processed['label'].astype(int)
    else:
        # If label column doesn't exist after processing, use original data
        logger.warning("Label column not found in processed test data, using original labels")
        test_features = test_processed
        test_labels = test_df['label'].astype(int) if 'label' in test_df.columns else test_df['Label'].astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred_proba = bst.predict(test_data)  # Get probabilities from multi:softprob
    y_pred_labels = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to predicted labels
    y_true = test_data.get_label()
    # Compute multi-class metrics using predicted labels
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file=None, test_file=None, data_file=None, num_samples=100, cpus_per_trial=1, gpu_fraction=None, output_dir="./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune with consistent preprocessing.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Step 1: Create global feature processor for consistent preprocessing
    logger.info("Creating global feature processor for consistent preprocessing...")
    if data_file:
        processor_path = create_global_feature_processor(data_file, output_dir)
    elif train_file:
        processor_path = create_global_feature_processor(train_file, output_dir)
    else:
        raise ValueError("Either data_file or train_file must be provided to create global processor")
    # Load and prepare data
    if train_file and test_file:
        logger.info(f"Loading training data from {train_file}")
        train_data = load_csv_data(train_file)["train"].to_pandas()
        logger.info(f"Loading testing data from {test_file}")
        test_data = load_csv_data(test_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        for df in [train_data]:
            if 'label' not in df.columns and 'Label' in df.columns:
                df['label'] = df['Label']
        # Check if test data has a label column
        if 'label' not in test_data.columns and 'Label' in test_data.columns:
            test_data['label'] = test_data['Label']
        # If test data doesn't have a label column, create a dummy one
        if 'label' not in test_data.columns:
            logger.warning("Test data doesn't have a label column. Creating a dummy label column with zeros.")
            test_data['label'] = 0
        # Load the global feature processor and process data
        processor = load_global_feature_processor(processor_path)
        # Process the data using the global processor
        train_processed = processor.transform(train_data, is_training=True)
        test_processed = processor.transform(test_data, is_training=False)
        # Extract features and labels
        if 'label' in train_processed.columns:
            train_features = train_processed.drop(columns=['label'])
            train_labels = train_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed training data, using original labels")
            train_features = train_processed
            train_labels = train_data['label'].astype(int) if 'label' in train_data.columns else train_data['Label'].astype(int)
        if 'label' in test_processed.columns:
            test_features = test_processed.drop(columns=['label'])
            test_labels = test_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed test data, using original labels")
            test_features = test_processed
            test_labels = test_data['label'].astype(int) if 'label' in test_data.columns else test_data['Label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    else:
        # Fall back to original behavior with single file and splitting
        logger.info(f"Loading data from {data_file}")
        data = load_csv_data(data_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        if 'label' not in data.columns and 'Label' in data.columns:
            data['label'] = data['Label']
        # Use temporal splitting to avoid data leakage
        from sklearn.model_selection import train_test_split as sklearn_split
        # Check if Stime column exists for temporal splitting
        if 'Stime' in data.columns:
            logger.info("Using temporal splitting based on Stime to avoid data leakage")
            data_sorted = data.sort_values('Stime').reset_index(drop=True)
            train_size = int(0.8 * len(data_sorted))
            train_split_orig = data_sorted.iloc[:train_size].copy()
            test_split_orig = data_sorted.iloc[train_size:].copy()
        else:
            logger.warning("No Stime column found, using random split")
            train_split_orig, test_split_orig = sklearn_split(data, test_size=0.2, random_state=42)
        # Load the global processor and transform data
        processor = load_global_feature_processor(processor_path)
        train_processed = processor.transform(train_split_orig, is_training=True)
        test_processed = processor.transform(test_split_orig, is_training=False)
        # Extract features and labels
        if 'label' in train_processed.columns:
            train_features = train_processed.drop(columns=['label'])
            train_labels = train_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed training data, using original labels from split")
            train_features = train_processed
            # Get labels from the original split data before processing
            train_labels = train_split_orig['label'].astype(int)
        if 'label' in test_processed.columns:
            test_features = test_processed.drop(columns=['label'])
            test_labels = test_processed['label'].astype(int)
        else:
            logger.warning("Label column not found in processed test data, using original labels from split")
            test_features = test_processed
            # Get labels from the original split data before processing
            test_labels = test_split_orig['label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # Modified range for FL compatibility
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the original data DataFrames and processor path
    def _train_with_data_wrapper(config):
        # Add processor path to config - ensure it's absolute for Ray Tune workers
        config['global_processor_path'] = os.path.abspath(processor_path)
        # Pass copies of the original dataframes to the training function
        if train_file and test_file:
            return train_xgboost(config, train_data.copy(), test_data.copy())
        else:
            # For single data file mode, pass the original loaded data directly
            # instead of trying to reload it in the worker
            return train_xgboost(config, data.copy(), data.copy())
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200,
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        _train_with_data_wrapper,
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=RunConfig(
            local_dir=output_dir,
            name="xgboost_tune"
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info(f"  mlogloss: {best_metrics['mlogloss']:.4f}")
    logger.info(f"  merror: {best_metrics['merror']:.4f}")
    logger.info(f"  precision: {best_metrics['precision']:.4f}")
    logger.info(f"  recall: {best_metrics['recall']:.4f}")
    logger.info(f"  f1: {best_metrics['f1']:.4f}")
    logger.info(f"  accuracy: {best_metrics['accuracy']:.4f}")
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    with open(best_params_file, 'w') as f:
        json.dump(best_config, f, indent=2)
    logger.info(f"Best parameters saved to {best_params_file}")
    # Train a final model with the best parameters
    train_final_model(best_config, train_features, train_labels, test_features, test_labels, output_dir)
    return best_config
def train_final_model(config, train_features, train_labels, test_features, test_labels, output_dir):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info(f"Final model saved to {model_path}")
    # Evaluate the model
    y_pred_proba = final_model.predict(test_data)  # Get probabilities from multi:softprob
    y_pred_labels = np.argmax(y_pred_proba, axis=1)  # Convert probabilities to predicted labels
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Log final performance
    logger.info("Final model performance:")
    logger.info(f"  Precision: {precision:.4f}")
    logger.info(f"  Recall: {recall:.4f}")
    logger.info(f"  F1 Score: {f1:.4f}")
    logger.info(f"  Accuracy: {accuracy:.4f}")
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Ray Tune for XGBoost hyperparameter optimization")
    parser.add_argument("--data-file", type=str, help="Path to single CSV data file (optional if train and test files are provided)")
    parser.add_argument("--train-file", type=str, help="Path to training CSV data file")
    parser.add_argument("--test-file", type=str, help="Path to testing CSV data file")
    parser.add_argument("--num-samples", type=int, default=10, help="Number of hyperparameter combinations to try")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="CPUs per trial")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="GPU fraction per trial (0.1 for 10%)")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Output directory for results")
    args = parser.parse_args()
    # Validate arguments
    if not args.data_file and not (args.train_file and args.test_file):
        parser.error("Either --data-file or both --train-file and --test-file must be provided")
    # Run the hyperparameter tuning
    tune_xgboost(
        train_file=args.train_file,
        test_file=args.test_file,
        data_file=args.data_file,
        num_samples=args.num_samples,
        cpus_per_trial=args.cpus_per_trial,
        gpu_fraction=args.gpu_fraction,
        output_dir=args.output_dir
    )
if __name__ == "__main__":
    main()
</file>

<file path="ray_tune_xgboost.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import logging
from ray import air
# Import existing data processing code
from dataset import load_csv_data, preprocess_data, FeatureProcessor
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info("Starting trial with config: %s", config)
    # Use preprocess_data to get features and encoded labels
    processor = FeatureProcessor()
    train_features, train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    test_features, test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle cases where test data might be unlabeled
    if test_labels is None:
        logger.warning("Test data has no labels. Creating dummy labels for DMatrix.")
        test_labels = np.zeros(len(test_features))
    else:
        test_labels = test_labels.astype(int) # Ensure labels are integers
    # Ensure train labels are integers
    train_labels = train_labels.astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    # FeatureProcessor already handles feature types and drops unnecessary columns
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred_proba = bst.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Compute multi-class metrics using predicted labels
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file: str, test_file: str, num_samples: int = 100, cpus_per_trial: int = 1, gpu_fraction: float = None, output_dir: str = "./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune.
    Args:
        train_file (str): Path to the training CSV data file.
        test_file (str): Path to the testing CSV data file.
        num_samples (int): Number of hyperparameter combinations to try.
        cpus_per_trial (int): Number of CPUs to allocate per trial.
        gpu_fraction (float): Fraction of GPU to use per trial (if None, no GPU is used).
        output_dir (str): Directory to save results.
    Returns:
        dict: Best hyperparameters found.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Load and prepare data *once* before tuning starts
    logger.info("Loading training data from %s", train_file)
    train_df = load_csv_data(train_file)["train"].to_pandas()
    logger.info("Loading testing data from %s", test_file)
    test_df = load_csv_data(test_file)["train"].to_pandas() # Use train split of test file for validation
    # Preprocess data *once* to get features/labels for the final model training
    # and to fit the processor
    logger.info("Preprocessing data for final model training and fitting processor...")
    processor = FeatureProcessor()
    # We need the original dataframes (train_df, test_df) to pass to the trial wrapper
    # But we also need the processed features/labels for the final model training step
    final_train_features, final_train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    final_test_features, final_test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle potential missing labels in the test set for final model evaluation
    if final_test_labels is None:
        logger.warning("Test data has no labels. Using zeros for final model evaluation.")
        final_test_labels = np.zeros(len(final_test_features))
    else:
        final_test_labels = final_test_labels.astype(int) # Ensure labels are integers
    final_train_labels = final_train_labels.astype(int)
    logger.info("Training data size for final model: %d", len(final_train_features))
    logger.info("Validation data size for final model: %d", len(final_test_features))
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # Modified range for FL compatibility
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        # NOTE: Ray Tune handles GPU allocation via resources_per_trial typically
        # Setting tree_method here might be sufficient, but review Ray Tune docs if issues persist
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the *original* data DataFrames
    # The train_xgboost function inside the trial will handle preprocessing
    def _train_with_data_wrapper(config):
        # Pass copies of the original dataframes to the training function
        return train_xgboost(config, train_df.copy(), test_df.copy()) 
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200, # Corresponds roughly to num_boost_round max
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Wrap trainable with resource specification
    trainable_with_resources = tune.with_resources(
        _train_with_data_wrapper, 
        resources={"cpu": cpus_per_trial, "gpu": gpu_fraction if gpu_fraction else 0}
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        trainable_with_resources, # Use wrapped trainable
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=air.RunConfig( # Use air.RunConfig
            local_dir=output_dir,
            name="xgboost_tune",
            # resources_per_trial is handled by tune.with_resources
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info("  mlogloss: %.4f", best_metrics['mlogloss'])
    logger.info("  merror: %.4f", best_metrics['merror'])
    logger.info("  precision: %.4f", best_metrics['precision'])
    logger.info("  recall: %.4f", best_metrics['recall'])
    logger.info("  f1: %.4f", best_metrics['f1'])
    logger.info("  accuracy: %.4f", best_metrics['accuracy'])
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    # Use utf-8 encoding when writing JSON
    with open(best_params_file, 'w', encoding='utf-8') as f:
        json.dump(best_config, f, indent=2)
    logger.info("Best parameters saved to %s", best_params_file)
    # Train a final model with the best parameters using the preprocessed data
    train_final_model(best_config, final_train_features, final_train_labels, final_test_features, final_test_labels, output_dir)
    return best_config
def train_final_model(config: dict, 
                      train_features: pd.DataFrame, train_labels: pd.Series, 
                      test_features: pd.DataFrame, test_labels: pd.Series, 
                      output_dir: str):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels (encoded)
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels (encoded)
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True # Show progress for final model
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info("Final model saved to %s", model_path)
    # Evaluate the model
    y_pred_proba = final_model.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Log final performance
    logger.info("Final model performance:")
    logger.info("  Precision: %.4f", precision)
    logger.info("  Recall: %.4f", recall)
    logger.info("  F1 Score: %.4f", f1)
    logger.info("  Accuracy: %.4f", accuracy)
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="XGBoost Hyperparameter Tuning with Ray Tune")
    parser.add_argument("--train-file", type=str, required=True, help="Path to the training data CSV file.")
    parser.add_argument("--test-file", type=str, required=True, help="Path to the testing data CSV file.")
    parser.add_argument("--num-samples", type=int, default=100, help="Number of hyperparameter samples to try.")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="Number of CPUs to allocate per trial.")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="Fraction of GPU resources per trial (e.g., 0.5). Default is None (CPU only).")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Directory to save Ray Tune results and best parameters.")
    args = parser.parse_args()
    logger.info("===== XGBoost Hyperparameter Tuning with Ray Tune ====")
    logger.info("Training file: %s", args.train_file)
    logger.info("Testing file: %s", args.test_file)
    logger.info("Number of hyperparameter samples: %d", args.num_samples)
    logger.info("CPUs per trial: %d", args.cpus_per_trial)
    logger.info("GPU: %s", f"{args.gpu_fraction * 100}% per trial" if args.gpu_fraction else "Not used")
    logger.info("Output directory: %s", args.output_dir)
    logger.info("=======================================================")
    try:
        # Run the tuning process
        best_params = tune_xgboost(
            train_file=args.train_file,
            test_file=args.test_file,
            num_samples=args.num_samples,
            cpus_per_trial=args.cpus_per_trial,
            gpu_fraction=args.gpu_fraction,
            output_dir=args.output_dir
        )
        # Train the final model with the best hyperparameters
        # Load data again for final training (could be optimized if memory is a concern)
        train_df_final = load_csv_data(args.train_file)["train"].to_pandas()
        test_df_final = load_csv_data(args.test_file)["train"].to_pandas()
        # Preprocess data using the same processor logic used during tuning
        processor_final = FeatureProcessor()
        final_train_features, final_train_labels = preprocess_data(train_df_final, processor=processor_final, is_training=True)
        final_test_features, final_test_labels = preprocess_data(test_df_final, processor=processor_final, is_training=False)
        # Ensure labels are integers
        final_train_labels = final_train_labels.astype(int)
        if final_test_labels is not None:
            final_test_labels = final_test_labels.astype(int)
        else:
            # Handle case where test set might still be unlabeled after tuning run
            final_test_labels = np.zeros(len(final_test_features)) # Dummy labels if needed
        train_final_model(
            config=best_params, 
            train_features=final_train_features, train_labels=final_train_labels, 
            test_features=final_test_features, test_labels=final_test_labels, 
            output_dir=args.output_dir
        )
        logger.info("===== Hyperparameter tuning finished successfully ====")
    except Exception as e:
        logger.error("Hyperparameter tuning failed: %s", str(e), exc_info=True)
        print("===== Hyperparameter tuning failed ====") # Also print to stderr for visibility in GH Actions
if __name__ == "__main__":
    main()
</file>

<file path="README.md">
# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## ðŸ› ï¸ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture â†’ 2. Clean Capture and Convert to Dataset â†’ 3. Train/Test â†’ 4.ï¸Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## ðŸ—ºï¸ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## ðŸŽ¯ What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **ðŸ“š Table of Contents**
- [âœ¨ Features](#-features)  
- [ðŸ“‚ Project Structure](#-project-structure)  
- [ðŸš€ Getting Started](#-getting-started)  
- [âš™ï¸ Configuration](#-configuration)
- [ðŸ“‚ Output Structure](#-output-structure)
- [ðŸ§ª Running Experiments](#-running-experiments)  
- [âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## âœ¨ Features  
âœ… **Privacy-Preserving Training** - Federated learning implementation with data isolation  
âœ… **Flexible Configuration** - Hydra-powered experiment management  
âœ… **Reproducible Experiments** - âš ï¸Automatic output organization   
âœ… **CI/CD Integration** - GitHub Actions workflow with CML reporting  
âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **ðŸ“‚ Project Structure**
```bash
â”œâ”€â”€ github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cml.yaml      # CI/CD workflow definition
â”œâ”€â”€ pyecache/             # Python cache directory
â”œâ”€â”€ data/                 # Dataset files, data capture script, and data cleaning script
â”œ   â””â”€â”€ received/         # Data from Zeek/Kafka stream
â”œâ”€â”€ outputs/              # Model, Predictions, Eval; Output files
â”œâ”€â”€ results/              # Latest aggregated metrics
â”œâ”€â”€ client.py             # Flower client logic
â”œâ”€â”€ client_utils.py       # Client helper functions
â”œâ”€â”€ dataset.py            # Data loading/preprocessing
â”œâ”€â”€ poetry.lock           # Poetry dependency lockfile - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ pyproject.toml        # Poetry project configuration - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - ðŸš§ under construction (implementation phase) ðŸš§
â”œâ”€â”€ run_bagging.sh        # Bagging experiment script - runs script.py + client.py
â”œâ”€â”€ run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
â”œâ”€â”€ server.py             # Flower server logic
â”œâ”€â”€ server_utils.py       # Server helper functions
â”œâ”€â”€ sim.py                # Start simulation - âš ï¸ deprecated soon âš ï¸
â”œâ”€â”€ utils.py              # Shared utilities
â””â”€â”€ README.md             # Project documentation
```

---

## **ðŸš€ Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **âš ï¸âš™ï¸ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **âš ðŸ“‚ Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
â””â”€â”€ YYYY-MM-DD/                  # Run date
    â””â”€â”€ HH-MM-SS/                # Run time
        â”œâ”€â”€ .hydra/              # âš ï¸Config snapshots
        â”‚   â”œâ”€â”€ config.yaml
        â”‚   â””â”€â”€ hydra.yaml
        â”œâ”€â”€ results.pkl          # Training history
        â”œâ”€â”€ predictions/         # Model predictions
            â”œâ”€â”€ predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **ðŸ§ª Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## ðŸ”„ **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## ðŸŽ’ **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## ðŸ“Š Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
<div align="center">

## ðŸ”¥ **R&D Led By** ðŸ”¥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  ðŸ§‘ðŸ’» Coded with 100% chaos-driven curiosity    !
!  â˜• Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
ðŸ” Cyber Alchemy Brewing For ðŸ›ï¸ Indiana University of Pennsylvania's ARMZTA Project

ðŸ”— https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
</file>

<file path="requirements.txt">
flwr[simulation]>=1.7.0, <2.0
flwr-datasets>=0.2.0, <1.0.0
xgboost>=2.0.0, <3.0.0
ray[tune]>=2.9.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.2.0
matplotlib
seaborn
</file>

<file path="run_bagging.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
# Ensure the global feature processor is created before starting server and clients
# This guarantees consistent preprocessing across all clients and the server
echo "Step 1: Ensuring global feature processor is created..."
python create_global_processor.py \
    --data-file "data/received/final_dataset.csv" \
    --output-dir "outputs" \
    --force
if [ $? -ne 0 ]; then
    echo "Error creating global feature processor. Exiting."
    exit 1
fi
echo "âœ“ Global feature processor ready at outputs/global_feature_processor.pkl"
echo ""
echo "Step 2: Starting federated learning server..."
python3 server.py --pool-size=5 --num-rounds=5 --num-clients-per-round=5 --centralised-eval &
SERVER_PID=$!
sleep 30  # Sleep for 30s to give the server enough time to start
echo "Step 3: Starting federated learning clients..."
# Start regular client (partition 0)
echo "Starting regular client (partition 0)"
python3 client.py --partition-id=0 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[0]=$!
# Start regular client (partition 1)
echo "Starting regular client (partition 1)"
python3 client.py --partition-id=1 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[1]=$!
# Start regular client (partition 2)
echo "Starting regular client (partition 2)"
python3 client.py --partition-id=2 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[2]=$!
# Start regular client (partition 3)
echo "Starting regular client (partition 3)"
python3 client.py --partition-id=3 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[3]=$!
# Start regular client (partition 4)
echo "Starting regular client (partition 4)"
python3 client.py --partition-id=4 --num-partitions=5 --partitioner-type=exponential &
CLIENT_PIDS[4]=$!
echo "All clients started. Waiting for federated learning to complete..."
# Enable CTRL+C to stop all background processes
trap "echo 'Stopping all processes...'; kill -- -$$" SIGINT SIGTERM
# Wait for server to complete (it will finish after all rounds)
wait $SERVER_PID
# Wait for all client processes to complete
for pid in "${CLIENT_PIDS[@]}"; do
    if kill -0 $pid 2>/dev/null; then
        wait $pid
    fi
done
echo ""
echo "âœ“ Federated learning completed successfully!"
echo "Results saved to: outputs/"
</file>

<file path="run_cyclic.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --train-method=cyclic --pool-size=20 --num-rounds=10 &
sleep 30  # Sleep for 15s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --train-method=cyclic --num-partitions=5 --partitioner-type=uniform &
    sleep 5
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_ray_tune.sh">
#!/bin/bash
# Script to run Ray Tune for XGBoost hyperparameter optimization
# with consistent preprocessing to fix disconnection between tuning and FL phases
# Default values
DATA_FILE="data/received/final_dataset.csv"
NUM_SAMPLES=50
CPUS_PER_TRIAL=2
OUTPUT_DIR="./tune_results"
TRAIN_FILE=""
TEST_FILE=""
# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --data-file)
            DATA_FILE="$2"
            shift 2
            ;;
        --train-file)
            TRAIN_FILE="$2"
            shift 2
            ;;
        --test-file)
            TEST_FILE="$2"
            shift 2
            ;;
        --num-samples)
            NUM_SAMPLES="$2"
            shift 2
            ;;
        --cpus-per-trial)
            CPUS_PER_TRIAL="$2"
            shift 2
            ;;
        --output-dir)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        *)
            echo "Unknown argument: $1"
            shift
            ;;
    esac
done
echo "Starting Ray Tune XGBoost hyperparameter optimization with consistent preprocessing..."
# Step 1: Create global feature processor for consistent preprocessing
echo "Step 1: Creating global feature processor..."
# Determine which data file to use for the global processor
PROCESSOR_DATA_FILE="$DATA_FILE"
if [[ -n "$TRAIN_FILE" && -f "$TRAIN_FILE" ]]; then
    PROCESSOR_DATA_FILE="$TRAIN_FILE"
fi
python create_global_processor.py \
    --data-file "$PROCESSOR_DATA_FILE" \
    --output-dir "outputs" \
    --force
if [ $? -ne 0 ]; then
    echo "Failed to create global feature processor. Exiting."
    exit 1
fi
echo "Global feature processor created successfully."
# Step 2: Run Ray Tune with the updated script
echo "Step 2: Running Ray Tune optimization..."
# Build the Python command based on available files
PYTHON_CMD="python ray_tune_xgboost_updated.py"
PYTHON_CMD="$PYTHON_CMD --num-samples $NUM_SAMPLES"
PYTHON_CMD="$PYTHON_CMD --cpus-per-trial $CPUS_PER_TRIAL"
PYTHON_CMD="$PYTHON_CMD --output-dir \"$OUTPUT_DIR\""
if [[ -n "$TRAIN_FILE" && -n "$TEST_FILE" && -f "$TRAIN_FILE" && -f "$TEST_FILE" ]]; then
    echo "Using separate train and test files"
    PYTHON_CMD="$PYTHON_CMD --train-file \"$TRAIN_FILE\" --test-file \"$TEST_FILE\""
else
    echo "Using single data file: $DATA_FILE"
    PYTHON_CMD="$PYTHON_CMD --data-file \"$DATA_FILE\""
fi
echo "Executing: $PYTHON_CMD"
eval $PYTHON_CMD
if [ $? -ne 0 ]; then
    echo "Ray Tune optimization failed. Exiting."
    exit 1
fi
echo "Ray Tune optimization completed successfully."
# Step 3: Generate updated parameters file
echo "Step 3: Generating tuned parameters file..."
python use_tuned_params.py
if [ $? -ne 0 ]; then
    echo "Failed to generate tuned parameters. Continuing anyway."
fi
echo "Ray Tune with consistent preprocessing completed!"
echo "Global feature processor is available at: outputs/global_feature_processor.pkl"
echo "Tuned parameters are available at: $OUTPUT_DIR/best_params.json"
</file>

<file path="run.py">
#!/usr/bin/env python3
"""
Main script to run the federated learning pipeline with consistent preprocessing.
This script ensures that:
1. A global feature processor is created for consistent preprocessing
2. Ray Tune hyperparameter optimization uses this global processor
3. Federated learning uses the same global processor
4. All phases maintain preprocessing consistency
"""
import subprocess
import sys
from flwr.common.logger import log
from logging import INFO
def run_command(command, description):
    """Run a command and handle errors."""
    log(INFO, "Running: %s", description)
    log(INFO, "Command: %s", " ".join(command))
    try:
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        log(INFO, "âœ“ %s completed successfully", description)
        if result.stdout:
            log(INFO, "Output: %s", result.stdout.strip())
        return True
    except subprocess.CalledProcessError as e:
        log(INFO, "âœ— %s failed with exit code %d", description, e.returncode)
        if e.stdout:
            log(INFO, "Stdout: %s", e.stdout.strip())
        if e.stderr:
            log(INFO, "Stderr: %s", e.stderr.strip())
        return False
def main():
    """Main execution pipeline."""
    log(INFO, "Starting Federated Learning Pipeline with Consistent Preprocessing")
    log(INFO, "=" * 80)
    # Step 1: Create global feature processor
    log(INFO, "Step 1: Creating global feature processor for consistent preprocessing")
    if not run_command([
        "python", "create_global_processor.py",
        "--data-file", "data/received/final_dataset.csv",
        "--output-dir", "outputs",
        "--force"
    ], "Global feature processor creation"):
        log(INFO, "Failed to create global feature processor. Exiting.")
        sys.exit(1)
    # Step 2: Run hyperparameter tuning with consistent preprocessing
    log(INFO, "Step 2: Running hyperparameter tuning with consistent preprocessing")
    if not run_command([
        "python", "ray_tune_xgboost_updated.py",
        "--data-file", "data/received/final_dataset.csv",
        "--num-samples", "30",
        "--cpus-per-trial", "2",
        "--output-dir", "./tune_results"
    ], "Ray Tune hyperparameter optimization"):
        log(INFO, "Hyperparameter tuning failed. Continuing with default parameters.")
    # Step 3: Generate tuned parameters file
    log(INFO, "Step 3: Generating tuned parameters file")
    if not run_command([
        "python", "use_tuned_params.py"
    ], "Tuned parameters generation"):
        log(INFO, "Failed to generate tuned parameters. Using default parameters.")
    # Step 4: Run federated learning simulation
    log(INFO, "Step 4: Starting federated learning simulation")
    if not run_command([
        "python", "sim.py",
        "--train-method", "bagging",
        "--pool-size", "5",
        "--num-rounds", "5",
        "--num-clients-per-round", "5",
        "--centralised-eval",
        "--csv-file", "data/received/final_dataset.csv"
    ], "Federated learning simulation"):
        log(INFO, "Federated learning simulation failed.")
        sys.exit(1)
    log(INFO, "=" * 80)
    log(INFO, "Federated Learning Pipeline completed successfully!")
    log(INFO, "Key improvements:")
    log(INFO, "âœ“ Consistent preprocessing across all phases")
    log(INFO, "âœ“ Temporal splitting to prevent data leakage")
    log(INFO, "âœ“ Global feature processor for uniform data representation")
    log(INFO, "âœ“ Tuned hyperparameters applied to federated learning")
if __name__ == "__main__":
    main()
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, log_loss, accuracy_score
from logging import INFO, WARNING
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
import numpy as np
# Assuming visualization_utils.py is in the same directory or accessible via PYTHONPATH
from visualization_utils import (
    plot_confusion_matrix,
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution
)
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    # aggregated_metrics["loss"] = loss  # REMOVED - Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["mlogloss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    if not (isinstance(loss, (int, float)) and isinstance(aggregated_metrics, dict)):
        log(INFO, "[ERROR] Output of evaluate_metrics_aggregation is not (loss, dict): %s, %s", type(loss), type(aggregated_metrics))
        raise TypeError("evaluate_metrics_aggregation must return (loss, dict)")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices or array of probabilities)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'Normal', 'Reconnaissance', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Check if predictions is a 2D array (multi-class probabilities)
    if isinstance(predictions, np.ndarray) and len(predictions.shape) > 1:
        log(INFO, "Detected multi-class probability predictions with shape: %s", predictions.shape)
        # Convert probabilities to class labels
        predicted_labels = np.argmax(predictions, axis=1)
    else:
        # Already a list of class indices
        predicted_labels = predictions
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predicted_labels,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Default mapping for UNSW_NB15 multi-class predictions
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic',
            10: 'Class_10'  # Added for the engineered dataset which has 11 classes
        }
        # Safely convert predictions to integers
        predictions_dict['prediction_type'] = [label_mapping.get(int(label), f'unknown_{label}') for label in predicted_labels]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
        # Generate and save visualizations if we have true labels to compare with
        try:
            class_names = list(label_mapping.values())
            num_classes = len(class_names)
            # Convert to numpy arrays if they're not already
            y_true = np.array(true_labels) if not isinstance(true_labels, np.ndarray) else true_labels
            y_pred = np.array(predicted_labels) if not isinstance(predicted_labels, np.ndarray) else predicted_labels
            # Create confusion matrix
            cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{round_num}.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{round_num}.png")
            plot_class_distribution(y_true, y_pred, class_names, dist_path)
            log(INFO, f"Visualizations saved for round {round_num}")
        except Exception as e:
            log(WARNING, f"Error generating visualizations: {e}")
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions shape: %s", raw_predictions.shape if hasattr(raw_predictions, 'shape') else 'scalar')
    # Log distribution of scores
    if hasattr(raw_predictions, 'shape'):
        log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
            np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # For multi-class with multi:softprob, the raw predictions will be probabilities for each class
    if hasattr(raw_predictions, 'shape') and len(raw_predictions.shape) > 1:
        log(INFO, "Processing multi-class probability predictions with shape: %s", raw_predictions.shape)
        predicted_labels = np.argmax(raw_predictions, axis=1)
        # Default mapping for the engineered dataset
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic',
            10: 'Class_10'  # Added for the engineered dataset which has 11 classes
        }
        # Save predictions to CSV with class names
        predictions_df = pd.DataFrame({
            'predicted_label': predicted_labels,
            'prediction_type': [label_mapping.get(int(p), f'unknown_{p}') for p in predicted_labels],
        })
        # Add probability columns for each class
        for i in range(raw_predictions.shape[1]):
            predictions_df[f'prob_class_{i}'] = raw_predictions[:, i]
    elif len(raw_predictions.shape) == 1:  # Binary case or multi:softmax
        # Check if this is binary classification or multi-class with direct labels
        if np.max(raw_predictions) <= 1.0 and np.min(raw_predictions) >= 0.0:
            # Binary case
            probabilities = raw_predictions  # Already probabilities
            predicted_labels = (probabilities >= 0.5).astype(int)
            # Save predictions to CSV
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
                'prediction_score': probabilities
            })
        else:
            # Likely multi:softmax with direct class labels
            predicted_labels = np.round(raw_predictions).astype(int)
            # Default mapping for the engineered dataset
            label_mapping = {
                0: 'Normal', 
                1: 'Reconnaissance', 
                2: 'Backdoor', 
                3: 'DoS', 
                4: 'Exploits', 
                5: 'Analysis', 
                6: 'Fuzzers', 
                7: 'Worms', 
                8: 'Shellcode', 
                9: 'Generic',
                10: 'Class_10'  # Added for the engineered dataset which has 11 classes
            }
            # Save predictions to CSV with class names
            predictions_df = pd.DataFrame({
                'predicted_label': predicted_labels,
                'prediction_type': [label_mapping.get(int(p), f'unknown_{p}') for p in predicted_labels],
            })
    else:
        # Fallback for unexpected prediction format
        log(WARNING, "Unexpected prediction format. Creating basic predictions DataFrame.")
        predictions_df = pd.DataFrame({
            'raw_prediction': raw_predictions
        })
    # Log predicted class distribution
    if 'predicted_label' in predictions_df.columns:
        unique, counts = np.unique(predictions_df['predicted_label'], return_counts=True)
        log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Generate visualizations if true labels are available
    try:
        true_labels = dmatrix.get_label()
        if true_labels is not None and 'predicted_label' in predictions_df.columns:
            output_dir = os.path.dirname(output_path)
            num_classes = max(11, np.max(true_labels) + 1)  # Ensure at least 11 classes for the engineered dataset
            class_names = [label_mapping.get(i, f'Class_{i}') for i in range(num_classes)]
            predicted_labels = predictions_df['predicted_label'].values
            # Create confusion matrix
            cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))
            cm_path = os.path.join(output_dir, "final_confusion_matrix.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, "final_class_distribution.png")
            plot_class_distribution(true_labels, predicted_labels, class_names, dist_path)
            # Plot ROC and Precision-Recall Curves (for multi-class)
            if len(raw_predictions.shape) > 1 and raw_predictions.shape[1] >= num_classes:
                roc_path = os.path.join(output_dir, "final_roc_curves.png")
                plot_roc_curves(true_labels, raw_predictions, class_names, roc_path)
                pr_path = os.path.join(output_dir, "final_precision_recall_curves.png")
                plot_precision_recall_curves(true_labels, raw_predictions, class_names, pr_path)
            log(INFO, "Visualizations saved with final model predictions")
    except Exception as e:
        log(WARNING, f"Error generating visualizations: {e}")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_model(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Get predictions
            y_pred_proba = bst.predict(test_data)
            # For multi:softprob, we get probabilities for each class
            # Convert to labels by taking argmax if predictions are probabilities
            if isinstance(y_pred_proba, np.ndarray) and len(y_pred_proba.shape) > 1:
                y_pred_labels = np.argmax(y_pred_proba, axis=1)
                log(INFO, "Converting probability predictions to labels (argmax), shape: %s", y_pred_proba.shape)
            else:
                y_pred_labels = y_pred_proba  # Already labels
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_proba, server_round, "results", y_true)
            # Compute metrics using the predictions
            predictions = y_pred_labels  # Use the converted labels for metrics
            pred_proba = y_pred_proba    # The original probabilities for plots that need them
            # Ensure pred_proba has the correct shape for multi-class
            if len(pred_proba.shape) == 1 or pred_proba.shape[1] == 1:
                 # If predict gives labels or single class proba, try predict_proba if available
                 try:
                     # Note: XGBoost predict() with multi:softmax directly gives labels.
                     # To get probabilities, the objective might need to be multi:softprob
                     log(WARNING, "Predict output seems 1D, attempting to handle for multi-class probability plots...")
                     if BST_PARAMS.get('objective') == 'multi:softmax':
                         # Create dummy probabilities centered around the predicted class
                         num_classes = BST_PARAMS.get('num_class', 11) # Default to 11 if not set
                         pred_proba = np.zeros((len(predictions), num_classes))
                         for i, label in enumerate(predictions):
                             if 0 <= int(label) < num_classes: # Check bounds
                                pred_proba[i, int(label)] = 0.9 # Assign high prob to predicted
                                other_prob = 0.1 / max(1, (num_classes - 1))
                                for j in range(num_classes):
                                    if j != int(label):
                                        pred_proba[i,j] = other_prob
                             else:
                                 log(WARNING, f"Prediction label {label} out of bounds [0, {num_classes-1}]")
                                 # Assign uniform probability as fallback if label is invalid
                                 pred_proba[i, :] = 1.0 / num_classes
                         log(WARNING, "Reconstructed dummy probabilities for multi:softmax. Plots may be inaccurate. Consider using 'multi:softprob' objective for better probability estimates.")
                     else: # Cannot determine probabilities
                         pred_proba = None
                 except AttributeError:
                     log(WARNING, "Could not get probabilities, ROC and PR curves will not be generated.")
                     pred_proba = None
                 except Exception as e:
                     log(WARNING, f"Error processing probabilities: {e}. ROC/PR plots skipped.")
                     pred_proba = None
            elif pred_proba.shape[1] != BST_PARAMS.get('num_class', 11):
                 log(WARNING, f"Probability shape mismatch ({pred_proba.shape[1]} columns vs {BST_PARAMS.get('num_class', 11)} classes). Plots may fail.")
                 # Attempt to proceed, but plots requiring probabilities might error out
            # Calculate metrics
            # Ensure y_test is integer type for log_loss if using one-hot encoding
            y_test_int = y_true.astype(int)
            num_classes_actual = BST_PARAMS.get('num_class', 11)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                 try:
                     # Manual clipping to replace deprecated eps parameter
                     epsilon = 1e-15
                     pred_proba_clipped = np.clip(pred_proba, epsilon, 1 - epsilon)
                     loss = log_loss(y_test_int, pred_proba_clipped, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"pred_proba shape: {pred_proba.shape}")
                     log(WARNING, f"pred_proba sample: {pred_proba[:5]}")
            else:
                 log(WARNING, "Calculating log_loss using one-hot encoding due to missing/invalid probabilities.")
                 try:
                     predictions_int = np.array(predictions).astype(int)
                     # Manual clipping to replace deprecated eps parameter
                     epsilon = 1e-15
                     one_hot_predictions = np.eye(num_classes_actual)[predictions_int]
                     one_hot_clipped = np.clip(one_hot_predictions, epsilon, 1 - epsilon)
                     loss = log_loss(y_test_int, one_hot_clipped, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during one-hot log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"predictions unique: {np.unique(predictions)}, shape: {predictions.shape}")
            accuracy = accuracy_score(y_true, predictions)
            precision = precision_score(y_true, predictions, average='weighted', zero_division=0)
            recall = recall_score(y_true, predictions, average='weighted', zero_division=0)
            f1 = f1_score(y_true, predictions, average='weighted', zero_division=0)
            cm = confusion_matrix(y_true, predictions, labels=range(num_classes_actual)) # Ensure labels match num_classes
            log(INFO, f"Centralized eval round {server_round} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            # --- Generate and Save Plots ---
            output_dir = config.get("output_dir", "results") # Get output dir from config or default
            # Instead of creating a separate plots directory, save directly in output_dir
            log(INFO, f"Saving evaluation plots to: {output_dir}")
            # Update class names to include the 11th class
            class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic', 'Class_10'] 
            # Plot Confusion Matrix
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{server_round}.png")
            plot_confusion_matrix(cm, class_names[:num_classes_actual], cm_path) # Use actual num_classes
            # Plot Class Distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{server_round}.png")
            plot_class_distribution(y_test_int, predictions.astype(int), class_names[:num_classes_actual], dist_path)
            # Plot ROC and Precision-Recall Curves (only if probabilities are available and valid)
            if pred_proba is not None and len(pred_proba.shape) > 1 and pred_proba.shape[1] == num_classes_actual:
                roc_path = os.path.join(output_dir, f"roc_curves_round_{server_round}.png")
                plot_roc_curves(y_test_int, pred_proba, class_names[:num_classes_actual], roc_path)
                pr_path = os.path.join(output_dir, f"precision_recall_curves_round_{server_round}.png")
                plot_precision_recall_curves(y_test_int, pred_proba, class_names[:num_classes_actual], pr_path)
            else:
                 log(WARNING, f"Skipping ROC and PR curve generation due to unavailable/invalid probabilities (shape: {pred_proba.shape if pred_proba is not None else 'None'}).")
            # --- End Plot Generation ---
            # Return metrics
            return loss, {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}
    return evaluate_model
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO, WARNING
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data, FeatureProcessor, create_global_feature_processor, load_global_feature_processor
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Create global feature processor before starting federated learning
log(INFO, "Creating global feature processor for consistent preprocessing across all clients...")
test_csv_path = "data/received/final_dataset.csv"
global_processor_path = create_global_feature_processor(test_csv_path, output_dir)
global_processor = load_global_feature_processor(global_processor_path)
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    # Use the engineered dataset for testing
    log(INFO, "Using original final dataset with temporal splitting for centralized evaluation: %s", test_csv_path)
    test_set = load_csv_data(test_csv_path)["test"]
    test_set.set_format("pandas")
    test_df = test_set.to_pandas()
    # Use the global processor for consistent evaluation
    log(INFO, "Using global feature processor for centralized evaluation")
    # Transform to DMatrix with the global processor
    test_dmatrix = transform_dataset_to_dmatrix(
        test_df, 
        processor=global_processor,
        is_training=False
    )
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
class CustomFedXgbBagging(FedXgbBagging):
    def aggregate_evaluate(self, server_round, results, failures):
        if self.evaluate_metrics_aggregation_fn is not None:
            eval_metrics = []
            for r in results:
                # Case 1: Object with num_examples and metrics
                if hasattr(r, "num_examples") and hasattr(r, "metrics"):
                    eval_metrics.append((r.num_examples, r.metrics))
                # Case 2: Tuple of (num_examples, metrics_dict)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and isinstance(r[0], (int, float))
                    and isinstance(r[1], dict)
                ):
                    eval_metrics.append(r)
                # Case 3: Tuple of (client_proxy, EvaluateRes)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and hasattr(r[1], "num_examples")
                    and hasattr(r[1], "metrics")
                ):
                    eval_metrics.append((r[1].num_examples, r[1].metrics))
                else:
                    raise TypeError(
                        f"aggregate_evaluate: Unexpected result format: {type(r)}, value: {r}"
                    )
            aggregated_result = self.evaluate_metrics_aggregation_fn(eval_metrics)
            if not (isinstance(aggregated_result, tuple) and len(aggregated_result) == 2):
                raise TypeError("aggregate_evaluate must return (loss, dict)")
            loss, metrics = aggregated_result
            if not isinstance(metrics, dict):
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        return super().aggregate_evaluate(server_round, results, failures)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = CustomFedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        log(INFO, "[DEBUG] aggregate_evaluate received aggregated_result type: %s, value: %s", type(aggregated_result), aggregated_result)
        # Expect (loss, metrics_dict)
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "[ERROR] Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        log(INFO, "[ERROR] Unexpected format from aggregate_evaluate: %s", type(aggregated_result))
        raise TypeError("aggregate_evaluate must return (loss, dict)")
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add distributed losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["losses_distributed"] = history.losses_distributed
else:
    results["losses_distributed"] = []
    log(INFO, "No distributed losses found in history")
# Add centralized losses if available
if hasattr(history, 'losses_centralized') and history.losses_centralized:
    results["losses_centralized"] = history.losses_centralized
else:
    results["losses_centralized"] = []
    log(INFO, "No centralized losses found in history")
# Add distributed metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics_distributed"] = history.metrics_distributed
else:
    results["metrics_distributed"] = {}
    log(INFO, "No distributed metrics found in history")
# Add centralized metrics if available
if hasattr(history, 'metrics_centralized') and history.metrics_centralized:
    results["metrics_centralized"] = history.metrics_centralized
else:
    results["metrics_centralized"] = {}
    log(INFO, "No centralized metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
log(INFO, "Generating additional visualizations...")
# Define CLASS_NAMES based on the engineered dataset labels found during preprocessing
# This is a placeholder - we'll use generic labels since engineered dataset has numeric classes
CLASS_NAMES = [str(i) for i in range(11)]  # Using generic labels 0-10 as a fallback
# Check if we can get real class names from the processor
if centralised_eval and hasattr(global_processor, 'unique_labels'):
    CLASS_NAMES = [str(label) for label in global_processor.unique_labels]
    log(INFO, "Using class names from engineered dataset: %s", CLASS_NAMES)
# Import visualization functions and other necessary modules
from visualization_utils import (
    plot_learning_curves,
    plot_confusion_matrix as vis_plot_confusion_matrix, # Alias to avoid conflict
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution,
    plot_per_class_metrics,
    plot_prediction_probability_distributions
)
from sklearn.metrics import confusion_matrix
import numpy as np
# 1. Plot Learning Curves (Loss and Metrics over rounds)
try:
    metrics_for_learning_curve = ['accuracy', 'precision', 'recall', 'f1', 'mlogloss'] # Common metrics
    results_pkl_path = os.path.join(output_dir, "results.pkl")
    if os.path.exists(results_pkl_path):
        plot_learning_curves(results_pkl_path, metrics_for_learning_curve, output_dir)
    else:
        log(WARNING, "results.pkl not found at %s, skipping learning curve plots.", results_pkl_path)
except Exception as e:
    log(WARNING, "Failed to generate learning curve plots: %s", e)
# 2. Generate other plots if centralised evaluation was performed and model is available
if centralised_eval and hasattr(strategy, 'global_model') and strategy.global_model is not None and 'test_dmatrix' in globals():
    log(INFO, "Performing final evaluation on centralised test set for detailed visualizations...")
    try:
        # Reconstruct the final model (Booster)
        final_bst = xgb.Booster(params=BST_PARAMS)
        if isinstance(strategy.global_model, (bytes, bytearray)):
            final_bst.load_model(bytearray(strategy.global_model))
        elif isinstance(strategy.global_model, xgb.Booster): # if it was already a booster (e.g. from a custom strategy)
            final_bst = strategy.global_model
        else:
            raise TypeError("Unsupported global_model type in strategy for visualization.")
        y_true = test_dmatrix.get_label()
        y_pred_proba = final_bst.predict(test_dmatrix)
        y_pred = np.argmax(y_pred_proba, axis=1)
        # Plot Confusion Matrix
        conf_matrix_data = confusion_matrix(y_true, y_pred)
        vis_plot_confusion_matrix(conf_matrix_data, CLASS_NAMES, os.path.join(output_dir, "final_confusion_matrix.png"))
        # Plot ROC Curves
        plot_roc_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_roc_curves.png"))
        # Plot Precision-Recall Curves
        plot_precision_recall_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_pr_curves.png"))
        # Plot Class Distribution (True vs Predicted on test set)
        plot_class_distribution(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_class_distribution.png"))
        # Plot Per-Class Metrics (Precision, Recall, F1)
        plot_per_class_metrics(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_per_class_metrics.png"))
        # Plot Prediction Probability Distributions
        # This function saves to output_dir/prediction_probability_distributions.png by default
        plot_prediction_probability_distributions(y_true, y_pred_proba, CLASS_NAMES, output_dir)
        log(INFO, "Successfully generated all detailed visualizations for the final model.")
    except Exception as e:
        log(WARNING, "Failed to generate final model visualizations: %s", e)
elif not centralised_eval:
    log(INFO, "Centralised evaluation was not enabled. Skipping final model detailed visualizations.")
elif not (hasattr(strategy, 'global_model') and strategy.global_model is not None):
    log(INFO, "No final global model available in strategy. Skipping final model detailed visualizations.")
elif 'test_dmatrix' not in globals():
    log(INFO, "Centralised test_dmatrix not available. Skipping final model detailed visualizations.")
log(INFO, "Server process finished.")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    BST_PARAMS,
)
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="tuned_params.py">
# This file is generated automatically by use_tuned_params.py
# It contains optimized XGBoost parameters found by Ray Tune
NUM_LOCAL_ROUND = 82
TUNED_PARAMS = {
    'objective': 'multi:softprob',
    'num_class': 11,
    'eta': 0.0017574056471531343,
    'max_depth': 5,
    'min_child_weight': 19,
    'gamma': 1.0,
    'subsample': 0.7626600174065583,
    'colsample_bytree': 0.5836140774820693,
    'colsample_bylevel': 0.6,
    'nthread': 16,
    'tree_method': 'hist',
    'eval_metric': ['mlogloss', 'merror'],
    'max_delta_step': 5,
    'reg_alpha': 4.000877452784171,
    'reg_lambda': 6.121688592715271,
    'base_score': 0.5,
    'scale_pos_weight': 1.0,
    'grow_policy': 'lossguide',
    'normalize_type': 'tree',
    'random_state': 42,
    'num_boost_round': 82,
}
</file>

<file path="update_ray_tune_xgboost.py">
#!/usr/bin/env python3
"""
Script to update ray_tune_xgboost.py for use with UNSW_NB15 dataset
"""
import re
import sys
def update_file(input_file, output_file):
    with open(input_file, 'r') as f:
        content = f.read()
    # Update num_class in train_xgboost function
    content = re.sub(
        r"'num_class': 3,  # benign \(0\), dns_tunneling \(1\), icmp_tunneling \(2\)",
        "'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)",
        content
    )
    # Update num_class in train_final_model function
    content = re.sub(
        r"'num_class': 3,",
        "'num_class': 11,  # UNSW_NB15 has 11 classes (0-10)",
        content
    )
    with open(output_file, 'w') as f:
        f.write(content)
    print(f"Successfully updated {input_file} and saved to {output_file}")
if __name__ == "__main__":
    input_file = "ray_tune_xgboost.py"
    output_file = "ray_tune_xgboost_updated.py"
    update_file(input_file, output_file)
</file>

<file path="use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the
federated learning process to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data>
    --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json
    --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from server_utils import load_saved_model
from dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get feature importance: %s", str(error))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get model parameters: %s", str(error))
def clean_data_for_xgboost(data_frame):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        data_frame (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = data_frame.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # Use max() to set minimum threshold
        threshold = max(threshold, 1e6)
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path):
    """
    Save detailed prediction information to CSV for multi-class classification.
    Args:
        predictions (np.ndarray): Raw predictions from the model
        output_path (str): Path to save the predictions
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are multi-dimensional (one-hot encoded)
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        # Store raw probabilities
        results_df['raw_probabilities'] = predictions.tolist()
        # Get predicted class (argmax)
        predicted_labels = np.argmax(predictions, axis=1)
        results_df['predicted_label'] = predicted_labels
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predicted_labels
        ]
        # Store confidence scores (probability of predicted class)
        results_df['prediction_score'] = predictions[
            np.arange(len(predicted_labels)),
            predicted_labels
        ]
    else:
        # For single class predictions
        results_df['predicted_label'] = predictions.astype(int)
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predictions
        ]
        # Default confidence score of 1.0 for direct class predictions
        results_df['prediction_score'] = 1.0
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    label_counts = results_df['predicted_label'].value_counts()
    log(INFO, "Prediction counts by class:")
    for label, count in label_counts.items():
        class_name = label_mapping.get(int(label), f'unknown_{label}')
        log(INFO, "  %s: %d", class_name, count)
    if 'prediction_score' in results_df.columns:
        log(INFO, "Confidence score statistics: min=%.6f, max=%.6f, mean=%.6f",
            results_df['prediction_score'].min(),
            results_df['prediction_score'].max(),
            results_df['prediction_score'].mean())
    return results_df
def evaluate_labeled_data(model, dataset, output_path):
    """Handle evaluation of labeled data."""
    # Convert to DMatrix
    dmatrix = transform_dataset_to_dmatrix(dataset)
    # Get true labels for evaluation
    y_true = dmatrix.get_label()
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save detailed predictions
    _ = save_detailed_predictions(raw_predictions, output_path)
    # Evaluate if data has labels
    if raw_predictions.ndim > 1:
        y_pred_labels = np.argmax(raw_predictions, axis=1)
    else:
        y_pred_labels = raw_predictions.astype(int)
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_labels)
    precision = precision_score(y_true, y_pred_labels, average='weighted')
    recall = recall_score(y_true, y_pred_labels, average='weighted')
    f1_score_val = f1_score(y_true, y_pred_labels, average='weighted')
    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_labels)
    # Generate classification report
    class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
    report = classification_report(y_true, y_pred_labels, target_names=class_names)
    # Log evaluation results
    log(INFO, "Evaluation Results:")
    log(INFO, "  Accuracy: %.4f", accuracy)
    log(INFO, "  Precision (weighted): %.4f", precision)
    log(INFO, "  Recall (weighted): %.4f", recall)
    log(INFO, "  F1 Score (weighted): %.4f", f1_score_val)
    log(INFO, "Confusion Matrix:\n%s", conf_matrix)
    log(INFO, "Classification Report:\n%s", report)
def predict_unlabeled_data(model, data_path, output_path):
    """Handle prediction of unlabeled data."""
    # Load unlabeled data
    data = pd.read_csv(data_path)
    # Clean data
    data = clean_data_for_xgboost(data)
    # Convert to DMatrix
    dmatrix = xgb.DMatrix(data)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save predictions
    save_detailed_predictions(raw_predictions, output_path)
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Process data based on whether it has labels
        if args.has_labels:
            try:
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                evaluate_labeled_data(model, dataset, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during evaluation: %s", str(error))
                raise
        else:
            try:
                predict_unlabeled_data(model, args.data_path, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during prediction: %s", str(error))
                raise
    except Exception as error:  # pylint: disable=broad-except
        log(INFO, "Error: %s", str(error))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="use_tuned_params.py">
"""
use_tuned_params.py
This script loads the optimized hyperparameters found by Ray Tune and integrates them
into the existing federated learning system. It replaces the default XGBoost parameters
in both client_utils.py and utils.py with the optimized ones.
Usage:
    python use_tuned_params.py --params-file ./tune_results/best_params.json
"""
import os
import json
import argparse
import logging
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def load_tuned_params(params_file):
    """
    Load the optimized hyperparameters from a JSON file.
    Args:
        params_file (str): Path to the JSON file containing the optimized parameters
    Returns:
        dict: Optimized hyperparameters
    """
    if not os.path.exists(params_file):
        if params_file == "./tune_results/best_params.json":
            logger.error(f"Default parameters file not found: {params_file}")
            logger.error("This usually means Ray Tune hasn't been run yet or completed successfully.")
            logger.error("Please run the Ray Tune optimization first or specify a different --params-file")
        raise FileNotFoundError(f"Parameters file not found: {params_file}")
    logger.info("Loading optimized parameters from %s", params_file)
    with open(params_file, 'r', encoding='utf-8') as f:
        params = json.load(f)
    return params
def create_xgboost_params(tuned_params):
    """
    Create XGBoost parameters dictionary from the tuned parameters.
    Args:
        tuned_params (dict): Optimized hyperparameters from Ray Tune
    Returns:
        dict: XGBoost parameters dictionary for use in the existing system
    """
    # Start with the base parameters
    xgb_params = BST_PARAMS.copy()
    # Update with tuned parameters - convert float values to ints for integer parameters
    xgb_params.update({
        'max_depth': int(tuned_params['max_depth']),  # HyperOpt returns float, convert to int
        'min_child_weight': int(tuned_params['min_child_weight']),  # HyperOpt returns float, convert to int
        'eta': tuned_params['eta'],  # Learning rate
        'subsample': tuned_params['subsample'],
        'colsample_bytree': tuned_params['colsample_bytree'],
        'reg_alpha': tuned_params['reg_alpha'],
        'reg_lambda': tuned_params['reg_lambda']
    })
    # Add num_boost_round if it exists in tuned_params
    if 'num_boost_round' in tuned_params:
        xgb_params['num_boost_round'] = int(tuned_params['num_boost_round'])  # Convert to int
    # Add GPU support if specified in tuned parameters
    if 'tree_method' in tuned_params:
        if isinstance(tuned_params['tree_method'], list) and len(tuned_params['tree_method']) > 0:
            # If it's from hp.choice, it will be a list
            xgb_params['tree_method'] = tuned_params['tree_method'][0]
        else:
            xgb_params['tree_method'] = tuned_params['tree_method']
    return xgb_params
def save_updated_params(params, output_file):
    """
    Save updated parameters to a Python file that can be imported by client_utils.py
    Args:
        params (dict): Updated XGBoost parameters
        output_file (str): Path to save the updated parameters
    """
    # Extract num_boost_round if present to use as NUM_LOCAL_ROUND
    num_local_round = None
    if 'num_boost_round' in params:
        num_local_round = int(params['num_boost_round'])
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# This file is generated automatically by use_tuned_params.py\n")
        f.write("# It contains optimized XGBoost parameters found by Ray Tune\n\n")
        # Add NUM_LOCAL_ROUND if it was extracted from num_boost_round
        if num_local_round is not None:
            f.write(f"NUM_LOCAL_ROUND = {num_local_round}\n\n")
        f.write("TUNED_PARAMS = {\n")
        for key, value in params.items():
            if isinstance(value, str):
                f.write(f"    '{key}': '{value}',\n")
            elif isinstance(value, list):
                f.write(f"    '{key}': {value},\n")
            else:
                f.write(f"    '{key}': {value},\n")
        f.write("}\n")
    logger.info("Updated XGBoost parameters saved to %s", output_file)
    if num_local_round is not None:
        logger.info("NUM_LOCAL_ROUND set to %d based on tuned num_boost_round", num_local_round)
def backup_original_params():
    """
    Backup the original parameters to a Python file for reference.
    Returns:
        str: Path to the backup file
    """
    backup_file = "original_bst_params.json"
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(BST_PARAMS, f, indent=2)
    logger.info("Original parameters backed up to %s", backup_file)
    return backup_file
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Use tuned hyperparameters with the existing XGBoost client")
    parser.add_argument(
        "--params-file", 
        type=str, 
        default="./tune_results/best_params.json",
        help="Path to the tuned parameters JSON file (default: ./tune_results/best_params.json)"
    )
    parser.add_argument("--output-file", type=str, default="tuned_params.py", help="Output Python file for updated parameters")
    args = parser.parse_args()
    # Log which parameters file is being used
    if args.params_file == "./tune_results/best_params.json":
        logger.info("Using default parameters file: %s", args.params_file)
    else:
        logger.info("Using specified parameters file: %s", args.params_file)
    # Backup original parameters
    backup_file = backup_original_params()
    # Load tuned parameters
    tuned_params = load_tuned_params(args.params_file)
    logger.info("Loaded the following optimized parameters:")
    for key, value in tuned_params.items():
        logger.info("  %s: %s", key, value)
    # Create updated XGBoost parameters
    updated_params = create_xgboost_params(tuned_params)
    # Save updated parameters
    save_updated_params(updated_params, args.output_file)
    # Success message
    logger.info("Optimized parameters saved to %s", args.output_file)
    logger.info("Original parameters backed up to %s", backup_file)
    logger.info("These parameters will be automatically used by the XGBoost client")
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 2
BST_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 11,  # Updated for engineered dataset which has 11 classes (0-10)
    "eta": 0.05,  # Reduced learning rate to prevent overfitting
    "max_depth": 6,  # Increased from 3 to allow more complex trees
    "min_child_weight": 10,  # Increased to prevent fitting to small samples
    "gamma": 1.0,  # Increased minimum loss reduction for split
    "subsample": 0.7,  # Sample fewer rows per iteration
    "colsample_bytree": 0.6,  # Sample fewer features per tree
    "colsample_bylevel": 0.6,  # Sample fewer features per level
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 5,
    "reg_alpha": 0.8,  # Decreased L1 regularization from 2.0
    "reg_lambda": 0.8,  # Decreased L2 regularization from 5.0
    "base_score": 0.5,  # Neutral starting point
    "scale_pos_weight": 1.0,  # Simplified for multi-class with >3 classes
    "grow_policy": "lossguide",  # Alternative tree growing policy
    "normalize_type": "tree",  # Helps with interpretability
    "random_state": 42  # Fixed seed for reproducibility
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

<file path="visualization_utils.py">
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import pickle
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
from itertools import cycle
from flwr.common.logger import log
from logging import INFO, WARNING
def plot_confusion_matrix(conf_matrix, class_names, output_path):
    """Generates and saves a heatmap visualization of the confusion matrix."""
    try:
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names, ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Confusion matrix plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate confusion matrix plot: %s", e)
def plot_roc_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves ROC curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        fpr = {}
        tpr = {}
        roc_auc = {}
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(fpr[i], tpr[i], color=color, lw=2,
                     label='ROC curve of class {0} (area = {1:0.2f})'
                     ''.format(class_names[i], roc_auc[i]))
        ax.plot([0, 1], [0, 1], 'k--', lw=2)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('Receiver Operating Characteristic (ROC) - One-vs-Rest')
        ax.legend(loc="lower right")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "ROC curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate ROC curve plot: %s", e)
def plot_precision_recall_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves Precision-Recall curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        precision = {}
        recall = {}
        average_precision = {}
        for i in range(n_classes):
            precision[i], recall[i], _ = precision_recall_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            average_precision[i] = auc(recall[i], precision[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(recall[i], precision[i], color=color, lw=2,
                     label='PR curve of class {0} (AP = {1:0.2f})'
                     ''.format(class_names[i], average_precision[i]))
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_ylim([0.0, 1.05])
        ax.set_xlim([0.0, 1.0])
        ax.set_title('Precision-Recall Curve - One-vs-Rest')
        ax.legend(loc="lower left")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Precision-Recall curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate Precision-Recall curve plot: %s", e)
def plot_class_distribution(y_true, y_pred, class_names, output_path):
    """Generates and saves bar plots comparing true and predicted class distributions."""
    try:
        n_classes = len(class_names)
        true_counts = np.bincount(y_true.astype(int), minlength=n_classes)
        pred_counts = np.bincount(y_pred.astype(int), minlength=n_classes)
        x = np.arange(n_classes)
        width = 0.35
        fig, ax = plt.subplots(figsize=(12, 6))
        rects1 = ax.bar(x - width/2, true_counts, width, label='True Labels')
        rects2 = ax.bar(x + width/2, pred_counts, width, label='Predicted Labels')
        ax.set_ylabel('Count')
        ax.set_title('True vs Predicted Class Distribution')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.bar_label(rects1, padding=3)
        ax.bar_label(rects2, padding=3)
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Class distribution plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate class distribution plot: %s", e)
def plot_per_class_metrics(y_true, y_pred, class_names, output_path):
    """Generates and saves a bar chart of per-class precision, recall, and F1-score."""
    try:
        from sklearn.metrics import classification_report
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
        metrics_to_plot = ['precision', 'recall', 'f1-score']
        class_metrics = {class_name: [] for class_name in class_names}
        for class_name in class_names:
            if class_name in report:
                for metric in metrics_to_plot:
                    class_metrics[class_name].append(report[class_name][metric])
            else: 
                for _ in metrics_to_plot:
                    class_metrics[class_name].append(0)
        x = np.arange(len(class_names)) 
        width = 0.2  
        multiplier = 0
        fig, ax = plt.subplots(figsize=(max(12, len(class_names) * 1.5), 6))
        for i, metric_name in enumerate(metrics_to_plot):
            metric_values = [class_metrics[cn][i] for cn in class_names]
            offset = width * multiplier
            rects = ax.bar(x + offset, metric_values, width, label=metric_name.capitalize())
            ax.bar_label(rects, padding=3, fmt='%.2f')
            multiplier += 1
        ax.set_ylabel('Score')
        ax.set_title('Per-Class Precision, Recall, and F1-Score')
        ax.set_xticks(x + width, class_names, rotation=45, ha="right")
        ax.legend(loc='lower right', ncols=len(metrics_to_plot))
        ax.set_ylim(0, 1.1) 
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Per-class metrics plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate per-class metrics plot: %s", e)
def _plot_single_metric_curve(ax, history_attr, metric_key, label_prefix, marker):
    """Helper function to plot a single metric curve on a given axis."""
    plot_successful = False
    if hasattr(history_attr, '__contains__') and metric_key in history_attr: # Flower 1.x format
        if isinstance(history_attr[metric_key], list) and history_attr[metric_key]:
            rounds, values = zip(*history_attr[metric_key])
            ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
            plot_successful = True
    # Flower 2.x format: history_attr is a list of (round, {dict_of_metrics})
    elif isinstance(history_attr, list) and history_attr:
        if all(isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict) for item in history_attr):
            rounds = [item[0] for item in history_attr if metric_key in item[1]]
            values = [item[1][metric_key] for item in history_attr if metric_key in item[1]]
            if rounds and values:
                ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
                plot_successful = True
    return plot_successful
def plot_learning_curves(results_pkl_path, metrics_to_plot, output_dir):
    """Generates and saves learning curve plots (loss and specified metrics vs. round)
       from a pickled Flower history object.
    Args:
        results_pkl_path (str): Path to the results.pkl file.
        metrics_to_plot (list): A list of metric keys (str) to plot from the history object.
        output_dir (str): Directory to save the plots.
    """
    try:
        if not os.path.exists(results_pkl_path):
            log(WARNING, "Results file not found: %s", results_pkl_path)
            return
        with open(results_pkl_path, 'rb') as f:
            history_data = pickle.load(f) # history_data is now a dict
        # Plot Loss
        fig_loss, ax_loss = plt.subplots(figsize=(12, 6))
        loss_plotted = False
        # Access as dictionary keys
        if "losses_distributed" in history_data and history_data["losses_distributed"]:
            rounds_dist, losses_dist = zip(*history_data["losses_distributed"])
            ax_loss.plot(rounds_dist, losses_dist, label='Distributed Loss', marker='o')
            loss_plotted = True
        if "losses_centralized" in history_data and history_data["losses_centralized"]:
            rounds_cent, losses_cent = zip(*history_data["losses_centralized"])
            ax_loss.plot(rounds_cent, losses_cent, label='Centralized Loss', marker='x')
            loss_plotted = True
        if loss_plotted:
            ax_loss.set_title('Loss Over Federated Learning Rounds')
            ax_loss.set_xlabel('Server Round')
            ax_loss.set_ylabel('Loss')
            ax_loss.legend()
            ax_loss.grid(True)
            fig_loss.tight_layout()
            loss_plot_path = os.path.join(output_dir, "learning_curve_loss.png")
            fig_loss.savefig(loss_plot_path)
            log(INFO, "Loss learning curve plot saved to %s", loss_plot_path)
        else:
            log(INFO, "No loss data found to plot.")
        plt.close(fig_loss)
        # Plot Specified Metrics
        if not metrics_to_plot:
            log(INFO, "No metrics specified for plotting learning curves.")
            return
        num_metrics = len(metrics_to_plot)
        fig_metrics, axes_metrics = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=True)
        if num_metrics == 1:
            axes_metrics = [axes_metrics] 
        any_metric_plotted = False
        for i, metric_key in enumerate(metrics_to_plot):
            ax = axes_metrics[i]
            dist_plotted = False
            cent_plotted = False
            # Access as dictionary keys
            if "metrics_distributed" in history_data and history_data["metrics_distributed"]:
                dist_plotted = _plot_single_metric_curve(ax, history_data["metrics_distributed"], metric_key, 'Distributed', 'o')
            if "metrics_centralized" in history_data and history_data["metrics_centralized"]:
                cent_plotted = _plot_single_metric_curve(ax, history_data["metrics_centralized"], metric_key, 'Centralized', 'x')
            if dist_plotted or cent_plotted:
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} Over Federated Learning Rounds')
                ax.set_ylabel(metric_key.capitalize())
                ax.legend()
                ax.grid(True)
                any_metric_plotted = True
            else:
                log(WARNING, "Could not plot metric '%s' from history. Data not found or in unexpected format.", metric_key)
                ax.text(0.5, 0.5, f"Data for '{metric_key}' not found \nor in unexpected format.", 
                        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} (Data Unavailable)')
        if any_metric_plotted:
            axes_metrics[-1].set_xlabel('Server Round') # Set x-label only for the bottom-most plot
            fig_metrics.tight_layout()
            metrics_plot_path = os.path.join(output_dir, "learning_curves_metrics.png")
            fig_metrics.savefig(metrics_plot_path)
            log(INFO, "Metrics learning curves plot saved to %s", metrics_plot_path)
        else:
            log(INFO, "No data found for any of the specified metrics to plot.")
        plt.close(fig_metrics)
    except FileNotFoundError:
        log(WARNING, "Results file not found: %s", results_pkl_path)
    except pickle.UnpicklingError:
        log(WARNING, "Error unpickling results file: %s", results_pkl_path)
    except Exception as e:
        log(WARNING, "Could not generate learning curve plots: %s", e)
def plot_prediction_probability_distributions(y_true, y_pred_proba, class_names, output_dir, bins=50):
    """Generates and saves histograms of prediction probabilities for each true class.
    For each class, this plot shows the distribution of the predicted probabilities 
    assigned to that class, for samples that actually belong to that class.
    High probabilities bunched towards 1.0 are desirable.
    Args:
        y_true (np.array): Array of true labels (integers).
        y_pred_proba (np.array): Array of predicted probabilities, shape (n_samples, n_classes).
        class_names (list): List of class names (strings).
        output_dir (str): Directory to save the plot.
        bins (int): Number of bins for the histograms.
    """
    try:
        n_classes = len(class_names)
        if y_pred_proba.shape[1] != n_classes:
            log(WARNING, "Number of classes in y_pred_proba does not match len(class_names).")
            return
        # Determine the number of rows and columns for subplots
        n_cols = 3 
        n_rows = (n_classes + n_cols - 1) // n_cols # Ceiling division
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=True, sharey=False)
        axes = axes.flatten() # Flatten to easily iterate regardless of shape
        for i in range(n_classes):
            ax = axes[i]
            # Get probabilities for the current class where the true label is this class
            true_class_indices = np.where(y_true == i)[0]
            if len(true_class_indices) == 0:
                ax.text(0.5, 0.5, "No true samples for this class", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{class_names[i]} (No true samples)')
                ax.set_xlabel('Predicted Probability')
                ax.set_ylabel('Frequency')
                continue
            class_probabilities = y_pred_proba[true_class_indices, i]
            ax.hist(class_probabilities, bins=bins, range=(0,1), edgecolor='black', alpha=0.7)
            ax.set_title(f'Class: {class_names[i]}')
            ax.set_xlabel('Predicted Probability for this Class')
            ax.set_ylabel('Frequency')
            ax.grid(True, axis='y', linestyle='--', alpha=0.7)
            mean_proba = np.mean(class_probabilities)
            ax.axvline(mean_proba, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_proba:.2f}')
            ax.legend(fontsize='small')
        # Hide any unused subplots
        for j in range(n_classes, n_rows * n_cols):
            fig.delaxes(axes[j])
        fig.suptitle('Distribution of Predicted Probabilities for True Classes', fontsize=16, y=1.02)
        fig.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for suptitle
        plot_path = os.path.join(output_dir, "prediction_probability_distributions.png")
        fig.savefig(plot_path)
        plt.close(fig)
        log(INFO, "Prediction probability distribution plot saved to %s", plot_path)
    except Exception as e:
        log(WARNING, "Could not generate prediction probability distribution plot: %s", e)
</file>

</files>
