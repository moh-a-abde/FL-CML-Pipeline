This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    cursor-tools.mdc
  changes_summary.md
.github/
  workflows/
    cml.yaml
diagrams/
  client_operations.mmd
  data_handling.mmd
  overall_workflow.mmd
  server_operations.mmd
local_utils/
  __init__.py
  smote_processor.py
.repomixignore
client_utils.py
client.py
commit.sh
dataset.py
go_to_work.sh
multi_class_implementation_plan.md
pretrained_model_usage.md
progress.md
pyproject.toml
README.md
requirements.txt
run_bagging copy.sh
run_bagging.sh
run_cyclic.sh
run_smote.sh
run.py
server_utils.py
server.py
sim.py
smote_client.py
use_saved_model.py
utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/cursor-tools.mdc">
---
description: Global Rule. This rule should ALWAYS be loaded.
globs: *,**/*
alwaysApply: true
---
cursor-tools is a CLI tool that allows you to interact with AI models and other tools.
cursor-tools is installed on this machine and it is available to you to execute. You're encouraged to use it.

<cursor-tools Integration>
# Instructions
Use the following commands to get AI assistance:

**Direct Model Queries:**
`cursor-tools ask "<your question>" --provider <provider> --model <model>` - Ask any model from any provider a direct question (e.g., `cursor-tools ask "What is the capital of France?" --provider openai --model o3-mini`). Note that this command is generally less useful than other commands like `repo` or `plan` because it does not include any context from your codebase or repository.

**Implementation Planning:**
`cursor-tools plan "<query>"` - Generate a focused implementation plan using AI (e.g., `cursor-tools plan "Add user authentication to the login page"`)
The plan command uses multiple AI models to:
1. Identify relevant files in your codebase (using Gemini by default)
2. Extract content from those files
3. Generate a detailed implementation plan (using OpenAI o3-mini by default)

**Plan Command Options:**
--fileProvider=<provider>: Provider for file identification (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--thinkingProvider=<provider>: Provider for plan generation (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--fileModel=<model>: Model to use for file identification
--thinkingModel=<model>: Model to use for plan generation
--debug: Show detailed error information

**Web Search:**
`cursor-tools web "<your question>"` - Get answers from the web using a provider that supports web search (e.g., Perplexity models and Gemini Models either directly or from OpenRouter or ModelBox) (e.g., `cursor-tools web "latest shadcn/ui installation instructions"`)
when using web for complex queries suggest writing the output to a file somewhere like local-research/<query summary>.md.

**Web Command Options:**
--provider=<provider>: AI provider to use (perplexity, gemini, modelbox, or openrouter)
--model=<model>: Model to use for web search (model name depends on provider)
--max-tokens=<number>: Maximum tokens for response

**Repository Context:**
`cursor-tools repo "<your question>" [--subdir=<path>]` - Get context-aware answers about this repository using Google Gemini (e.g., `cursor-tools repo "explain authentication flow"`). Use the optional `--subdir` parameter to analyze a specific subdirectory instead of the entire repository (e.g., `cursor-tools repo "explain the code structure" --subdir=src/components`)

**Documentation Generation:**
`cursor-tools doc [options]` - Generate comprehensive documentation for this repository (e.g., `cursor-tools doc --output docs.md`)
when using doc for remote repos suggest writing the output to a file somewhere like local-docs/<repo-name>.md.

**GitHub Information:**
`cursor-tools github pr [number]` - Get the last 10 PRs, or a specific PR by number (e.g., `cursor-tools github pr 123`)
`cursor-tools github issue [number]` - Get the last 10 issues, or a specific issue by number (e.g., `cursor-tools github issue 456`)

**ClickUp Information:**
`cursor-tools clickup task <task_id>` - Get detailed information about a ClickUp task including description, comments, status, assignees, and metadata (e.g., `cursor-tools clickup task "task_id"`)

**Model Context Protocol (MCP) Commands:**
Use the following commands to interact with MCP servers and their specialized tools:
`cursor-tools mcp search "<query>"` - Search the MCP Marketplace for available servers that match your needs (e.g., `cursor-tools mcp search "git repository management"`)
`cursor-tools mcp run "<query>"` - Execute MCP server tools using natural language queries (e.g., `cursor-tools mcp run "list files in the current directory" --provider=openrouter`). The query must include sufficient information for cursor-tools to determine which server to use, provide plenty of context.

The `search` command helps you discover servers in the MCP Marketplace based on their capabilities and your requirements. The `run` command automatically selects and executes appropriate tools from these servers based on your natural language queries. If you want to use a specific server include the server name in your query. E.g. `cursor-tools mcp run "using the mcp-server-sqlite list files in directory --provider=openrouter"`

**Notes on MCP Commands:**
- MCP commands require `ANTHROPIC_API_KEY` or `OPENROUTER_API_KEY` to be set in your environment
- By default the `mcp` command uses Anthropic, but takes a --provider argument that can be set to 'anthropic' or 'openrouter'
- Results are streamed in real-time for immediate feedback
- Tool calls are automatically cached to prevent redundant operations
- Often the MCP server will not be able to run because environment variables are not set. If this happens ask the user to add the missing environment variables to the cursor tools env file at ~/.cursor-tools/.env

**Stagehand Browser Automation:**
`cursor-tools browser open <url> [options]` - Open a URL and capture page content, console logs, and network activity (e.g., `cursor-tools browser open "https://example.com" --html`)
`cursor-tools browser act "<instruction>" --url=<url | 'current'> [options]` - Execute actions on a webpage using natural language instructions (e.g., `cursor-tools browser act "Click Login" --url=https://example.com`)
`cursor-tools browser observe "<instruction>" --url=<url> [options]` - Observe interactive elements on a webpage and suggest possible actions (e.g., `cursor-tools browser observe "interactive elements" --url=https://example.com`)
`cursor-tools browser extract "<instruction>" --url=<url> [options]` - Extract data from a webpage based on natural language instructions (e.g., `cursor-tools browser extract "product names" --url=https://example.com/products`)

**Notes on Browser Commands:**
- All browser commands are stateless unless --connect-to is used to connect to a long-lived interactive session. In disconnected mode each command starts with a fresh browser instance and closes it when done.
- When using `--connect-to`, special URL values are supported:
  - `current`: Use the existing page without reloading
  - `reload-current`: Use the existing page and refresh it (useful in development)
  - If working interactively with a user you should always use --url=current unless you specifically want to navigate to a different page. Setting the url to anything else will cause a page refresh loosing current state.
- Multi step workflows involving state or combining multiple actions are supported in the `act` command using the pipe (|) separator (e.g., `cursor-tools browser act "Click Login | Type 'user@example.com' into email | Click Submit" --url=https://example.com`)
- Video recording is available for all browser commands using the `--video=<directory>` option. This will save a video of the entire browser interaction at 1280x720 resolution. The video file will be saved in the specified directory with a timestamp.
- DO NOT ask browser act to "wait" for anything, the wait command is currently disabled in Stagehand.

**Tool Recommendations:**
- `cursor-tools web` is best for general web information not specific to the repository. Generally call this without additional arguments.
- `cursor-tools repo` is ideal for repository-specific questions, planning, code review and debugging. E.g. `cursor-tools repo "Review recent changes to command error handling looking for mistakes, omissions and improvements"`. Generally call this without additional arguments.
- `cursor-tools plan` is ideal for planning tasks. E.g. `cursor-tools plan "Adding authentication with social login using Google and Github"`. Generally call this without additional arguments.
- `cursor-tools doc` generates documentation for local or remote repositories.
- `cursor-tools browser` is useful for testing and debugging web apps and uses Stagehand
- `cursor-tools mcp` enables interaction with specialized tools through MCP servers (e.g., for Git operations, file system tasks, or custom tools)

**Running Commands:**
1. Use `cursor-tools <command>` to execute commands (make sure cursor-tools is installed globally using npm install -g cursor-tools so that it is in your PATH)

**General Command Options (Supported by all commands):**
--provider=<provider>: AI provider to use (openai, anthropic, perplexity, gemini, or openrouter). If provider is not specified, the default provider for that task will be used.
--model=<model name>: Specify an alternative AI model to use. If model is not specified, the provider's default model for that task will be used.
--max-tokens=<number>: Control response length
--save-to=<file path>: Save command output to a file (in *addition* to displaying it)
--help: View all available options (help is not fully implemented yet)

**Repository Command Options:**
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for repository analysis
--max-tokens=<number>: Maximum tokens for response

**Documentation Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Generate documentation for a remote GitHub repository
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for documentation generation
--max-tokens=<number>: Maximum tokens for response

**GitHub Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Access PRs/issues from a specific GitHub repository

**Browser Command Options (for 'open', 'act', 'observe', 'extract'):**
--console: Capture browser console logs (enabled by default, use --no-console to disable)
--html: Capture page HTML content (disabled by default)
--network: Capture network activity (enabled by default, use --no-network to disable)
--screenshot=<file path>: Save a screenshot of the page
--timeout=<milliseconds>: Set navigation timeout (default: 120000ms for Stagehand operations, 30000ms for navigation)
--viewport=<width>x<height>: Set viewport size (e.g., 1280x720). When using --connect-to, viewport is only changed if this option is explicitly provided
--headless: Run browser in headless mode (default: true)
--no-headless: Show browser UI (non-headless mode) for debugging
--connect-to=<port>: Connect to existing Chrome instance. Special values: 'current' (use existing page), 'reload-current' (refresh existing page)
--wait=<time:duration or selector:css-selector>: Wait after page load (e.g., 'time:5s', 'selector:#element-id')
--video=<directory>: Save a video recording (1280x720 resolution, timestamped subdirectory). Not available when using --connect-to
--url=<url>: Required for `act`, `observe`, and `extract` commands. Url to navigate to before the main command or one of the special values 'current' (to stay on the current page without navigating or reloading) or 'reload-current' (to reload the current page)
--evaluate=<string>: JavaScript code to execute in the browser before the main command

**Nicknames**
Users can ask for these tools using nicknames
Gemini is a nickname for cursor-tools repo
Perplexity is a nickname for cursor-tools web
Stagehand is a nickname for cursor-tools browser

**Xcode Commands:**
`cursor-tools xcode build [buildPath=<path>] [destination=<destination>]` - Build Xcode project and report errors.
**Build Command Options:**
--buildPath=<path>: (Optional) Specifies a custom directory for derived build data. Defaults to ./.build/DerivedData.
--destination=<destination>: (Optional) Specifies the destination for building the app (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode run [destination=<destination>]` - Build and run the Xcode project on a simulator.
**Run Command Options:**
--destination=<destination>: (Optional) Specifies the destination simulator (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode lint` - Run static analysis on the Xcode project to find and fix issues.

**Additional Notes:**
- For detailed information, see `node_modules/cursor-tools/README.md` (if installed locally).
- Configuration is in `cursor-tools.config.json` (or `~/.cursor-tools/config.json`).
- API keys are loaded from `.cursor-tools.env` (or `~/.cursor-tools/.env`).
- ClickUp commands require a `CLICKUP_API_TOKEN` to be set in your `.cursor-tools.env` file.
- The default Stagehand model is set in `cursor-tools.config.json`, but can be overridden with the `--model` option.
- Available models depend on your configured provider (OpenAI or Anthropic) in `cursor-tools.config.json`.
- repo has a limit of 2M tokens of context. The context can be reduced by filtering out files in a .repomixignore file.
- problems running browser commands may be because playwright is not installed. Recommend installing playwright globally.
- MCP commands require `ANTHROPIC_API_KEY` to be set in your environment.
- **Remember:** You're part of a team of superhuman expert AIs. Work together to solve complex problems.

**MCP Command Options:**
--provider=<provider>: AI provider to use (anthropic or openrouter)
--model=<model name>: Specify an alternative AI model to use with OpenRouter. Ignored if provider is Anthropic.

<!-- cursor-tools-version: 0.6.0-alpha.11 -->
</cursor-tools Integration>
</file>

<file path=".cursor/changes_summary.md">
# Summary of Changes (Session ending 2025-04-08)

This document summarizes the key code modifications made during the pair programming session focused on integrating a `FeatureProcessor` into the data pipeline.

## Key Goals Achieved:

1.  **Integrated `FeatureProcessor`:** Replaced manual data preprocessing and encoding with a centralized `FeatureProcessor` class for consistency and to prevent data leakage.
2.  **Refactored Data Splitting and Transformation:** Updated `train_test_split` and `transform_dataset_to_dmatrix` functions to utilize the `FeatureProcessor` and handle data format conversions correctly (pandas DataFrame <-> XGBoost DMatrix).
3.  **Corrected Data Handling Logic:** Addressed several bugs related to function signatures, imports, redundant operations, and incorrect feature type classification (`local_orig`, `local_resp`).
4.  **Improved Unlabeled Data Processing:** Ensured the *same* fitted `FeatureProcessor` instance (fitted on training data) is used to preprocess both validation and unlabeled data in the client script.

## Specific File Changes:

### `dataset.py`

*   **`FeatureProcessor` Class:**
    *   Defined feature groups (`categorical_features`, `numerical_features`, `object_columns`).
    *   Implemented `fit` method to learn encoding mappings and numerical statistics from training data.
    *   Implemented `transform` method to apply learned preprocessing steps consistently.
    *   Moved `local_orig` and `local_resp` from `numerical_features` to `categorical_features` to fix a `TypeError` during quantile calculation.
*   **`preprocess_data` Function:**
    *   Modified to accept and utilize a `FeatureProcessor` instance.
    *   Ensured labels are correctly handled (including validation) and features are returned separately.
*   **`transform_dataset_to_dmatrix` Function:**
    *   Updated signature to accept `processor` and `is_training` arguments.
    *   Removed old manual encoding logic.
    *   Calls `preprocess_data` using the provided processor.
    *   Removed a redundant `.to_pandas()` call that caused an `AttributeError`.
*   **`train_test_split` Function:**
    *   Updated signature for clarity and standard arguments (`random_state`).
    *   Replaced Hugging Face `Dataset.train_test_split` with `sklearn.model_selection.train_test_split` for DataFrame splitting.
    *   Added import for `sklearn.model_selection.train_test_split`.
    *   Initializes and fits the `FeatureProcessor` on the training split.
    *   Calls `transform_dataset_to_dmatrix` to get DMatrices.
    *   **Modified to return the fitted `FeatureProcessor` instance** along with the train/test DMatrices.
*   **Code Cleanup:**
    *   Removed several unused imports (`Dict`, `List`, `defaultdict`, `NDArrays`).
    *   Removed unnecessary `else` blocks after `return` statements.

### `client.py`

*   **Data Loading:** Updated to load specific train and unlabeled CSV files.
*   **`train_test_split` Call:**
    *   Updated the function call to match the new signature (using `random_state`, expecting DMatrix and processor return values).
    *   Calculated `num_train` and `num_val` from the returned DMatrix objects (`.num_row()`).
*   **Unlabeled Data Preprocessing:**
    *   **Removed the inefficient and error-prone logic** that attempted to reconstruct a DataFrame from `train_dmatrix` to re-fit a processor.
    *   **Now uses the `FeatureProcessor` instance returned directly by `train_test_split`** to preprocess the unlabeled data, ensuring consistency.
    *   Added error handling around unlabeled data preprocessing.
*   **Imports:**
    *   Added necessary imports (`pd`, `xgb`, `np`, `FeatureProcessor`, `preprocess_data`, `WARNING`, `ERROR`).
    *   Removed unused imports (`transform_dataset_to_dmatrix`, `resplit`).
*   **Logging:** Updated log messages for clarity and fixed formatting (using %-formatting).

### `client_utils.py`

*   (No functional changes made in this session, but reviewed during debugging). 

## Addressing 100% Accuracy Issue (2025-04-08 Update)

The following changes were made to fix the issue where all clients were getting 100% accuracy despite proper data partitioning:

### `dataset.py`

1. **Removed Object Columns to Prevent Data Leakage:**
   * Completely removed object columns (`uid`, `client_initial_dcid`, `server_scid`) from FeatureProcessor
   * Added explicit dropping of these columns in the transform method
   * Added logging to indicate when potential leakage columns are dropped

2. **Enhanced Data Preprocessing:**
   * Added automatic detection of highly predictive categorical features
   * Added warning logs when a feature value is >90% predictive of a specific label
   * Re-enabled outlier capping using 99th percentile values
   * Fixed NaN handling to properly apply noise to filled values

3. **Improved Train/Test Split:**
   * Added multiple random noise features with different distributions (Gaussian, uniform, exponential)
   * Implemented UID-based splitting for complete train/test separation when UIDs are available
   * Added checks for data leakage indicators (e.g., UIDs with single labels)
   * Added test-specific noise to make validation more challenging
   * Added comprehensive logging of data distributions

4. **Fixed Data Separation Issues:**
   * Changed the NaN handling approach to avoid TypeError with ndarray
   * Used two-step approach for NaN filling: first fill with median, then add noise

### `utils.py`

5. **Modified XGBoost Parameters:**
   * Reduced max_depth from 6 to 3 to prevent overfitting
   * Reduced learning rate from 0.1 to 0.05
   * Increased regularization parameters (alpha and lambda)
   * Added column subsampling at different levels
   * Used alternative tree growing policy (lossguide)
   * Added fixed random seed for reproducibility

### `client.py`

6. **Client-Specific Randomization:**
   * Added client-specific random seeds based on partition_id
   * Implemented proper partitioning to ensure clients get different data

These changes collectively address the 100% accuracy issue by preventing data leakage, adding appropriate noise, ensuring proper data separation, and making the model less prone to memorizing patterns in the data.
</file>

<file path=".github/workflows/cml.yaml">
name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      - uses: iterative/setup-cml@v1
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          pip debug --verbose
          pip install virtualenv
          virtualenv venv
          source venv/bin/activate
          pip install mamba
          mamba init
          source ~/.bashrc
          mamba create
          mamba activate
          pip install xgboost
          pip install scikit-learn
          pip install -U flwr["simulation"]
          pip install -U "ray[all]"
          pip install torch
          pip install torchvision
          pip install torchaudio
          pip install hydra-core
          pip install imbalanced-learn
          pip install -r requirements.txt
          #python sim.py
          ./run_bagging.sh
          #python use_saved_model.py --model_path outputs/2025-02-27/04-38-36/final_model.json --data_path data/received/data/received/network_traffic_20250226_200827.csv --output_path outputs/pretrained/predictions_pretrained_model.csv
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          git checkout multi-class-predictions
          # Add aggegrated results files
          git add results/
          # Add new files in outputs directory
          git add outputs/
          # Commit all changes
          git commit -m "workflow in actionðŸš€"
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true
</file>

<file path="diagrams/client_operations.mmd">
graph LR
    subgraph ClientOperations [Client Operations client.py_client_utils.py]
        A[Receive Parameters from Server] --> B[Local Training: fit]
        B --> C[Evaluation: evaluate]
        C --> D[Metrics Calculation]
        D --> E[Send Results to Server]
        B --> F[Update Model]
        F --> B
        C --> G[Prediction if unlabeled data]
        G --> H[Save Predictions]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ClientOperations component;
</file>

<file path="diagrams/data_handling.mmd">
graph LR
    subgraph DataHandling [Data Handling dataset.py]
        A[Load CSV: load_csv_data] --> B[Preprocessing: preprocess_data]
        B --> C[Separate Features/Labels: separate_xy]
        C --> D[DMatrix Conversion: transform_dataset_to_dmatrix]
        D --> E[Train/Test Split: train_test_split]
        B --> F[Handle inf/NaN]
        F --> C
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class DataHandling component;
</file>

<file path="diagrams/overall_workflow.mmd">
graph LR
    subgraph OverallWorkflow [Overall Workflow]
        A[Client] --> B[Server]
        B --> A
        A --> C[Data]
        C --> A
    end
    
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class OverallWorkflow component
</file>

<file path="diagrams/server_operations.mmd">
graph LR
    subgraph ServerOperations [Server Operations server.py]
        A[Strategy Selection: FedXgbBagging/FedXgbCyclic] --> B[Client Management]
        B --> C[Model Aggregation]
        C --> D[Send Parameters to Clients]
        B --> E[Receive Results from Clients]
        E --> F[Metrics Aggregation]
        C --> G[Evaluation if centralized]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ServerOperations component;
</file>

<file path="local_utils/__init__.py">
"""
local_utils package for extending the FL-CML-Pipeline functionality
without modifying the core codebase.
"""
from local_utils.smote_processor import apply_smote_to_benign, apply_smote_wrapper
__all__ = ['apply_smote_to_benign', 'apply_smote_wrapper']
</file>

<file path="local_utils/smote_processor.py">
"""
smote_processor.py
This module implements SMOTE (Synthetic Minority Over-sampling Technique) specifically for
the benign class in the network traffic classification task. It provides a wrapper that
can be used without modifying the existing codebase.
Key Components:
- SMOTE application to benign class
- Integration with XGBoost DMatrix
- Utility functions for train/test dataset modification
"""
import numpy as np
import pandas as pd
import xgboost as xgb
from logging import INFO
from flwr.common.logger import log
from imblearn.over_sampling import SMOTE
from typing import Tuple, Optional, Any
import warnings
# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)
def apply_smote_to_benign(
    train_dmatrix: xgb.DMatrix,
    valid_dmatrix: xgb.DMatrix,
    processor: Any,
    random_state: int = 42
) -> Tuple[xgb.DMatrix, xgb.DMatrix, Any]:
    """
    Apply SMOTE specifically to the benign class (label 0) in the training data.
    Args:
        train_dmatrix (xgb.DMatrix): Training data in DMatrix format
        valid_dmatrix (xgb.DMatrix): Validation data in DMatrix format
        processor (Any): Feature processor instance for consistent preprocessing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, Any]: 
            - Training DMatrix with SMOTE applied to benign class
            - Original validation DMatrix (unchanged)
            - Original processor (unchanged)
    """
    log(INFO, "Starting SMOTE processing for benign class (label 0)...")
    try:
        # Get feature names if they exist (important for preserving model compatibility)
        feature_names = None
        feature_types = None
        try:
            feature_names = train_dmatrix.feature_names
            feature_types = train_dmatrix.feature_types
        except AttributeError:
            log(INFO, "No feature names found in DMatrix")
        # Extract features and labels from training DMatrix
        X_train = train_dmatrix.get_data()
        try:
            # Convert to dense array if it's a sparse matrix
            X_train = X_train.toarray()
        except:
            # Already dense or other format, proceed
            pass
        y_train = train_dmatrix.get_label()
        # Check class distribution before SMOTE
        class_counts_before = np.bincount(y_train.astype(int))
        log(INFO, "Class distribution before SMOTE:")
        class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        for i, count in enumerate(class_counts_before):
            class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
            log(INFO, f"  Class {class_name}: {count}")
        # We want to oversample only the benign class (label 0)
        # First, we'll find the size of the largest class
        target_class_size = max(class_counts_before)
        # Only proceed with SMOTE if benign is not already the largest class
        if class_counts_before[0] < target_class_size:
            log(INFO, f"Applying SMOTE to increase benign class from {class_counts_before[0]} to {target_class_size}")
            # Prepare the sampling strategy dictionary
            # We only want to oversample class 0 (benign) to match the largest class
            sampling_strategy = {0: target_class_size}
            # Configure SMOTE for the benign class only
            smote = SMOTE(
                sampling_strategy=sampling_strategy,
                random_state=random_state,
                k_neighbors=min(5, class_counts_before[0] - 1),  # Ensure k is suitable for the class size
                n_jobs=-1  # Use all available cores
            )
            # Apply SMOTE
            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
            # Check class distribution after SMOTE
            class_counts_after = np.bincount(y_resampled.astype(int))
            log(INFO, "Class distribution after SMOTE:")
            for i, count in enumerate(class_counts_after):
                class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
                log(INFO, f"  Class {class_name}: {count}")
            # Create new DMatrix with the resampled data, preserving feature names
            kwargs = {'missing': np.nan}
            if feature_names is not None:
                kwargs['feature_names'] = feature_names
                log(INFO, f"Preserved {len(feature_names)} feature names from original DMatrix")
            if feature_types is not None:
                kwargs['feature_types'] = feature_types
            # Create a new DMatrix with preserved metadata
            train_dmatrix_resampled = xgb.DMatrix(
                data=X_resampled, 
                label=y_resampled,
                **kwargs
            )
            # Additional compatibility check (debug information)
            if feature_names is not None:
                log(INFO, "Original DMatrix feature count: %d, Resampled DMatrix feature count: %d", 
                    len(feature_names), train_dmatrix_resampled.num_col())
            return train_dmatrix_resampled, valid_dmatrix, processor
        else:
            log(INFO, "Benign class is already the largest class. No need to apply SMOTE.")
            return train_dmatrix, valid_dmatrix, processor
    except Exception as e:
        log(INFO, f"Error in SMOTE processing: {str(e)}")
        log(INFO, "Continuing with original training data")
        return train_dmatrix, valid_dmatrix, processor
def apply_smote_wrapper(
    train_test_split_result: Tuple[xgb.DMatrix, xgb.DMatrix, Any],
    random_state: int = 42
) -> Tuple[xgb.DMatrix, xgb.DMatrix, Any]:
    """
    Wrapper function that takes the result of train_test_split and applies SMOTE
    to the benign class.
    Args:
        train_test_split_result (Tuple): Result from the original train_test_split function
            (train_dmatrix, valid_dmatrix, processor)
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple: Same format as input but with SMOTE applied to train_dmatrix
    """
    train_dmatrix, valid_dmatrix, processor = train_test_split_result
    # Apply SMOTE
    return apply_smote_to_benign(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        processor=processor,
        random_state=random_state
    )
</file>

<file path=".repomixignore">
# Ignore data directory containing large files
data/

# Common large file types
*.csv
*.parquet
*.pkl
*.model

# Ignore outputs and results
outputs/
results/

# Cache directories
__pycache__/
.cache/
</file>

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
# Default XGBoost parameters for multi-class classification
BST_PARAMS = {
    'objective': 'multi:softmax',  # Multi-class classification
    'num_class': 3,  # Classes: benign (0), dns_tunneling (1), icmp_tunneling (2)
    'eval_metric': ['mlogloss', 'merror'],  # Multi-class metrics
    'learning_rate': 0.05,  # Reduced to match utils.py and prevent overfitting
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': [1.0, 2.0, 1.0]  # More moderate weight adjustment for dns_tunneling
}
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            params (dict): XGBoost parameters (defaults to BST_PARAMS if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.params = params if params is not None else BST_PARAMS.copy()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        y_train = self.train_dmatrix.get_label()
        class_counts = np.bincount(y_train.astype(int))
        # Log class distribution for all three classes
        class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        for i, count in enumerate(class_counts):
            class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch
            bst = xgb.train(
                self.params,
                self.train_dmatrix,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (self.train_dmatrix, "train")],
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        y_pred_proba = bst.predict(self.valid_dmatrix, output_margin=True)  # Get raw predictions for mlogloss
        y_pred_labels = bst.predict(self.valid_dmatrix)  # Get class predictions
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        for i, count in enumerate(true_counts):
            class_name = class_names[i] if i < len(class_names) else f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics
        precision = precision_score(y_true, y_pred_labels, average='weighted')
        recall = recall_score(y_true, y_pred_labels, average='weighted')
        f1 = f1_score(y_true, y_pred_labels, average='weighted')
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss manually
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_pred_proba_softmax = np.exp(y_pred_proba) / np.sum(np.exp(y_pred_proba), axis=1, keepdims=True)
        y_true_one_hot = np.zeros_like(y_pred_proba_softmax)
        y_true_one_hot[np.arange(len(y_true)), y_true.astype(int)] = 1
        mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_softmax + epsilon), axis=1))
        # Compute confusion matrix
        conf_matrix = confusion_matrix(y_true, y_pred_labels)
        # Generate detailed classification report
        class_report = classification_report(y_true, y_pred_labels, target_names=class_names)
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix:\n{conf_matrix}")
        log(INFO, f"Classification Report:\n{class_report}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from server_utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss),
            # Store confusion matrix elements as individual metrics
            "conf_00": int(conf_matrix[0][0]),  # benign correct
            "conf_01": int(conf_matrix[0][1]),  # benign misclassified as dns
            "conf_02": int(conf_matrix[0][2]),  # benign misclassified as icmp
            "conf_10": int(conf_matrix[1][0]),  # dns misclassified as benign
            "conf_11": int(conf_matrix[1][1]),  # dns correct
            "conf_12": int(conf_matrix[1][2]),  # dns misclassified as icmp
            "conf_20": int(conf_matrix[2][0]),  # icmp misclassified as benign
            "conf_21": int(conf_matrix[2][1]),  # icmp misclassified as dns
            "conf_22": int(conf_matrix[2][2]),  # icmp correct
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data
)
from utils import client_args_parser, BST_PARAMS, NUM_LOCAL_ROUND
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    #latest_csv_path = get_latest_csv(data_directory)
    #labeled_dataset = load_csv_data(latest_csv_path)
    #unlabeled_dataset = load_csv_data(latest_csv_path)
    # Load labeled data for training
    labeled_csv_path = "data/received/network_train_60.csv"
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction
    unlabeled_csv_path = "data/received/network_test_40_nolabel.csv"
    unlabeled_dataset = load_csv_data(unlabeled_csv_path)
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    # The ExponentialPartitioner doesn't have get_indices, but provides partition method
    # which returns the partition directly rather than just indices
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except AttributeError:
        # If that fails, try using the partition method (used in newer versions)
        try:
            # Newer versions use a different API
            train_partition = partitioner.partition(full_train_data)[args.partition_id]
        except (AttributeError, TypeError, IndexError):
            # As a fallback, if all partition methods fail, use a simple numerical partition
            # by getting evenly spaced indices based on partition ID
            total_samples = len(full_train_data)
            samples_per_partition = total_samples // args.num_partitions
            start_idx = args.partition_id * samples_per_partition
            end_idx = start_idx + samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
            partition_indices = list(range(start_idx, end_idx))
            train_partition = full_train_data.select(partition_indices)
            log(INFO, "Used fallback partitioning. Partition %d: samples %d to %d", 
                args.partition_id, start_idx, end_idx)
    log(INFO, "Partition size: %d samples (out of %d total)", 
        len(train_partition), len(full_train_data))
    # Handle data splitting based on evaluation strategy
    if args.centralised_eval:
        # Use centralized test set for evaluation
        train_data = train_partition
        valid_data = labeled_dataset["test"]
        valid_data.set_format("numpy")
        num_train = train_data.shape[0]
        num_val = valid_data.shape[0]
    else:
        # Perform local train/test split using the updated function
        # This now returns the fitted processor as well
        log(INFO, "Performing local train/test split...")
        # Generate a unique seed for each client based on partition_id to ensure
        # different clients get different train/test splits
        client_specific_seed = args.seed + (args.partition_id * 1000)
        log(INFO, "Using client-specific random seed for train/test split: %d", client_specific_seed)
        train_dmatrix, valid_dmatrix, processor = train_test_split( # Capture processor
            train_partition,
            test_fraction=args.test_fraction,
            random_state=client_specific_seed  # Use client-specific seed
        )
        # Get counts from the DMatrix objects
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
        log(INFO, "Local split: %d train samples, %d validation samples", num_train, num_val)
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="commit.sh">
#!/bin/bash
# Stage all changes
git add .
# Commit the changes with a commit message
git commit -m "commit.sh"
# Push the changes to the remote repository
git push
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self):
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        # Define feature groups
        self.categorical_features = [
            'id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history', 
            'validation_status', 'method', 'status_msg', 'is_orig',
            'local_orig', 'local_resp'
        ]
        self.numerical_features = [
            'id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
            'missed_bytes', 'orig_pkts',
            'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'ts_delta',
            'rtt', 'acks', 'percent_lost', 'request_body_len',
            'response_body_len', 'seen_bytes', 'missing_bytes', 'overflow_bytes'
        ]
        # Removed object_columns as they may cause data leakage
        log(INFO, "Note: Removed object_columns (uid, client_initial_dcid, server_scid) to prevent potential data leakage")
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df.columns:
                unique_values = df[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df[df[col] == val]
                        if 'label' in df.columns and len(subset) > 0:
                            most_common_label = subset['label'].value_counts().idxmax()
                            label_pct = subset['label'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics
        for col in self.numerical_features:
            if col in df.columns:
                self.numerical_stats[col] = {
                    'mean': df[col].mean(),
                    'std': df[col].std(),
                    'median': df[col].median(),
                    'q99': df[col].quantile(0.99)
                }
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            raise ValueError("FeatureProcessor must be fitted before transform")
        df = df.copy()
        # Drop object columns since they might be causing data leakage
        object_columns_to_remove = ['uid', 'client_initial_dcid', 'server_scid']
        columns_dropped = []
        for col in object_columns_to_remove:
            if col in df.columns:
                df.drop(columns=[col], inplace=True)
                columns_dropped.append(col)
        if columns_dropped and not is_training:
            log(INFO, "Dropped potential leakage columns: %s", columns_dropped)
        # Transform categorical features
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with added noise for validation data
        for col in self.numerical_features:
            if col in df.columns and col in self.numerical_stats:
                # Replace infinities
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                # Add noise to all numerical features for validation data
                # This ensures validation is a truly independent test
                if not is_training:
                    # Add noise proportional to the standard deviation of each feature
                    std = self.numerical_stats[col]['std']
                    # If std is 0 or NaN, use a small default value
                    noise_scale = max(std, 0.1) * 0.05  # 5% of standard deviation
                    noise = np.random.normal(0, noise_scale, size=df.shape[0])
                    df[col] = df[col] + noise
                # Cap outliers using 99th percentile
                q99 = self.numerical_stats[col]['q99']
                df.loc[df[col] > q99, col] = q99  # Re-enabled outlier capping
                # Fill NaN with median plus small noise
                median = self.numerical_stats[col]['median']
                # Find NaN positions
                nan_mask = df[col].isna()
                # First fill with median value
                df[col] = df[col].fillna(median)
                # Then add noise only to the previously NaN positions if not training
                if not is_training and nan_mask.any():
                    # Add a tiny bit of noise to medians for previously NaN values
                    noise_scale = median * 0.01 if median != 0 else 0.001
                    noise = np.random.normal(0, noise_scale, size=nan_mask.sum())
                    df.loc[nan_mask, col] += noise
        return df
def preprocess_data(data: pd.DataFrame, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification with three classes: benign (0), dns_tunneling (1), and icmp_tunneling (2).
    Args:
        data (pd.DataFrame): Input DataFrame
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    if processor is None:
        processor = FeatureProcessor()
    # Process features
    df = processor.transform(data, is_training)
    # Handle labels
    if 'label' in df.columns:
        features = df.drop(columns=['label'])
        labels = df['label'].astype(int)
        # Validate labels
        unique_labels = labels.unique()
        if not all(label in [0, 1, 2] for label in unique_labels):
            print(f"Warning: Unexpected label values found: {unique_labels}")
            labels = labels.map(lambda x: x if x in [0, 1, 2] else -1)
        return features, labels
    return df, None
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, create a proper 80/20 split to avoid data leakage
        # Use a specific random seed for reproducibility
        train_test_dict = dataset.train_test_split(test_size=0.2, seed=42)
        return DatasetDict({
            "train": train_test_dict["train"],
            "test": train_test_dict["test"]
        })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def preprocess_data_deprec2(data):
    """/
    Preprocess the data by encoding categorical features and separating features and labels.
    Args:
        data (pd.DataFrame): Input DataFrame
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Define categorical and numerical features
    categorical_features = ['id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history']
    numerical_features = ['id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
                         'local_orig', 'local_resp', 'missed_bytes', 'orig_pkts', 
                         'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Convert categorical features to category type
    for col in categorical_features:
        df[col] = df[col].astype('category')
        # Get numerical codes for categories
        df[col] = df[col].cat.codes
    # Ensure numerical features are float type
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Check if this is labeled or unlabeled data
    if 'label' in df.columns:
        # For labeled data
        features = df.drop(columns=['label'])
        labels = df['label'].astype(float)
        return features, labels
    # For unlabeled data
    return df, None
def preprocess_data_deprec(data):
    """
    Preprocess the static_data.csv dataset by:
      - Dropping the 'Timestamp' column.
      - Converting 'Dst Port' and 'Protocol' to categorical features.
      - Converting remaining features (except 'Label') to numerical (float).
      - Separating features and target (Label), and encoding the target if necessary.
    Args:
        filepath (str): Path to the static_data.csv file.
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Drop 'Timestamp' as it is not used for training directly
    if 'Timestamp' in df.columns:
        df.drop(columns=['Timestamp'], inplace=True)
    # Define which columns to treat as categorical based on domain knowledge
    categorical_features = []
    if 'Dst Port' in df.columns:
        categorical_features.append('Dst Port')
    if 'Protocol' in df.columns:
        categorical_features.append('Protocol')
    # Convert categorical features to type 'category' and then to numerical codes
    for col in categorical_features:
        df[col] = df[col].astype('category').cat.codes
    # The numerical features are all columns except the ones we have categorized or the target
    numerical_features = [col for col in df.columns if col not in categorical_features + ['Label']]
    # Convert these numerical features to float
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Replace inf with NaN and cap large values
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    for col in numerical_features:
        max_val = 1e15  # Example max value, adjust as needed
        df[col] = np.where(df[col] > max_val, np.nan, df[col])
    # Process the target variable if present
    if 'Label' in df.columns:
        # If the label column is non-numeric (object), encode it as categorical codes.
        if df['Label'].dtype == object:
            labels = df['Label'].astype('category').cat.codes
        else:
            labels = df['Label']
        features = df.drop(columns=['Label'])
        return features, labels
    # If no label column, return the processed DataFrame and None for labels
    return df, None
def separate_xy(data):
    """
    Separate features and labels from the dataset.
    Args:
        data: Input dataset
    Returns:
        tuple: (features, labels or None if unlabeled)
    """
    return preprocess_data(data.to_pandas())
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # The input 'data' should already be a pandas DataFrame in this context
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # Handle case where preprocess_data might return None for labels (e.g., unlabeled data)
    if y is None:
        log(INFO, "No labels found in data. This appears to be unlabeled data for prediction only.")
        return xgb.DMatrix(x, missing=np.nan)
    # For validation data, log label distribution to help identify issues
    if not is_training:
        # Count occurrences of each label
        label_counts = np.bincount(y.astype(int))
        label_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
        log(INFO, "Label distribution in validation data:")
        for i, count in enumerate(label_counts):
            class_name = label_names[i] if i < len(label_names) else f'unknown_{i}'
            log(INFO, f"  {class_name}: {count}")
    return xgb.DMatrix(x, label=y, missing=np.nan)
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into train and test sets, preprocess, and return DMatrices and the fitted processor.
    Args:
        data: Input dataset (Hugging Face Dataset or pandas DataFrame)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: 
            - Training DMatrix
            - Test DMatrix
            - Fitted FeatureProcessor instance
    """
    # Convert to pandas if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use sklearn's train_test_split with shuffle=True to ensure data is properly randomized
    log(INFO, "Original data shape before splitting: %s", data.shape)
    # Add multiple random noise features to make the problem harder
    np.random.seed(random_state)
    # Add 3 noise columns with different distributions and scales
    data['random_noise1'] = np.random.normal(0, 0.5, size=data.shape[0])  # Gaussian noise (higher variance)
    data['random_noise2'] = np.random.uniform(-1, 1, size=data.shape[0])  # Uniform noise
    data['random_noise3'] = np.random.exponential(0.5, size=data.shape[0])  # Exponential noise
    # Check if 'label' column exists in data
    if 'label' not in data.columns:
        log(INFO, "Warning: No 'label' column found in data. Available columns: %s", data.columns.tolist())
    else:
        # Report class distribution
        label_counts = data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in original data: %s", label_counts)
    # Check for data leakage indicators
    if 'uid' in data.columns:
        uid_label_counts = data.groupby('uid')['label'].value_counts()
        uid_with_multiple_labels = uid_label_counts.index.get_level_values(0).duplicated(keep=False)
        if not any(uid_with_multiple_labels):
            log(WARNING, "CRITICAL: Each UID has only one label, indicating potential perfect data leakage through UIDs")
    # Generate a completely different random_state for validation split
    validation_random_state = (random_state * 17 + 3) % 10000
    log(INFO, "Using different random states for train/validation split: %d/%d", 
        random_state, validation_random_state)
    # Split data ensuring complete partition separation
    if 'uid' in data.columns:
        # If we have UIDs, use them to ensure no data leakage across train/test
        log(INFO, "Using UID-based splitting to ensure no data leakage")
        unique_uids = data['uid'].unique()
        np.random.seed(validation_random_state)
        np.random.shuffle(unique_uids)
        test_size = int(len(unique_uids) * test_fraction)
        test_uids = unique_uids[:test_size]
        train_uids = unique_uids[test_size:]
        # Split based on UIDs
        train_data = data[data['uid'].isin(train_uids)].copy()
        test_data = data[data['uid'].isin(test_uids)].copy()
        log(INFO, "Split by UIDs: %d train UIDs, %d test UIDs", len(train_uids), len(test_uids))
    else:
        # If no UIDs, use standard stratified split
        train_data, test_data = train_test_split_pandas(
            data,
            test_size=test_fraction,
            random_state=validation_random_state,
            shuffle=True,  # Ensure data is shuffled for a proper split
            stratify=data['label'] if 'label' in data.columns else None  # Use stratified split if possible
        )
    # Log the shapes to verify they're different sets
    log(INFO, "Train data shape: %s, Test data shape: %s", train_data.shape, test_data.shape)
    # Verify label distributions to ensure proper stratification
    if 'label' in data.columns:
        train_label_counts = train_data['label'].value_counts().to_dict()
        test_label_counts = test_data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in train data: %s", train_label_counts)
        log(INFO, "Class distribution in test data: %s", test_label_counts)
    # Check for unique values in both sets to verify they're actually different
    if 'uid' in data.columns:
        train_uids = set(train_data['uid'].unique())
        test_uids = set(test_data['uid'].unique())
        common_uids = train_uids.intersection(test_uids)
        if common_uids:
            log(WARNING, "WARNING: Found %d UIDs in both train and test sets! This indicates data leakage.", 
                len(common_uids))
        else:
            log(INFO, "Good: Train and test sets have completely separate UIDs (no overlap).")
    # Add dataset-specific noise to test set to make it more challenging
    # This helps prevent the model from simply memorizing patterns
    for col in train_data.columns:
        if col.startswith('id.') or col in ['proto', 'conn_state', 'duration', 'bytes']:
            continue  # Skip these columns
        if pd.api.types.is_numeric_dtype(train_data[col]):
            # Add noise only to numerical columns in test set
            col_std = test_data[col].std()
            if col_std > 0 and not pd.isna(col_std):
                noise_scale = col_std * 0.1  # 10% of standard deviation
                test_data[col] = test_data[col] + np.random.normal(0, noise_scale, size=test_data.shape[0])
                log(INFO, "Added noise to test column: %s", col)
    # Initialize feature processor
    processor = FeatureProcessor()
    # Fit processor on training data and transform both sets
    # Note: transform calls fit implicitly if is_training=True and not fitted
    train_dmatrix = transform_dataset_to_dmatrix(train_data, processor=processor, is_training=True)
    test_dmatrix = transform_dataset_to_dmatrix(test_data, processor=processor, is_training=False)
    # Log number of examples for verification
    log(INFO, "Train DMatrix has %d rows, Test DMatrix has %d rows", 
        train_dmatrix.num_row(), test_dmatrix.num_row())
    return train_dmatrix, test_dmatrix, processor # Return the fitted processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
class ModelPredictor:
    """
    Handles model prediction and dataset labeling
    """
    def __init__(self, model_path: str):
        self.model = xgb.Booster()
        self.model.load_model(model_path)
    def predict_and_save(
        self,
        input_data: Union[str, pd.DataFrame],
        output_path: str,
        include_confidence: bool = True
    ):
        """
        Predict on new data and save labeled dataset
        """
        # Load/preprocess input data
        data = self._prepare_data(input_data)
        # Generate predictions
        predictions = self.model.predict(data)
        confidence = None
        if include_confidence:
            confidence = self.model.predict(data, output_margin=True)
        # Save labeled dataset
        self._save_output(data, predictions, confidence, output_path)
</file>

<file path="go_to_work.sh">
#!/bin/bash
# Run Python scripts in the background and store their process IDs (PIDs)
python3 data/receiving_data.py &
PID1=$!
python3 data/livepreprocessing_socket.py &
PID2=$!
# Wait for 1 minute
sleep 30
# Kill the Python scripts
kill $PID1 $PID2
# Run Git commands and GitHub workflow
git pull
./commit.sh
gh workflow run cml.yaml
# Wait for 5 minutess
sleep 300
git pull
</file>

<file path="multi_class_implementation_plan.md">
# Multi-Class Classification Implementation Plan

## Current State
- Binary classification (benign vs malicious) using XGBoost
- Features include both categorical and numerical network traffic data
- Using federated learning with Flower framework
- Need to expand to classify specific types of malicious traffic

## Implementation Steps

### 1. Data Preprocessing Modifications (`dataset.py`)
- [ ] Update `preprocess_data()` function:
  ```python
  def preprocess_data(data):
      # ... existing code ...
      if 'label' in df.columns:
          features = df.drop(columns=['label'])
          
          # New label mapping for three classes
          label_mapping = {
              'benign': 0, 
              'dns_tunneling': 1, 
              'icmp_tunneling': 2
          }
          labels_series = df['label'].map(label_mapping)
          
          # Handle unmapped labels
          if labels_series.isnull().any():
              unmapped_labels = df['label'][labels_series.isnull()].unique()
              print(f"Warning: Unmapped labels found: {unmapped_labels}")
              labels_series = labels_series.fillna(-1)
          
          labels = labels_series.astype(int)
          return features, labels
      else:
          return df, None
  ```

### 2. Model Configuration Updates (`client_utils.py`)
- [ ] Update XGBoost parameters in `utils.py`:
  ```python
  BST_PARAMS = {
      'objective': 'multi:softmax',
      'num_class': 3,
      'eval_metric': ['mlogloss', 'merror'],
      'learning_rate': 0.1,
      'max_depth': 6,
      'min_child_weight': 1,
      'subsample': 0.8,
      'colsample_bytree': 0.8,
      'scale_pos_weight': [1.0, 2.0, 2.0]  # Adjust based on class distribution
  }
  ```

- [ ] Modify `evaluate()` method in `XgbClient` class:
  ```python
  def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
      # ... existing model loading code ...
      
      # Generate predictions - No thresholding needed for multi-class
      y_pred_proba = bst.predict(self.valid_dmatrix)
      y_pred_labels = np.argmax(y_pred_proba, axis=1)
      
      # Get ground truth labels
      y_true = self.valid_dmatrix.get_label()
      
      # Compute multi-class metrics
      precision = precision_score(y_true, y_pred_labels, average='weighted')
      recall = recall_score(y_true, y_pred_labels, average='weighted')
      f1 = f1_score(y_true, y_pred_labels, average='weighted')
      accuracy = accuracy_score(y_true, y_pred_labels)
      
      # Multi-class loss calculation
      epsilon = 1e-10
      log_likelihood = -np.log(y_pred_proba[np.arange(len(y_true)), y_true.astype(int)] + epsilon)
      loss = np.mean(log_likelihood)
      
      # Multi-class confusion matrix
      conf_matrix = confusion_matrix(y_true, y_pred_labels)
      
      metrics = {
          "precision": float(precision),
          "recall": float(recall),
          "f1": float(f1),
          "accuracy": float(accuracy),
          "loss": float(loss),
          "confusion_matrix": conf_matrix.tolist(),
          "num_predictions": self.num_val
      }
      
      return metrics
  ```

### 3. Server-Side Updates (`server_utils.py`)
- [ ] Update metrics aggregation:
  ```python
  def evaluate_metrics_aggregation(eval_metrics):
      total_num = sum([num for num, _ in eval_metrics])
      
      # Aggregate evaluation metrics
      metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy', 'loss']
      aggregated_metrics = {}
      
      for metric in metrics_to_aggregate:
          if all(metric in metrics for _, metrics in eval_metrics):
              aggregated_metrics[metric] = sum([metrics[metric] * num for num, metrics in eval_metrics]) / total_num
          else:
              aggregated_metrics[metric] = 0.0
      
      # Aggregate confusion matrix
      aggregated_conf_matrix = None
      for num, metrics in eval_metrics:
          conf_matrix = np.array(metrics.get("confusion_matrix", [[0, 0, 0], [0, 0, 0], [0, 0, 0]]))
          if aggregated_conf_matrix is None:
              aggregated_conf_matrix = conf_matrix
          else:
              aggregated_conf_matrix += conf_matrix
      
      aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix.tolist()
      aggregated_metrics["prediction_mode"] = eval_metrics[0][1].get("prediction_mode", False)
      
      return aggregated_metrics["loss"], aggregated_metrics
  ```

- [ ] Update prediction saving:
  ```python
  def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None):
      predictions_dict = {
          'predicted_label': predictions,
          'prediction_type': [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling') 
              for p in predictions
          ]
      }
      # ... rest of the function
  ```

### 4. Prediction Updates (`use_saved_model.py`)
- [ ] Update prediction saving:
  ```python
  def save_detailed_predictions(predictions, output_path):
      results_df = pd.DataFrame()
      
      if predictions.ndim > 1 and predictions.shape[1] > 1:
          results_df['raw_probabilities'] = predictions.tolist()
          predicted_labels = np.argmax(predictions, axis=1)
          results_df['predicted_label'] = predicted_labels
          
          results_df['prediction_type'] = [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling')
              for p in predicted_labels
          ]
          
          results_df['prediction_score'] = predictions[
              np.arange(len(predicted_labels)), 
              predicted_labels
          ]
      
      results_df.to_csv(output_path, index=False)
      return results_df
  ```

- [ ] Update main evaluation:
  ```python
  def main():
      # ... existing code ...
      if args.has_labels:
          y_pred_labels = np.argmax(raw_predictions, axis=1)
          accuracy = accuracy_score(y_true, y_pred_labels)
          
          cm = confusion_matrix(y_true, y_pred_labels)
          report = classification_report(y_true, y_pred_labels)
          
          log(INFO, f"Accuracy: {accuracy:.4f}")
          log(INFO, f"Confusion Matrix:\n{cm}")
          log(INFO, f"Classification Report:\n{report}")
  ```

## Testing Strategy

### 1. Unit Tests
- [ ] Test label mapping in `dataset.py`
- [ ] Test multi-class metrics calculation
- [ ] Test prediction format and class probabilities
- [ ] Test confusion matrix calculation

### 2. Integration Tests
- [ ] Test full training pipeline with three classes
- [ ] Test federated learning convergence
- [ ] Test model saving and loading
- [ ] Test prediction pipeline

### 3. System Tests
- [ ] End-to-end training and evaluation
- [ ] Performance testing with large datasets
- [ ] Class imbalance handling
- [ ] Error handling and edge cases

## Success Criteria
1. Model successfully trains on three-class data
2. All evaluation metrics properly calculated and reported
3. Predictions include class probabilities
4. Federated learning pipeline works with multiple classes
5. High accuracy in distinguishing between benign and malicious traffic
6. Accurate classification of DNS vs ICMP tunneling attacks
7. Documentation is complete and accurate
8. Test cases pass successfully

## Potential Challenges and Mitigations
1. Class imbalance
   - Solution: Use class weights in model parameters
   - Monitor class distribution in training data
   - Consider data augmentation for underrepresented classes

2. Model complexity
   - Solution: Start with simpler model and gradually increase complexity
   - Monitor training metrics for overfitting
   - Use cross-validation for parameter tuning

3. Federated learning convergence
   - Solution: Adjust learning rate and number of rounds
   - Monitor loss curves across clients
   - Consider using FedAvg with momentum

4. Performance impact
   - Solution: Profile code for bottlenecks
   - Optimize prediction pipeline
   - Consider batch processing for large datasets
</file>

<file path="pretrained_model_usage.md">
# Using the Saved Federated Learning Model

This document explains how to use the saved XGBoost model that was trained using the federated learning process.

## Overview

The federated learning process now saves the final trained model after the training process completes. The model is saved in two formats:

1. JSON format (`final_model.json`) - Human-readable format
2. Binary format (`final_model.bin`) - More efficient for loading

These files are saved in the output directory created for each training run, which follows the pattern:
```
outputs/YYYY-MM-DD/HH-MM-SS/
```

## Making Predictions with the Saved Model

We provide a utility script `use_saved_model.py` that demonstrates how to load the saved model and use it for making predictions on new data.

### Prerequisites

Make sure you have all the required dependencies installed:
- XGBoost
- Pandas
- NumPy
- Scikit-learn (for evaluation metrics)
- Flower (for logging utilities)

### Basic Usage

```bash
python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data> --output_path <path_for_predictions>
```

#### Arguments:

- `--model_path`: Path to the saved model file (.json or .bin)
- `--data_path`: Path to the data file (.csv)
- `--output_path`: Path to save the predictions (default: predictions.csv)
- `--has_labels`: Flag to indicate if the data file contains labels (for evaluation)
- `--info_only`: Display model information without making predictions

### Examples

#### Viewing model information:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --info_only
```

This will display information about the model such as the number of trees, feature importance, and model parameters.

#### Making predictions on unlabeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/unlabeled_data.csv --output_path predictions.csv
```

#### Evaluating the model on labeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/test_data.csv --output_path predictions.csv --has_labels
```

When using the `--has_labels` flag, the script will also calculate and display evaluation metrics such as accuracy, precision, recall, and F1 score.

## Troubleshooting

### Model File Not Found
Make sure the path to the model file is correct. The model files are saved in the output directory created for each training run.

### Model Loading Issues
If you encounter issues loading the model, the system will try multiple approaches:
1. Direct loading using `load_model`
2. Reading the file as bytes and loading into a Booster
3. Creating a new Booster with parameters and then loading the model

### Data Format Issues
The data should be in a format that can be converted to an XGBoost DMatrix. If you're having issues, check that your data has the same features that were used during training.

If you use the `--has_labels` flag but the system can't process the data as labeled, it will automatically fall back to processing it as unlabeled data.

### Memory Issues
If you're working with large datasets, you might encounter memory issues. Consider processing the data in batches or using a machine with more memory.

## Additional Resources

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Flower Documentation](https://flower.dev/docs/)
</file>

<file path="progress.md">
# FL-CML-Pipeline: Progress Summary

## Project Overview

This project implements a privacy-preserving machine learning solution using federated learning with the Flower framework. The system allows multiple clients to collaboratively train XGBoost models for network intrusion detection without sharing raw data, preserving privacy while achieving high model performance.

## Key Components

### 1. Federated Learning Architecture

- **Server Implementation (`server.py`)**
  - Controls the federated learning process
  - Implements both bagging and cyclic training strategies
  - Handles model aggregation and evaluation
  - Manages client selection and coordination

- **Client Implementation (`client.py`)**
  - Loads and processes local data
  - Trains local models based on server instructions
  - Participates in the federated learning process
  - Reports results back to the server

### 2. Data Pipeline

- **Data Processing (`dataset.py`)**
  - Loads CSV data from network traffic captures
  - Implements preprocessing for network traffic features
  - Provides multiple partitioning strategies (IID, Linear, Square, Exponential)
  - Handles data format conversions for XGBoost compatibility

- **Real-time Data Capture**
  - Support for live network traffic capture
  - Processing and conversion to training datasets

### 3. Utility Functions

- **Client Utilities (`client_utils.py`)**
  - XGBoost client implementation
  - Client-side helper functions

- **Server Utilities (`server_utils.py`)**
  - Server-side helper functions
  - Client management systems
  - Results handling and storage

### 4. Experiment Framework

- **Training Methods**
  - Bagging approach (`run_bagging.sh`): Aggregates models from multiple clients
  - Cyclic approach (`run_cyclic.sh`): Passes model sequentially through clients

- **Evaluation**
  - Supports both centralized and federated evaluation
  - Tracks multiple metrics (precision, recall, F1 score)

## Project Features

- âœ… **Privacy-Preserving Training** - True federated learning with data isolation
- âœ… **Flexible Configuration** - Support for various training strategies
- âœ… **Reproducible Experiments** - Automatic output organization
- âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline
- âœ… **Multiple Partitioning Strategies** - IID, Linear, Square, Exponential

## Recent Developments

### Implemented Core Functionality
- Established federated learning architecture with Flower framework
- Created data processing pipeline for network traffic data
- Implemented both bagging and cyclic training approaches
- Set up experiment scripts for reproducible testing

### Technical Improvements
- Enhanced XGBoost integration with Flower framework
- Improved data partitioning strategies
- Optimized client-server communication
- Implemented comprehensive metrics tracking

### Documentation
- Created detailed README with project structure and instructions
- Documented key components and their relationships
- Added configuration guidelines and examples

### Infrastructure
- Set up project directory structure
- Created Bash scripts for easy experiment execution
- Implemented output storage and organization

## Next Steps

### Planned Enhancements
- Improve scalability for larger numbers of clients
- Enhance privacy guarantees with differential privacy techniques
- Optimize hyperparameters for better model performance
- Add support for more model architectures

### Ongoing Research
- Comparing performance of bagging vs. cyclic approaches
- Analyzing impact of different data partitioning strategies
- Evaluating model convergence across federated strategies

## Conclusion

The FL-CML-Pipeline project has established a solid foundation for privacy-preserving machine learning using federated learning. The implemented system successfully demonstrates collaborative model training across multiple clients without sharing raw data, achieving the core goal of privacy-preserving machine learning for network intrusion detection.

The project continues to evolve with ongoing research into optimal federated learning strategies and implementation improvements to enhance performance and usability.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.4.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "xgboost-comprehensive"
version = "0.1.0"
description = "Federated XGBoost with Flower (comprehensive)"
authors = ["The Flower Authors <hello@flower.ai>"]

[tool.poetry.dependencies]
python = ">=3.8,<3.11"
flwr = { extras = ["simulation"], version = ">=1.7.0,<2.0" }
flwr-datasets = ">=0.2.0,<1.0.0"
xgboost = ">=2.0.0,<3.0.0"
</file>

<file path="README.md">
# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## ðŸ› ï¸ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture â†’ 2. Clean Capture and Convert to Dataset â†’ 3. Train/Test â†’ 4.ï¸Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## ðŸ—ºï¸ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## ðŸŽ¯ What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **ðŸ“š Table of Contents**
- [âœ¨ Features](#-features)  
- [ðŸ“‚ Project Structure](#-project-structure)  
- [ðŸš€ Getting Started](#-getting-started)  
- [âš™ï¸ Configuration](#-configuration)
- [ðŸ“‚ Output Structure](#-output-structure)
- [ðŸ§ª Running Experiments](#-running-experiments)  
- [âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## âœ¨ Features  
âœ… **Privacy-Preserving Training** - Federated learning implementation with data isolation  
âœ… **Flexible Configuration** - Hydra-powered experiment management  
âœ… **Reproducible Experiments** - âš ï¸Automatic output organization   
âœ… **CI/CD Integration** - GitHub Actions workflow with CML reporting  
âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **ðŸ“‚ Project Structure**
```bash
â”œâ”€â”€ github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cml.yaml      # CI/CD workflow definition
â”œâ”€â”€ pyecache/             # Python cache directory
â”œâ”€â”€ data/                 # Dataset files, data capture script, and data cleaning script
â”œ   â””â”€â”€ received/         # Data from Zeek/Kafka stream
â”œâ”€â”€ outputs/              # Model, Predictions, Eval; Output files
â”œâ”€â”€ results/              # Latest aggregated metrics
â”œâ”€â”€ client.py             # Flower client logic
â”œâ”€â”€ client_utils.py       # Client helper functions
â”œâ”€â”€ dataset.py            # Data loading/preprocessing
â”œâ”€â”€ poetry.lock           # Poetry dependency lockfile - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ pyproject.toml        # Poetry project configuration - ðŸ” exploring (research phase) ðŸ”
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - ðŸš§ under construction (implementation phase) ðŸš§
â”œâ”€â”€ run_bagging.sh        # Bagging experiment script - runs script.py + client.py
â”œâ”€â”€ run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
â”œâ”€â”€ server.py             # Flower server logic
â”œâ”€â”€ server_utils.py       # Server helper functions
â”œâ”€â”€ sim.py                # Start simulation - âš ï¸ deprecated soon âš ï¸
â”œâ”€â”€ utils.py              # Shared utilities
â””â”€â”€ README.md             # Project documentation
```

---

## **ðŸš€ Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **âš ï¸âš™ï¸ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **âš ðŸ“‚ Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
â””â”€â”€ YYYY-MM-DD/                  # Run date
    â””â”€â”€ HH-MM-SS/                # Run time
        â”œâ”€â”€ .hydra/              # âš ï¸Config snapshots
        â”‚   â”œâ”€â”€ config.yaml
        â”‚   â””â”€â”€ hydra.yaml
        â”œâ”€â”€ results.pkl          # Training history
        â”œâ”€â”€ predictions/         # Model predictions
            â”œâ”€â”€ predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **ðŸ§ª Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## ðŸ”„ **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## ðŸŽ’ **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## ðŸ“Š Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
<div align="center">

## ðŸ”¥ **R&D Led By** ðŸ”¥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  ðŸ§‘ðŸ’» Coded with 100% chaos-driven curiosity    !
!  â˜• Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
ðŸ” Cyber Alchemy Brewing For ðŸ›ï¸ Indiana University of Pennsylvania's ARMZTA Project

ðŸ”— https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
</file>

<file path="requirements.txt">
flwr[simulation]>=1.7.0, <2.0
flwr-datasets>=0.2.0, <1.0.0
xgboost>=2.0.0, <3.0.0
</file>

<file path="run_bagging copy.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=2 --num-rounds=20 --num-clients-per-round=2 &
sleep 30  # Sleep for 30s to give the server enough time to start
for i in `seq 0 1`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --num-partitions=2 --partitioner-type=exponential &
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_bagging.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=2 --num-rounds=20 --num-clients-per-round=2 &
sleep 30  # Sleep for 30s to give the server enough time to start
# Start regular client (partition 0)
echo "Starting regular client (partition 0)"
python3 client.py --partition-id=0 --num-partitions=2 --partitioner-type=exponential &
# Start SMOTE-enhanced client (partition 1)
echo "Starting SMOTE-enhanced client (partition 1)"
python3 smote_client.py --partition-id=1 --num-partitions=2 --partitioner-type=exponential &
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_cyclic.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --train-method=cyclic --pool-size=20 --num-rounds=10 &
sleep 30  # Sleep for 15s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --train-method=cyclic --num-partitions=5 --partitioner-type=uniform &
    sleep 5
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_smote.sh">
#!/bin/bash
set -e
# Define default partition ID
PARTITION_ID=0
# Check if a partition ID was provided as an argument
if [ "$#" -ge 1 ]; then
  PARTITION_ID=$1
fi
echo "Starting SMOTE-enhanced client with partition ID: $PARTITION_ID"
# Run the SMOTE client with specified partition ID
python smote_client.py \
  --partition-id ${PARTITION_ID} \
  --train-method bagging \
  --num-partitions 2 \
  --partitioner-type exponential
echo "SMOTE client execution completed"
</file>

<file path="run.py">
import subprocess
import time
import os
def run_script(script_path):
    """Run a Python script from the given path."""
    return subprocess.Popen(['python3', script_path])
def wait_for_new_file(directory):
    """Wait for a new file to be created in the directory."""
    existing_files = set(os.listdir(directory))
    while True:
        time.sleep(5)  # Check every 5 seconds
        current_files = set(os.listdir(directory))
        new_files = current_files - existing_files
        if new_files:
            print(f"New file detected: {new_files}")
            return new_files.pop()  # Return the first detected new file
def main():
    # Paths to your scripts
    preprocessing_script = '/home/mohamed/Desktop/test_repo/data/livepreprocessing_socket.py'
    receiving_script = '/home/mohamed/Desktop/test_repo/data/receiving_data.py'
    data_directory = '/home/mohamed/Desktop/test_repo/data'
    # Start both scripts
    preprocessing_process = run_script(preprocessing_script)
    receiving_process = run_script(receiving_script)
    print("Both scripts are running...")
    try:
        # Wait for a new file to appear in the /data directory
        new_file = wait_for_new_file(data_directory)
        # Run the 'act -j run' command once the new file is detected
        subprocess.run(['act', '-j', 'run', '--container-architecture', 'linux/amd64', '-v'])
        print(f"'act -j run' executed after detecting {new_file}")
    except KeyboardInterrupt:
        print("Process interrupted by user.")
    finally:
        # Terminate the running processes
        preprocessing_process.terminate()
        receiving_process.terminate()
        print("Processes terminated.")
if __name__ == "__main__":
    main()
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from logging import INFO
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    aggregated_metrics["loss"] = loss  # Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Also store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["loss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'benign', 'dns_tunneling', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predictions,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Default mapping for multi-class predictions
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        predictions_dict['prediction_type'] = [label_mapping.get(int(p), 'unknown') for p in predictions]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions: %s", raw_predictions)
    # Log distribution of scores
    log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
        np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # Convert raw predictions to probabilities if necessary
    # (Assuming a binary classification with a threshold of 0.5)
    probabilities = 1 / (1 + np.exp(-raw_predictions))  # Example for sigmoid transformation
    predicted_labels = (probabilities >= 0.5).astype(int)
    # Log predicted class distribution
    unique, counts = np.unique(predicted_labels, return_counts=True)
    log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Save predictions to CSV
    predictions_df = pd.DataFrame({
        'predicted_label': predicted_labels,
        'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
        'prediction_score': probabilities
    })
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_fn(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Predict on test data
            y_pred = bst.predict(test_data)
            y_pred_labels = y_pred.astype(int)
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_labels, server_round, "results", y_true)
            # Compute metrics
            precision = precision_score(y_true, y_pred_labels, average='weighted')
            recall = recall_score(y_true, y_pred_labels, average='weighted')
            f1 = f1_score(y_true, y_pred_labels, average='weighted')
            # Generate confusion matrix
            conf_matrix = confusion_matrix(y_true, y_pred_labels)
            # Create metrics dictionary
            metrics = {
                "precision": float(precision),
                "recall": float(recall),
                "f1": float(f1),
                "true_negatives": int(conf_matrix[0][0]),
                "false_positives": int(conf_matrix[0][1]),
                "false_negatives": int(conf_matrix[1][0]),
                "true_positives": int(conf_matrix[1][1]),
                "predictions_file": output_path
            }
            log(INFO, f"Precision = {precision}, Recall = {recall}, F1 Score = {f1} at round {server_round}")
            log(INFO, f"Dataset with predictions saved to: {output_path}")
            return 0, metrics
    return evaluate_fn
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    test_set = load_csv_data("data/static_data.csv")["test"]
    test_set.set_format("pandas")
    test_dmatrix = transform_dataset_to_dmatrix(test_set)
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = FedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["loss"] = history.losses_distributed
else:
    results["loss"] = []
    log(INFO, "No distributed losses found in history")
# Add metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics"] = history.metrics_distributed
else:
    results["metrics"] = {}
    log(INFO, "No distributed metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    NUM_LOCAL_ROUND,
    BST_PARAMS,
)
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="smote_client.py">
"""
smote_client.py
This module is a modified version of client.py that applies SMOTE oversampling
specifically to the benign class in network traffic data.
Key Components:
- Same functionality as client.py
- Added SMOTE processing for benign class
- No modifications to the original codebase
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data
)
from utils import client_args_parser, BST_PARAMS, NUM_LOCAL_ROUND
from client_utils import XgbClient
from local_utils.smote_processor import apply_smote_wrapper
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    log(INFO, "SMOTE Client: Will apply SMOTE oversampling to benign class")
    data_directory = "data/received"
    # Load labeled data for training
    labeled_csv_path = "data/received/network_train_60.csv"
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction
    unlabeled_csv_path = "data/received/network_test_40_nolabel.csv"
    unlabeled_dataset = load_csv_data(unlabeled_csv_path)
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except AttributeError:
        # If that fails, try using the partition method (used in newer versions)
        try:
            # Newer versions use a different API
            train_partition = partitioner.partition(full_train_data)[args.partition_id]
        except (AttributeError, TypeError, IndexError):
            # As a fallback, if all partition methods fail, use a simple numerical partition
            # by getting evenly spaced indices based on partition ID
            total_samples = len(full_train_data)
            samples_per_partition = total_samples // args.num_partitions
            start_idx = args.partition_id * samples_per_partition
            end_idx = start_idx + samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
            partition_indices = list(range(start_idx, end_idx))
            train_partition = full_train_data.select(partition_indices)
            log(INFO, "Used fallback partitioning. Partition %d: samples %d to %d", 
                args.partition_id, start_idx, end_idx)
    log(INFO, "Partition size: %d samples (out of %d total)", 
        len(train_partition), len(full_train_data))
    # Handle data splitting based on evaluation strategy
    if args.centralised_eval:
        # Use centralized test set for evaluation
        train_data = train_partition
        valid_data = labeled_dataset["test"]
        valid_data.set_format("numpy")
        num_train = train_data.shape[0]
        num_val = valid_data.shape[0]
    else:
        # Perform local train/test split using the updated function
        log(INFO, "Performing local train/test split...")
        # Generate a unique seed for each client based on partition_id
        client_specific_seed = args.seed + (args.partition_id * 1000)
        log(INFO, "Using client-specific random seed for train/test split: %d", client_specific_seed)
        # Get the normal train/test split result
        train_test_result = train_test_split(
            train_partition,
            test_fraction=args.test_fraction,
            random_state=client_specific_seed  # Use client-specific seed
        )
        # Apply SMOTE to benign class
        train_dmatrix, valid_dmatrix, processor = apply_smote_wrapper(
            train_test_result, 
            random_state=client_specific_seed
        )
        # Get counts from the DMatrix objects
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
        log(INFO, "Local split with SMOTE: %d train samples, %d validation samples", num_train, num_val)
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the
federated learning process to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data>
    --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json
    --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from server_utils import load_saved_model
from dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get feature importance: %s", str(error))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get model parameters: %s", str(error))
def clean_data_for_xgboost(data_frame):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        data_frame (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = data_frame.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # Use max() to set minimum threshold
        threshold = max(threshold, 1e6)
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path):
    """
    Save detailed prediction information to CSV for multi-class classification.
    Args:
        predictions (np.ndarray): Raw predictions from the model
        output_path (str): Path to save the predictions
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are multi-dimensional (one-hot encoded)
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        # Store raw probabilities
        results_df['raw_probabilities'] = predictions.tolist()
        # Get predicted class (argmax)
        predicted_labels = np.argmax(predictions, axis=1)
        results_df['predicted_label'] = predicted_labels
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predicted_labels
        ]
        # Store confidence scores (probability of predicted class)
        results_df['prediction_score'] = predictions[
            np.arange(len(predicted_labels)),
            predicted_labels
        ]
    else:
        # For single class predictions
        results_df['predicted_label'] = predictions.astype(int)
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predictions
        ]
        # Default confidence score of 1.0 for direct class predictions
        results_df['prediction_score'] = 1.0
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    label_counts = results_df['predicted_label'].value_counts()
    log(INFO, "Prediction counts by class:")
    for label, count in label_counts.items():
        class_name = label_mapping.get(int(label), f'unknown_{label}')
        log(INFO, "  %s: %d", class_name, count)
    if 'prediction_score' in results_df.columns:
        log(INFO, "Confidence score statistics: min=%.6f, max=%.6f, mean=%.6f",
            results_df['prediction_score'].min(),
            results_df['prediction_score'].max(),
            results_df['prediction_score'].mean())
    return results_df
def evaluate_labeled_data(model, dataset, output_path):
    """Handle evaluation of labeled data."""
    # Convert to DMatrix
    dmatrix = transform_dataset_to_dmatrix(dataset)
    # Get true labels for evaluation
    y_true = dmatrix.get_label()
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save detailed predictions
    _ = save_detailed_predictions(raw_predictions, output_path)
    # Evaluate if data has labels
    if raw_predictions.ndim > 1:
        y_pred_labels = np.argmax(raw_predictions, axis=1)
    else:
        y_pred_labels = raw_predictions.astype(int)
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_labels)
    precision = precision_score(y_true, y_pred_labels, average='weighted')
    recall = recall_score(y_true, y_pred_labels, average='weighted')
    f1_score_val = f1_score(y_true, y_pred_labels, average='weighted')
    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_labels)
    # Generate classification report
    class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
    report = classification_report(y_true, y_pred_labels, target_names=class_names)
    # Log evaluation results
    log(INFO, "Evaluation Results:")
    log(INFO, "  Accuracy: %.4f", accuracy)
    log(INFO, "  Precision (weighted): %.4f", precision)
    log(INFO, "  Recall (weighted): %.4f", recall)
    log(INFO, "  F1 Score (weighted): %.4f", f1_score_val)
    log(INFO, "Confusion Matrix:\n%s", conf_matrix)
    log(INFO, "Classification Report:\n%s", report)
def predict_unlabeled_data(model, data_path, output_path):
    """Handle prediction of unlabeled data."""
    # Load unlabeled data
    data = pd.read_csv(data_path)
    # Clean data
    data = clean_data_for_xgboost(data)
    # Convert to DMatrix
    dmatrix = xgb.DMatrix(data)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save predictions
    save_detailed_predictions(raw_predictions, output_path)
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Process data based on whether it has labels
        if args.has_labels:
            try:
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                evaluate_labeled_data(model, dataset, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during evaluation: %s", str(error))
                raise
        else:
            try:
                predict_unlabeled_data(model, args.data_path, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during prediction: %s", str(error))
                raise
    except Exception as error:  # pylint: disable=broad-except
        log(INFO, "Error: %s", str(error))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 1
BST_PARAMS = {
    "objective": "multi:softmax",
    "num_class": 3,
    "eta": 0.05,  # Reduced learning rate to prevent overfitting
    "max_depth": 3,  # Reduced max_depth to prevent memorization
    "min_child_weight": 10,  # Increased to prevent fitting to small samples
    "gamma": 1.0,  # Increased minimum loss reduction for split
    "subsample": 0.7,  # Sample fewer rows per iteration
    "colsample_bytree": 0.6,  # Sample fewer features per tree
    "colsample_bylevel": 0.6,  # Sample fewer features per level
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 5,
    "reg_alpha": 2.0,  # Increased L1 regularization
    "reg_lambda": 5.0,  # Increased L2 regularization
    "base_score": 0.5,  # Neutral starting point
    "scale_pos_weight": [1.0, 2.0, 1.0],  # More moderate weight adjustment for dns_tunneling
    "grow_policy": "lossguide",  # Alternative tree growing policy
    "normalize_type": "tree",  # Helps with interpretability
    "random_state": 42  # Fixed seed for reproducibility
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

</files>
