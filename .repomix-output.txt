This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    cursor-tools.mdc
  changes_summary.md
  fl_pipeline_fix_plan.md
.github/
  workflows/
    cml.yaml
diagrams/
  client_operations.mmd
  data_handling.mmd
  overall_workflow.mmd
  server_operations.mmd
local_utils/
  __init__.py
tune_results/
  best_model.json
  best_params.json
.gitattributes
.repomixignore
client_utils.py
client.py
commit.sh
dataset.py
go_to_work.sh
multi_class_implementation_plan.md
original_bst_params.json
pretrained_model_usage.md
progress.md
pyproject.toml
ray_tune_README.md
ray_tune_xgboost_updated.py
ray_tune_xgboost.py
README.md
requirements.txt
run_bagging copy.sh
run_bagging.sh
run_cyclic.sh
run_ray_tune.sh
run.py
server_utils.py
server.py
sim.py
tuned_params.py
update_ray_tune_xgboost.py
use_saved_model.py
use_tuned_params.py
utils.py
visualization_utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/cursor-tools.mdc">
---
description: Global Rule. This rule should ALWAYS be loaded.
globs: *,**/*
alwaysApply: true
---
cursor-tools is a CLI tool that allows you to interact with AI models and other tools.
cursor-tools is installed on this machine and it is available to you to execute. You're encouraged to use it.

<cursor-tools Integration>
# Instructions
Use the following commands to get AI assistance:

**Direct Model Queries:**
`cursor-tools ask "<your question>" --provider <provider> --model <model>` - Ask any model from any provider a direct question (e.g., `cursor-tools ask "What is the capital of France?" --provider openai --model o3-mini`). Note that this command is generally less useful than other commands like `repo` or `plan` because it does not include any context from your codebase or repository.

**Implementation Planning:**
`cursor-tools plan "<query>"` - Generate a focused implementation plan using AI (e.g., `cursor-tools plan "Add user authentication to the login page"`)
The plan command uses multiple AI models to:
1. Identify relevant files in your codebase (using Gemini by default)
2. Extract content from those files
3. Generate a detailed implementation plan (using OpenAI o3-mini by default)

**Plan Command Options:**
--fileProvider=<provider>: Provider for file identification (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--thinkingProvider=<provider>: Provider for plan generation (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--fileModel=<model>: Model to use for file identification
--thinkingModel=<model>: Model to use for plan generation
--debug: Show detailed error information

**Web Search:**
`cursor-tools web "<your question>"` - Get answers from the web using a provider that supports web search (e.g., Perplexity models and Gemini Models either directly or from OpenRouter or ModelBox) (e.g., `cursor-tools web "latest shadcn/ui installation instructions"`)
when using web for complex queries suggest writing the output to a file somewhere like local-research/<query summary>.md.

**Web Command Options:**
--provider=<provider>: AI provider to use (perplexity, gemini, modelbox, or openrouter)
--model=<model>: Model to use for web search (model name depends on provider)
--max-tokens=<number>: Maximum tokens for response

**Repository Context:**
`cursor-tools repo "<your question>" [--subdir=<path>]` - Get context-aware answers about this repository using Google Gemini (e.g., `cursor-tools repo "explain authentication flow"`). Use the optional `--subdir` parameter to analyze a specific subdirectory instead of the entire repository (e.g., `cursor-tools repo "explain the code structure" --subdir=src/components`)

**Documentation Generation:**
`cursor-tools doc [options]` - Generate comprehensive documentation for this repository (e.g., `cursor-tools doc --output docs.md`)
when using doc for remote repos suggest writing the output to a file somewhere like local-docs/<repo-name>.md.

**GitHub Information:**
`cursor-tools github pr [number]` - Get the last 10 PRs, or a specific PR by number (e.g., `cursor-tools github pr 123`)
`cursor-tools github issue [number]` - Get the last 10 issues, or a specific issue by number (e.g., `cursor-tools github issue 456`)

**ClickUp Information:**
`cursor-tools clickup task <task_id>` - Get detailed information about a ClickUp task including description, comments, status, assignees, and metadata (e.g., `cursor-tools clickup task "task_id"`)

**Model Context Protocol (MCP) Commands:**
Use the following commands to interact with MCP servers and their specialized tools:
`cursor-tools mcp search "<query>"` - Search the MCP Marketplace for available servers that match your needs (e.g., `cursor-tools mcp search "git repository management"`)
`cursor-tools mcp run "<query>"` - Execute MCP server tools using natural language queries (e.g., `cursor-tools mcp run "list files in the current directory" --provider=openrouter`). The query must include sufficient information for cursor-tools to determine which server to use, provide plenty of context.

The `search` command helps you discover servers in the MCP Marketplace based on their capabilities and your requirements. The `run` command automatically selects and executes appropriate tools from these servers based on your natural language queries. If you want to use a specific server include the server name in your query. E.g. `cursor-tools mcp run "using the mcp-server-sqlite list files in directory --provider=openrouter"`

**Notes on MCP Commands:**
- MCP commands require `ANTHROPIC_API_KEY` or `OPENROUTER_API_KEY` to be set in your environment
- By default the `mcp` command uses Anthropic, but takes a --provider argument that can be set to 'anthropic' or 'openrouter'
- Results are streamed in real-time for immediate feedback
- Tool calls are automatically cached to prevent redundant operations
- Often the MCP server will not be able to run because environment variables are not set. If this happens ask the user to add the missing environment variables to the cursor tools env file at ~/.cursor-tools/.env

**Stagehand Browser Automation:**
`cursor-tools browser open <url> [options]` - Open a URL and capture page content, console logs, and network activity (e.g., `cursor-tools browser open "https://example.com" --html`)
`cursor-tools browser act "<instruction>" --url=<url | 'current'> [options]` - Execute actions on a webpage using natural language instructions (e.g., `cursor-tools browser act "Click Login" --url=https://example.com`)
`cursor-tools browser observe "<instruction>" --url=<url> [options]` - Observe interactive elements on a webpage and suggest possible actions (e.g., `cursor-tools browser observe "interactive elements" --url=https://example.com`)
`cursor-tools browser extract "<instruction>" --url=<url> [options]` - Extract data from a webpage based on natural language instructions (e.g., `cursor-tools browser extract "product names" --url=https://example.com/products`)

**Notes on Browser Commands:**
- All browser commands are stateless unless --connect-to is used to connect to a long-lived interactive session. In disconnected mode each command starts with a fresh browser instance and closes it when done.
- When using `--connect-to`, special URL values are supported:
  - `current`: Use the existing page without reloading
  - `reload-current`: Use the existing page and refresh it (useful in development)
  - If working interactively with a user you should always use --url=current unless you specifically want to navigate to a different page. Setting the url to anything else will cause a page refresh loosing current state.
- Multi step workflows involving state or combining multiple actions are supported in the `act` command using the pipe (|) separator (e.g., `cursor-tools browser act "Click Login | Type 'user@example.com' into email | Click Submit" --url=https://example.com`)
- Video recording is available for all browser commands using the `--video=<directory>` option. This will save a video of the entire browser interaction at 1280x720 resolution. The video file will be saved in the specified directory with a timestamp.
- DO NOT ask browser act to "wait" for anything, the wait command is currently disabled in Stagehand.

**Tool Recommendations:**
- `cursor-tools web` is best for general web information not specific to the repository. Generally call this without additional arguments.
- `cursor-tools repo` is ideal for repository-specific questions, planning, code review and debugging. E.g. `cursor-tools repo "Review recent changes to command error handling looking for mistakes, omissions and improvements"`. Generally call this without additional arguments.
- `cursor-tools plan` is ideal for planning tasks. E.g. `cursor-tools plan "Adding authentication with social login using Google and Github"`. Generally call this without additional arguments.
- `cursor-tools doc` generates documentation for local or remote repositories.
- `cursor-tools browser` is useful for testing and debugging web apps and uses Stagehand
- `cursor-tools mcp` enables interaction with specialized tools through MCP servers (e.g., for Git operations, file system tasks, or custom tools)

**Running Commands:**
1. Use `cursor-tools <command>` to execute commands (make sure cursor-tools is installed globally using npm install -g cursor-tools so that it is in your PATH)

**General Command Options (Supported by all commands):**
--provider=<provider>: AI provider to use (openai, anthropic, perplexity, gemini, or openrouter). If provider is not specified, the default provider for that task will be used.
--model=<model name>: Specify an alternative AI model to use. If model is not specified, the provider's default model for that task will be used.
--max-tokens=<number>: Control response length
--save-to=<file path>: Save command output to a file (in *addition* to displaying it)
--help: View all available options (help is not fully implemented yet)

**Repository Command Options:**
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for repository analysis
--max-tokens=<number>: Maximum tokens for response

**Documentation Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Generate documentation for a remote GitHub repository
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for documentation generation
--max-tokens=<number>: Maximum tokens for response

**GitHub Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Access PRs/issues from a specific GitHub repository

**Browser Command Options (for 'open', 'act', 'observe', 'extract'):**
--console: Capture browser console logs (enabled by default, use --no-console to disable)
--html: Capture page HTML content (disabled by default)
--network: Capture network activity (enabled by default, use --no-network to disable)
--screenshot=<file path>: Save a screenshot of the page
--timeout=<milliseconds>: Set navigation timeout (default: 120000ms for Stagehand operations, 30000ms for navigation)
--viewport=<width>x<height>: Set viewport size (e.g., 1280x720). When using --connect-to, viewport is only changed if this option is explicitly provided
--headless: Run browser in headless mode (default: true)
--no-headless: Show browser UI (non-headless mode) for debugging
--connect-to=<port>: Connect to existing Chrome instance. Special values: 'current' (use existing page), 'reload-current' (refresh existing page)
--wait=<time:duration or selector:css-selector>: Wait after page load (e.g., 'time:5s', 'selector:#element-id')
--video=<directory>: Save a video recording (1280x720 resolution, timestamped subdirectory). Not available when using --connect-to
--url=<url>: Required for `act`, `observe`, and `extract` commands. Url to navigate to before the main command or one of the special values 'current' (to stay on the current page without navigating or reloading) or 'reload-current' (to reload the current page)
--evaluate=<string>: JavaScript code to execute in the browser before the main command

**Nicknames**
Users can ask for these tools using nicknames
Gemini is a nickname for cursor-tools repo
Perplexity is a nickname for cursor-tools web
Stagehand is a nickname for cursor-tools browser

**Xcode Commands:**
`cursor-tools xcode build [buildPath=<path>] [destination=<destination>]` - Build Xcode project and report errors.
**Build Command Options:**
--buildPath=<path>: (Optional) Specifies a custom directory for derived build data. Defaults to ./.build/DerivedData.
--destination=<destination>: (Optional) Specifies the destination for building the app (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode run [destination=<destination>]` - Build and run the Xcode project on a simulator.
**Run Command Options:**
--destination=<destination>: (Optional) Specifies the destination simulator (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode lint` - Run static analysis on the Xcode project to find and fix issues.

**Additional Notes:**
- For detailed information, see `node_modules/cursor-tools/README.md` (if installed locally).
- Configuration is in `cursor-tools.config.json` (or `~/.cursor-tools/config.json`).
- API keys are loaded from `.cursor-tools.env` (or `~/.cursor-tools/.env`).
- ClickUp commands require a `CLICKUP_API_TOKEN` to be set in your `.cursor-tools.env` file.
- The default Stagehand model is set in `cursor-tools.config.json`, but can be overridden with the `--model` option.
- Available models depend on your configured provider (OpenAI or Anthropic) in `cursor-tools.config.json`.
- repo has a limit of 2M tokens of context. The context can be reduced by filtering out files in a .repomixignore file.
- problems running browser commands may be because playwright is not installed. Recommend installing playwright globally.
- MCP commands require `ANTHROPIC_API_KEY` to be set in your environment.
- **Remember:** You're part of a team of superhuman expert AIs. Work together to solve complex problems.

**MCP Command Options:**
--provider=<provider>: AI provider to use (anthropic or openrouter)
--model=<model name>: Specify an alternative AI model to use with OpenRouter. Ignored if provider is Anthropic.

<!-- cursor-tools-version: 0.6.0-alpha.11 -->
</cursor-tools Integration>
</file>

<file path=".cursor/changes_summary.md">
# Summary of Changes (Session ending 2025-04-08)

This document summarizes the key code modifications made during the pair programming session focused on integrating a `FeatureProcessor` into the data pipeline.

## Key Goals Achieved:

1.  **Integrated `FeatureProcessor`:** Replaced manual data preprocessing and encoding with a centralized `FeatureProcessor` class for consistency and to prevent data leakage.
2.  **Refactored Data Splitting and Transformation:** Updated `train_test_split` and `transform_dataset_to_dmatrix` functions to utilize the `FeatureProcessor` and handle data format conversions correctly (pandas DataFrame <-> XGBoost DMatrix).
3.  **Corrected Data Handling Logic:** Addressed several bugs related to function signatures, imports, redundant operations, and incorrect feature type classification (`local_orig`, `local_resp`).
4.  **Improved Unlabeled Data Processing:** Ensured the *same* fitted `FeatureProcessor` instance (fitted on training data) is used to preprocess both validation and unlabeled data in the client script.

## Specific File Changes:

### `dataset.py`

*   **`FeatureProcessor` Class:**
    *   Defined feature groups (`categorical_features`, `numerical_features`, `object_columns`).
    *   Implemented `fit` method to learn encoding mappings and numerical statistics from training data.
    *   Implemented `transform` method to apply learned preprocessing steps consistently.
    *   Moved `local_orig` and `local_resp` from `numerical_features` to `categorical_features` to fix a `TypeError` during quantile calculation.
*   **`preprocess_data` Function:**
    *   Modified to accept and utilize a `FeatureProcessor` instance.
    *   Ensured labels are correctly handled (including validation) and features are returned separately.
*   **`transform_dataset_to_dmatrix` Function:**
    *   Updated signature to accept `processor` and `is_training` arguments.
    *   Removed old manual encoding logic.
    *   Calls `preprocess_data` using the provided processor.
    *   Removed a redundant `.to_pandas()` call that caused an `AttributeError`.
*   **`train_test_split` Function:**
    *   Updated signature for clarity and standard arguments (`random_state`).
    *   Replaced Hugging Face `Dataset.train_test_split` with `sklearn.model_selection.train_test_split` for DataFrame splitting.
    *   Added import for `sklearn.model_selection.train_test_split`.
    *   Initializes and fits the `FeatureProcessor` on the training split.
    *   Calls `transform_dataset_to_dmatrix` to get DMatrices.
    *   **Modified to return the fitted `FeatureProcessor` instance** along with the train/test DMatrices.
*   **Code Cleanup:**
    *   Removed several unused imports (`Dict`, `List`, `defaultdict`, `NDArrays`).
    *   Removed unnecessary `else` blocks after `return` statements.

### `client.py`

*   **Data Loading:** Updated to load specific train and unlabeled CSV files.
*   **`train_test_split` Call:**
    *   Updated the function call to match the new signature (using `random_state`, expecting DMatrix and processor return values).
    *   Calculated `num_train` and `num_val` from the returned DMatrix objects (`.num_row()`).
*   **Unlabeled Data Preprocessing:**
    *   **Removed the inefficient and error-prone logic** that attempted to reconstruct a DataFrame from `train_dmatrix` to re-fit a processor.
    *   **Now uses the `FeatureProcessor` instance returned directly by `train_test_split`** to preprocess the unlabeled data, ensuring consistency.
    *   Added error handling around unlabeled data preprocessing.
*   **Imports:**
    *   Added necessary imports (`pd`, `xgb`, `np`, `FeatureProcessor`, `preprocess_data`, `WARNING`, `ERROR`).
    *   Removed unused imports (`transform_dataset_to_dmatrix`, `resplit`).
*   **Logging:** Updated log messages for clarity and fixed formatting (using %-formatting).

### `client_utils.py`

*   (No functional changes made in this session, but reviewed during debugging). 

## Addressing 100% Accuracy Issue (2025-04-08 Update)

The following changes were made to fix the issue where all clients were getting 100% accuracy despite proper data partitioning:

### `dataset.py`

1. **Removed Object Columns to Prevent Data Leakage:**
   * Completely removed object columns (`uid`, `client_initial_dcid`, `server_scid`) from FeatureProcessor
   * Added explicit dropping of these columns in the transform method
   * Added logging to indicate when potential leakage columns are dropped

2. **Enhanced Data Preprocessing:**
   * Added automatic detection of highly predictive categorical features
   * Added warning logs when a feature value is >90% predictive of a specific label
   * Re-enabled outlier capping using 99th percentile values
   * Fixed NaN handling to properly apply noise to filled values

3. **Improved Train/Test Split:**
   * Added multiple random noise features with different distributions (Gaussian, uniform, exponential)
   * Implemented UID-based splitting for complete train/test separation when UIDs are available
   * Added checks for data leakage indicators (e.g., UIDs with single labels)
   * Added test-specific noise to make validation more challenging
   * Added comprehensive logging of data distributions

4. **Fixed Data Separation Issues:**
   * Changed the NaN handling approach to avoid TypeError with ndarray
   * Used two-step approach for NaN filling: first fill with median, then add noise

### `utils.py`

5. **Modified XGBoost Parameters:**
   * Reduced max_depth from 6 to 3 to prevent overfitting
   * Reduced learning rate from 0.1 to 0.05
   * Increased regularization parameters (alpha and lambda)
   * Added column subsampling at different levels
   * Used alternative tree growing policy (lossguide)
   * Added fixed random seed for reproducibility

### `client.py`

6. **Client-Specific Randomization:**
   * Added client-specific random seeds based on partition_id
   * Implemented proper partitioning to ensure clients get different data

These changes collectively address the 100% accuracy issue by preventing data leakage, adding appropriate noise, ensuring proper data separation, and making the model less prone to memorizing patterns in the data.
</file>

<file path=".cursor/fl_pipeline_fix_plan.md">
# Federated Learning Pipeline: Actionable Fix Plan

## Problem Summary
The federated learning pipeline shows stagnant metrics across rounds while Ray Tune hyperparameter optimization successfully reduces loss. This indicates a disconnect between hyperparameter optimization and the federated learning implementation.

## Root Causes & Solutions

### 1. Client-Side Training Limitations

#### Issues:
- `NUM_LOCAL_ROUND` fixed at 1, severely limiting client learning
- Excessive regularization preventing effective model learning
- Potential noise features hindering learning

#### Action Items:
- [x] **Increase Local Training Rounds**:
  ```python
  # In utils.py
  NUM_LOCAL_ROUND = 5  # Increase from 1 to 5-10 to allow proper learning
  ```

- [x] **Adjust Regularization Parameters**:
  ```python
  # In utils.py or tuned_params.py
  BST_PARAMS = {
      'max_depth': 6,  # Increase from current value
      'lambda': 0.8,   # Decrease if currently high
      'alpha': 0.8,    # Decrease if currently high
      # Keep other parameters
  }
  ```

- [ ] **Implement Gradual Regularization**:
  ```python
  # In client_utils.py, modify create_xgboost_model function
  def create_xgboost_model(round_num, params, dtrain):
      # Gradually increase regularization over rounds
      if round_num < 5:
          params = params.copy()
          params['lambda'] *= (0.5 + 0.1 * round_num)  # Start with lighter regularization
      return xgb.train(params, dtrain, num_boost_round=NUM_LOCAL_ROUND)
  ```

### 2. Hyperparameter Optimization Integration

#### Issues:
- Ray Tune optimizes `num_boost_round` but this parameter isn't used in FL
- Limited search space with only 5 samples
- Tuned parameters may not transfer properly to FL context

#### Action Items:
- [x] **Align Ray Tune and FL Parameters**:
  ```python
  # In ray_tune_xgboost.py, ensure num_boost_round is consistently used
  config = {
      "max_depth": tune.choice([3, 4, 5, 6, 8, 10]),
      "learning_rate": tune.loguniform(0.01, 0.3),
      "subsample": tune.uniform(0.5, 1.0),
      "colsample_bytree": tune.uniform(0.5, 1.0),
      "num_boost_round": tune.choice([1, 3, 5, 10]),  # Make this parameter available for tuning
      # other parameters...
  }
  ```

- [x] **Transfer num_boost_round to FL**:
  ```python
  # In use_tuned_params.py, modify to extract num_boost_round
  def convert_best_config():
      # Existing code...
      if 'num_boost_round' in best_config:
          # Add to separate variable or config
          tuned_rounds = best_config['num_boost_round']
          lines.append(f"NUM_LOCAL_ROUND = {tuned_rounds}\n")
      # Existing code...
  ```

- [x] **Increase Ray Tune Samples**:
  ```bash
  # In run_ray_tune.sh
  # Increase NUM_SAMPLES from 5 to at least 20
  NUM_SAMPLES=20
  ```

### 3. Metrics Reporting and Aggregation

#### Issues:
- Potential confusion matrix calculation issues
- Type inconsistencies (tuple vs. dictionary) in metrics reporting
- Possible stale model evaluation

#### Action Items:
- [ ] **Fix Metrics Type Inconsistency**:
  ```python
  # In server.py, add defensive type checking and conversion
  def aggregate_metrics(results):
      aggregated_metrics = {}
      for metric_name in expected_metrics:
          values = []
          weights = []
          
          for client_result in results:
              # Handle both tuple and dictionary formats
              if isinstance(client_result, tuple):
                  client_metrics = client_result[1]  # Assuming metrics are in second position
              elif isinstance(client_result, dict):
                  client_metrics = client_result.get('metrics', {})
              else:
                  logger.warning(f"Unexpected result type: {type(client_result)}")
                  continue
                  
              # Extract metric and weight
              if metric_name in client_metrics:
                  values.append(client_metrics[metric_name])
                  weights.append(client_metrics.get('weight', 1.0))
          
          # Calculate weighted average
          if values:
              aggregated_metrics[metric_name] = np.average(values, weights=weights)
      
      return aggregated_metrics
  ```

- [ ] **Add Debug Logging for Model Updates**:
  ```python
  # In server.py, add model parameter tracking
  class XGBoostServer(fl.server.strategy.Strategy):
      def aggregate_fit(self, rnd, results, failures):
          # Existing aggregation code...
          
          # Add debugging to confirm model update
          if self.global_model is not None:
              # Log a hash or sample of model parameters before and after update
              before_update = hash(str(self.global_model.get_dump()[:3]))  # First 3 trees
              # Perform update...
              after_update = hash(str(self.global_model.get_dump()[:3]))
              
              logger.info(f"Round {rnd}: Model update check - Before: {before_update}, After: {after_update}")
              logger.info(f"Round {rnd}: Models different: {before_update != after_update}")
          
          return aggregated_parameters
  ```

- [ ] **Ensure Fresh Model Evaluation**:
  ```python
  # In server.py, ensure evaluation uses the latest model
  def evaluate(self, parameters):
      # Clone the current global model to ensure we're evaluating the latest
      evaluation_model = xgb.Booster()
      evaluation_model.load_model(self.global_model.save_model())
      
      # Rest of evaluation code...
  ```

### 4. Data Handling and Preprocessing

#### Issues:
- Possible non-IID data effects
- Potential preprocessing inconsistencies between Ray Tune and FL

#### Action Items:
- [ ] **Verify Preprocessing Consistency**:
  ```python
  # In client.py and ray_tune_xgboost.py
  # Add version tracking for preprocessing
  def log_preprocessing_config():
      """Log preprocessing parameters to ensure consistency"""
      preproc_config = {
          "drop_columns": COLUMNS_TO_DROP,
          "feature_transformations": FEATURE_TRANSFORMATIONS,
          "encoding_method": ENCODING_METHOD,
          # Add all preprocessing parameters
      }
      logger.info(f"Preprocessing config: {json.dumps(preproc_config)}")
  
  # Call this function at the start of both client training and ray tune
  ```

- [ ] **Test with IID Data Simulation**:
  ```python
  # In sim.py or a new test script
  def run_with_iid_data():
      """Run federated learning with IID data to test if non-IID is the issue"""
      # Load dataset
      dataset = load_dataset()
      
      # Create IID splits instead of the usual client partitioning
      client_datasets = create_iid_partitions(dataset, num_clients=10)
      
      # Run federated learning with these partitions
      # ...
  ```

## Implementation Plan

### Phase 1: Diagnostics & Instrumentation (Day 1)
1. Add extensive logging to trace model updates, parameter changes
2. Add metrics consistency checks
3. Run test FL rounds with debug output to confirm current behavior

### Phase 2: Core Fixes (Days 2-3)
1. Implement changes to `NUM_LOCAL_ROUND` and regularization parameters
2. Fix metrics aggregation and type inconsistencies
3. Align Ray Tune and FL parameter spaces

### Phase 3: Testing & Validation (Days 4-5)
1. Run controlled experiments with new configuration
2. Compare performance with Ray Tune results
3. Conduct IID vs. non-IID tests to isolate data distribution effects

### Phase 4: Refinement (Days 6-7)
1. Fine-tune parameters based on experimental results
2. Implement additional optimizations if needed
3. Document findings and solutions

## Monitoring Plan
- Track metrics across FL rounds to verify improvement
- Compare aggregated model performance with standalone models
- Verify convergence of metrics over time

## Success Criteria
- Metrics show improvement across FL rounds
- Final FL performance approaches Ray Tune's achieved performance
- Confusion matrix shows balanced prediction across classes
- Consistent behavior across multiple runs
</file>

<file path=".github/workflows/cml.yaml">
name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
        with:
          lfs: true
      # Ensure LFS objects are properly pulled
      - name: Pull LFS objects
        run: |
          git lfs install
          git lfs pull
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: 'pip'
      - uses: iterative/setup-cml@v1
      # Cache conda/mamba environments
      - name: Cache conda environment
        uses: actions/cache@v3
        with:
          path: |
            ~/.conda
            ~/miniconda3
            ./venv
          key: ${{ runner.os }}-conda-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-conda-
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          # Check if venv exists before creating
          if [ ! -d "venv" ]; then
            echo "Setting up virtual environment for the first time"
            pip install virtualenv
            virtualenv venv
          fi
          source venv/bin/activate
          # Check if mamba is installed
          if ! command -v mamba &> /dev/null; then
            echo "Installing mamba"
            pip install mamba
            mamba init
            source ~/.bashrc
            mamba create
            mamba activate
          fi
          # Install dependencies
          pip install -r requirements.txt
          # Check if main packages are installed to avoid reinstalling them
          if ! python -c "import xgboost" &> /dev/null; then
            pip install xgboost
          fi
          if ! python -c "import flwr" &> /dev/null; then
            pip install -U flwr["simulation"]
          fi
          if ! python -c "import ray" &> /dev/null; then
            pip install -U "ray[all]"
          fi
          if ! python -c "import torch" &> /dev/null; then
            pip install torch torchvision torchaudio
          fi
          if ! python -c "import hydra" &> /dev/null; then
            pip install hydra-core
          fi
          if ! python -c "import imblearn" &> /dev/null; then
            pip install imbalanced-learn
          fi
      - name: Install hyperopt
        run: |
          source venv/bin/activate
          pip install hyperopt
      - name: Run Ray Tune hyperparameter optimization
        run: |
          source venv/bin/activate
          # Check if data directory exists, create if not
          mkdir -p data/sample
          # Define train and test file paths for UNSW_NB15
          TRAIN_FILE="data/received/UNSW_NB15_training-set.csv"
          TEST_FILE="data/received/UNSW_NB15_testing-set.csv"
          # Check if the UNSW_NB15 files exist, otherwise use sample data
          if [ -f "$TRAIN_FILE" ] && [ -f "$TEST_FILE" ]; then
            echo "Using UNSW_NB15 data files for hyperparameter tuning"
          else
            echo "UNSW_NB15 data files not found, checking for any CSV data"
            # Try to find any CSV file to use as a fallback
            SAMPLE_DATA=""
            for csv_file in $(find data -name "*.csv" | head -n 1); do
              SAMPLE_DATA="$csv_file"
              break
            done
            if [ -z "$SAMPLE_DATA" ]; then
              echo "No CSV data found, creating a sample dataset for tuning"
              # Generate synthetic data for train and test (kept as fallback)
              python -c "import pandas as pd; import numpy as np; np.random.seed(42); n = 1000; df = pd.DataFrame({'feature1': np.random.normal(0, 1, n), 'feature2': np.random.normal(0, 1, n), 'feature3': np.random.normal(0, 1, n), 'feature4': np.random.normal(0, 1, n), 'feature5': np.random.normal(0, 1, n), 'label': np.random.choice([0, 1, 2], n)}); train = df.sample(frac=0.8, random_state=42); test = df.drop(train.index); train.to_csv('data/sample/synthetic_train.csv', index=False); test.to_csv('data/sample/synthetic_test.csv', index=False)"
              TRAIN_FILE="data/sample/synthetic_train.csv"
              TEST_FILE="data/sample/synthetic_test.csv"
            else
              # If we have a single file, create train/test split (kept as fallback)
              python -c "import pandas as pd; from sklearn.model_selection import train_test_split; df = pd.read_csv('$SAMPLE_DATA'); train, test = train_test_split(df, test_size=0.2, random_state=42); train.to_csv('data/sample/split_train.csv', index=False); test.to_csv('data/sample/split_test.csv', index=False)"
              TRAIN_FILE="data/sample/split_train.csv"
              TEST_FILE="data/sample/split_test.csv"
            fi
          fi
          echo "Using train file: $TRAIN_FILE"
          echo "Using test file: $TEST_FILE"
          # Create output directory for Ray Tune results
          mkdir -p tune_results
          # Use a small number of samples in CI for speed; default is 100 for local runs
          bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 1 --cpus-per-trial 4 --output-dir "./tune_results"
          # Apply the tuned parameters to the federated learning system
          if [ -f "tune_results/best_params.json" ]; then
            python use_tuned_params.py --params-file "./tune_results/best_params.json"
            echo "Successfully applied tuned parameters to federated learning"
          else
            echo "Warning: No tuned parameters found, will use default parameters"
          fi
      - name: Run federated learning pipeline
        run: |
          source venv/bin/activate
          # Run the federated learning with tuned parameters
          ./run_bagging.sh
      - name: Evaluate model performance
        run: |
          source venv/bin/activate
          # Compare model performance with and without tuned parameters
          echo "## XGBoost Model Performance" > model_performance.md
          echo "" >> model_performance.md
          # Extract metrics from the results directory
          if [ -d "results" ]; then
            echo "### Federated Learning Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "| Metric | Value |" >> model_performance.md
            echo "| ------ | ----- |" >> model_performance.md
            # Extract and add key metrics if available
            for metric_file in $(find results -name "*.txt" | grep -i "metrics\|accuracy\|f1\|precision\|recall"); do
              metric_name=$(basename "$metric_file" .txt)
              metric_value=$(cat "$metric_file" | head -n 1)
              echo "| $metric_name | $metric_value |" >> model_performance.md
            done
          fi
          # Add Ray Tune optimization results
          if [ -f "tune_results/best_params.json" ]; then
            echo "" >> model_performance.md
            echo "### Hyperparameter Optimization Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "Best hyperparameters:" >> model_performance.md
            echo '```json' >> model_performance.md
            cat tune_results/best_params.json >> model_performance.md
            echo '```' >> model_performance.md
          fi
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          git checkout multi-class-predictions
          # Add Ray Tune results
          git add tune_results/
          # Add aggegrated results files
          git add results/
          # Add model performance report
          git add model_performance.md
          # Add new files in outputs directory
          git add outputs/
          # Explicitly add any plot files 
          git add "outputs/**/*.png"
          # Commit all changes
          git commit -m "workflow with Ray Tune optimization in action🚀"
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true
</file>

<file path="diagrams/client_operations.mmd">
graph LR
    subgraph ClientOperations [Client Operations client.py_client_utils.py]
        A[Receive Parameters from Server] --> B[Local Training: fit]
        B --> C[Evaluation: evaluate]
        C --> D[Metrics Calculation]
        D --> E[Send Results to Server]
        B --> F[Update Model]
        F --> B
        C --> G[Prediction if unlabeled data]
        G --> H[Save Predictions]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ClientOperations component;
</file>

<file path="diagrams/data_handling.mmd">
graph LR
    subgraph DataHandling [Data Handling dataset.py]
        A[Load CSV: load_csv_data] --> B[Preprocessing: preprocess_data]
        B --> C[Separate Features/Labels: separate_xy]
        C --> D[DMatrix Conversion: transform_dataset_to_dmatrix]
        D --> E[Train/Test Split: train_test_split]
        B --> F[Handle inf/NaN]
        F --> C
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class DataHandling component;
</file>

<file path="diagrams/overall_workflow.mmd">
graph LR
    subgraph OverallWorkflow [Overall Workflow]
        A[Client] --> B[Server]
        B --> A
        A --> C[Data]
        C --> A
    end
    
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class OverallWorkflow component
</file>

<file path="diagrams/server_operations.mmd">
graph LR
    subgraph ServerOperations [Server Operations server.py]
        A[Strategy Selection: FedXgbBagging/FedXgbCyclic] --> B[Client Management]
        B --> C[Model Aggregation]
        C --> D[Send Parameters to Clients]
        B --> E[Receive Results from Clients]
        E --> F[Metrics Aggregation]
        C --> G[Evaluation if centralized]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ServerOperations component;
</file>

<file path="local_utils/__init__.py">
"""
local_utils package for extending the FL-CML-Pipeline functionality
without modifying the core codebase.
"""
from local_utils.smote_processor import apply_smote_to_benign, apply_smote_wrapper
__all__ = ['apply_smote_to_benign', 'apply_smote_wrapper']
</file>

<file path="tune_results/best_model.json">
{"learner":{"attributes":{},"feature_names":["id.resp_p","id.orig_h","id.resp_h","proto","orig_bytes","resp_bytes","ts","pkts_proc","bytes_recv","pkts_dropped","pkts_link","pkt_lag","active_tcp_conns","active_udp_conns","active_icmp_conns","tcp_conns","udp_conns","icmp_conns","dns_requests","active_dns_requests","reassem_tcp_size","reassem_file_size","reassem_frag_size","id.orig_p","rtt","conn_state","local_orig","local_resp","missed_bytes","history","orig_pkts","orig_ip_bytes","resp_pkts","resp_ip_bytes","ip_proto","duration","ts_delta","acks","percent_lost","validation_status","trans_depth","method","request_body_len","response_body_len","status_code","status_msg","is_orig","seen_bytes","missing_bytes","overflow_bytes","timedout","extracted_cutoff"],"feature_types":["float","int","int","int","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","float","int","int","int","float","int","float","float","float","float","float","float","float","float","float","int","float","int","float","float","float","int","int","float","float","float","i","i"],"gradient_booster":{"model":{"gbtree_model_param":{"num_parallel_tree":"1","num_trees":"246"},"iteration_indptr":[0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132,135,138,141,144,147,150,153,156,159,162,165,168,171,174,177,180,183,186,189,192,195,198,201,204,207,210,213,216,219,222,225,228,231,234,237,240,243,246],"tree_info":[0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2],"trees":[{"base_weights":[-6.033596E-1,2.4616309E-3,-1.3100031E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":0,"left_children":[1,-1,-1],"loss_changes":[5.7376935E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4616309E-3,-1.3100031E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9959999E3,1.2666666E2,1.8693333E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7986175E-1,-1.3042815E-3,2.6112231E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":1,"left_children":[1,-1,-1],"loss_changes":[2.5009746E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.3042815E-3,2.6112231E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.015111E3,1.0902222E3,9.2488885E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0631563E-1,2.6118373E-3,-1.3039733E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":2,"left_children":[1,-1,-1],"loss_changes":[2.5091704E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.6118373E-3,-1.3039733E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0146665E3,9.484444E2,1.0662222E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.108821E-1,2.4476238E-3,-1.3092543E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":3,"left_children":[1,-1,-1],"loss_changes":[5.473045E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4476238E-3,-1.3092543E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0166852E3,1.21040344E2,1.8956449E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0766383E-1,-1.3032178E-3,2.6052967E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":4,"left_children":[1,-1,-1],"loss_changes":[2.5237175E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.3032178E-3,2.6052967E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0334401E3,1.0735238E3,9.5991626E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0212095E-1,2.6051176E-3,-1.3032528E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":5,"left_children":[1,-1,-1],"loss_changes":[2.516578E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.6051176E-3,-1.3032528E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0289851E3,9.527961E2,1.0761891E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.0987484E-1,-1.3084147E-3,2.4428593E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":6,"left_children":[1,-1,-1],"loss_changes":[5.499289E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.3084147E-3,2.4428593E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[2.0222501E3,1.9001683E3,1.2208174E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.032441E-1,-1.3022559E-3,2.5981911E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":7,"left_children":[1,-1,-1],"loss_changes":[2.4864902E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.3022559E-3,2.5981911E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0121799E3,1.0648346E3,9.473453E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0444792E-1,2.5987101E-3,-1.3025309E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":8,"left_children":[1,-1,-1],"loss_changes":[2.539678E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5987101E-3,-1.3025309E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.054429E3,9.682887E2,1.0861403E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.040976E-1,2.4336856E-3,-7.3614573E-1,-1.3037194E-3,-6.265099E-5],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":9,"left_children":[1,-1,3,-1,-1],"loss_changes":[5.34681E2,0E0,4.3182373E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[2E0,2.4336856E-3,1.45496E3,-1.3037194E-3,-6.265099E-5],"split_indices":[32,0,31,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0158502E3,1.2000182E2,1.8958485E3,1.8745293E3,2.1319159E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.9266128E-1,-1.3016465E-3,2.5914602E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":10,"left_children":[1,-1,-1],"loss_changes":[2.498004E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.3016465E-3,2.5914602E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0313292E3,1.0832126E3,9.481165E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8812113E-1,-1.3007602E-3,1.3389257E0,-1.0915692E-3,2.5911732E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":11,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.1936072E3,0E0,2.7575977E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1E0,-1.3007602E-3,1.7429978E9,-1.0915692E-3,2.5911732E-3],"split_indices":[3,0,6,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0161832E3,1.01800323E3,9.9818E2,6.1217564E1,9.3696246E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-6.0735476E-1,-1.3066872E-3,2.4330167E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":12,"left_children":[1,-1,-1],"loss_changes":[5.5294934E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.3066872E-3,2.4330167E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[2.018724E3,1.8950089E3,1.2371508E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9352802E-1,-1.3007496E-3,2.5847733E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":13,"left_children":[1,-1,-1],"loss_changes":[2.485272E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.3007496E-3,2.5847733E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.028276E3,1.0793975E3,9.4887854E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9688382E-1,2.5850034E-3,-1.3008006E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":14,"left_children":[1,-1,-1],"loss_changes":[2.5006965E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5850034E-3,-1.3008006E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0398734E3,9.573634E2,1.08251E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.983136E-1,2.4374255E-3,-1.3058123E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":15,"left_children":[1,-1,-1],"loss_changes":[5.8633496E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4374255E-3,-1.3058123E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0216797E3,1.3191344E2,1.8897662E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.951508E-1,-1.2998972E-3,2.5782213E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":16,"left_children":[1,-1,-1],"loss_changes":[2.4816353E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2998972E-3,2.5782213E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0323475E3,1.0786887E3,9.536588E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0654007E-1,2.5783938E-3,-1.2997143E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":17,"left_children":[1,-1,-1],"loss_changes":[2.4738423E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5783938E-3,-1.2997143E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0235554E3,9.599179E2,1.0636376E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.062833E-1,2.4205998E-3,-1.3049373E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":18,"left_children":[1,-1,-1],"loss_changes":[5.480008E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4205998E-3,-1.3049373E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0076512E3,1.2356657E2,1.8840847E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9589015E-1,-1.2990732E-3,2.5717178E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":19,"left_children":[1,-1,-1],"loss_changes":[2.481015E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2990732E-3,2.5717178E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.0390798E3,1.0801912E3,9.588886E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0333686E-1,2.5717996E-3,-1.2989381E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":20,"left_children":[1,-1,-1],"loss_changes":[2.472174E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5717996E-3,-1.2989381E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0302649E3,9.6157294E2,1.068692E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9802985E-1,-1.3040853E-3,2.4245E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":21,"left_children":[1,-1,-1],"loss_changes":[5.7938556E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.3040853E-3,2.4245E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[2.0150485E3,1.8837135E3,1.3133493E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8726912E-1,-1.2981278E-3,2.5646351E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":22,"left_children":[1,-1,-1],"loss_changes":[2.4354766E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2981278E-3,2.5646351E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0114158E3,1.0719579E3,9.394579E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8620538E-1,2.564671E-3,-1.2981789E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":23,"left_children":[1,-1,-1],"loss_changes":[2.440082E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.564671E-3,-1.2981789E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0154165E3,9.403524E2,1.0750641E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.978865E-1,2.4185225E-3,-1.3032508E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":24,"left_children":[1,-1,-1],"loss_changes":[5.7796606E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4185225E-3,-1.3032508E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0183715E3,1.3149426E2,1.8868772E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9423544E-1,-1.2972517E-3,2.558381E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":25,"left_children":[1,-1,-1],"loss_changes":[2.4396318E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2972517E-3,2.558381E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0204357E3,1.0690382E3,9.513974E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0409217E-1,2.5586623E-3,-1.2971695E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":26,"left_children":[1,-1,-1],"loss_changes":[2.4466953E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5586623E-3,-1.2971695E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0241432E3,9.6217004E2,1.0619731E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9637415E-1,2.4127439E-3,-1.3023384E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":27,"left_children":[1,-1,-1],"loss_changes":[5.761769E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.4127439E-3,-1.3023384E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0031235E3,1.3164821E2,1.8714753E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9307505E-1,-1.2966368E-3,2.552254E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":28,"left_children":[1,-1,-1],"loss_changes":[2.4711108E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2966368E-3,2.552254E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0538452E3,1.086443E3,9.674023E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9639134E-1,2.5521056E-3,-1.2964711E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":29,"left_children":[1,-1,-1],"loss_changes":[2.446613E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5521056E-3,-1.2964711E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0329739E3,9.606565E2,1.0723173E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9754395E-1,2.40584E-3,-1.301529E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":30,"left_children":[1,-1,-1],"loss_changes":[5.710939E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.40584E-3,-1.301529E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0108347E3,1.309035E2,1.8799313E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8240708E-1,-1.2959549E-3,2.5456478E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":31,"left_children":[1,-1,-1],"loss_changes":[2.4679922E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2959549E-3,2.5456478E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0612466E3,1.0989642E3,9.622823E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0119276E-1,2.5458357E-3,-1.2955986E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":32,"left_children":[1,-1,-1],"loss_changes":[2.4465605E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.5458357E-3,-1.2955986E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.039315E3,9.6948096E2,1.069834E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.938964E-1,2.403219E-3,-1.3006568E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":33,"left_children":[1,-1,-1],"loss_changes":[5.80883E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.403219E-3,-1.3006568E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0066915E3,1.337597E2,1.8729319E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0202582E-1,-1.2946972E-3,2.5393823E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":34,"left_children":[1,-1,-1],"loss_changes":[2.4329844E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2946972E-3,2.5393823E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0353376E3,1.0655802E3,9.6975745E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.013728E-1,2.5391555E-3,-1.2945844E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":35,"left_children":[1,-1,-1],"loss_changes":[2.409144E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5391555E-3,-1.2945844E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0157274E3,9.598414E2,1.0558861E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.0371417E-1,2.3886214E-3,-1.2999834E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":36,"left_children":[1,-1,-1],"loss_changes":[5.498578E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3886214E-3,-1.2999834E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0398162E3,1.2670141E2,1.9131147E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8765088E-1,-1.2940272E-3,2.5326977E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":37,"left_children":[1,-1,-1],"loss_changes":[2.421859E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2940272E-3,2.5326977E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0363875E3,1.0780962E3,9.5829126E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.019921E-1,-1.2928575E-3,1.3112736E0,-1.0853208E-3,2.5328442E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":38,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.129888E3,0E0,2.6723523E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1E0,-1.2928575E-3,1.7429978E9,-1.0853208E-3,2.5328442E-3],"split_indices":[3,0,6,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0197627E3,9.9474695E2,1.0250159E3,6.1313187E1,9.6370264E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.977051E-1,2.3756505E-3,-7.2875476E-1,-1.2914903E-3,-2.7669064E-4],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":39,"left_children":[1,-1,3,-1,-1],"loss_changes":[5.1999786E2,0E0,2.3757324E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.08E2,2.3756505E-3,9.98E2,-1.2914903E-3,-2.7669064E-4],"split_indices":[4,0,4,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.015397E3,1.21431015E2,1.893966E3,1.8651981E3,2.8767864E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.8266805E-1,-1.2928804E-3,2.5256427E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":40,"left_children":[1,-1,-1],"loss_changes":[2.3501404E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2928804E-3,2.5256427E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9849962E3,1.0540186E3,9.3097766E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9856953E-1,2.5264986E-3,-1.2929963E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":41,"left_children":[1,-1,-1],"loss_changes":[2.403343E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5264986E-3,-1.2929963E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0259413E3,9.6485376E2,1.0610875E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.0543823E-1,2.3729645E-3,-1.2982432E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":42,"left_children":[1,-1,-1],"loss_changes":[5.316523E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3729645E-3,-1.2982432E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0193726E3,1.23365204E2,1.8960074E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7610043E-1,-1.2923408E-3,2.5195875E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":43,"left_children":[1,-1,-1],"loss_changes":[2.3789849E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2923408E-3,2.5195875E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.017784E3,1.0762072E3,9.4157684E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0432767E-1,2.5201675E-3,-1.2919673E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":44,"left_children":[1,-1,-1],"loss_changes":[2.3762932E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5201675E-3,-1.2919673E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.009577E3,9.637305E2,1.0458464E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.926997E-1,2.3802428E-3,-1.2972988E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":45,"left_children":[1,-1,-1],"loss_changes":[5.732129E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3802428E-3,-1.2972988E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0044283E3,1.339115E2,1.8705168E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7701393E-1,-1.2915783E-3,2.5135092E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":46,"left_children":[1,-1,-1],"loss_changes":[2.3877368E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2915783E-3,2.5135092E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0320206E3,1.0816392E3,9.503816E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9122904E-1,2.5140422E-3,-1.291547E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":47,"left_children":[1,-1,-1],"loss_changes":[2.4107285E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.5140422E-3,-1.291547E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.048029E3,9.7120386E2,1.0768251E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.988171E-1,2.3692937E-3,-1.2965751E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":48,"left_children":[1,-1,-1],"loss_changes":[5.535778E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3692937E-3,-1.2965751E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0264763E3,1.2953174E2,1.8969446E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.850253E-1,-1.2908989E-3,2.507892E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":49,"left_children":[1,-1,-1],"loss_changes":[2.4272043E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2908989E-3,2.507892E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0705522E3,1.0932225E3,9.7732983E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0055365E-1,2.508037E-3,-1.2906038E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":50,"left_children":[1,-1,-1],"loss_changes":[2.406536E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.508037E-3,-1.2906038E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0500532E3,9.8231067E2,1.0677424E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.0393673E-1,2.355465E-3,-1.295687E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":51,"left_children":[1,-1,-1],"loss_changes":[5.2554364E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.355465E-3,-1.295687E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0071166E3,1.23331474E2,1.883785E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9419658E-1,-1.2861784E-3,1.3971428E0,1.577024E-4,2.4993522E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":52,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.3053157E3,0E0,2.5315674E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2861784E-3,4.52E2,1.577024E-4,2.4993522E-3],"split_indices":[24,0,33,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0230558E3,1.0472463E3,9.7580945E2,2.1867914E1,9.539415E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[3.0435166E-1,2.5017452E-3,-1.2896062E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":53,"left_children":[1,-1,-1],"loss_changes":[2.3802026E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.5017452E-3,-1.2896062E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0345327E3,9.798196E2,1.0547131E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.952347E-1,2.3514454E-3,-7.280404E-1,-1.2911648E-3,-0E0],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":54,"left_children":[1,-1,3,-1,-1],"loss_changes":[5.25721E2,0E0,7.350891E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[2E0,2.3514454E-3,1.45496E3,-1.2911648E-3,-0E0],"split_indices":[32,0,31,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0277765E3,1.248373E2,1.9029392E3,1.882109E3,2.0830206E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.912869E-1,-1.2888256E-3,2.4951822E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":55,"left_children":[1,-1,-1],"loss_changes":[2.3538525E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2888256E-3,2.4951822E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0215844E3,1.0588175E3,9.6276685E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.1171894E-1,2.4955408E-3,-1.2885457E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":56,"left_children":[1,-1,-1],"loss_changes":[2.3509197E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4955408E-3,-1.2885457E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.016002E3,9.7912823E2,1.0368737E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-6.0280883E-1,2.3466484E-3,-1.2940947E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":57,"left_children":[1,-1,-1],"loss_changes":[5.2893176E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3466484E-3,-1.2940947E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0232872E3,1.2495693E2,1.8983303E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8895906E-1,-1.2880241E-3,2.489013E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":58,"left_children":[1,-1,-1],"loss_changes":[2.3463928E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2880241E-3,2.489013E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0227937E3,1.0602914E3,9.625022E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8355807E-1,-1.2870878E-3,1.2849402E0,2.4852355E-3,-1.0793898E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":59,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.056471E3,0E0,2.583855E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1E0,-1.2870878E-3,2E0,2.4852355E-3,-1.0793898E-3],"split_indices":[3,0,29,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0074589E3,9.953204E2,1.0121385E3,9.510993E2,6.1039177E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.9125334E-1,2.352258E-3,-1.2931438E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":60,"left_children":[1,-1,-1],"loss_changes":[5.6429877E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.352258E-3,-1.2931438E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0035868E3,1.341908E2,1.869396E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9371646E-1,-1.2874057E-3,2.4834785E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":61,"left_children":[1,-1,-1],"loss_changes":[2.3867686E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2874057E-3,2.4834785E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0635222E3,1.0757947E3,9.8772754E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0435753E-1,2.4833125E-3,-1.287056E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":62,"left_children":[1,-1,-1],"loss_changes":[2.3463115E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.4833125E-3,-1.287056E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.027318E3,9.804414E2,1.0468766E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9757996E-1,2.3380583E-3,-1.2922802E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":63,"left_children":[1,-1,-1],"loss_changes":[5.330286E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3380583E-3,-1.2922802E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9872727E3,1.2705051E2,1.8602222E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8890702E-1,-1.2863287E-3,2.4768305E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":64,"left_children":[1,-1,-1],"loss_changes":[2.3265298E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2863287E-3,2.4768305E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.019975E3,1.0562107E3,9.6376434E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.847915E-1,2.4768056E-3,-1.2864475E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":65,"left_children":[1,-1,-1],"loss_changes":[2.331774E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.4768056E-3,-1.2864475E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0252083E3,9.623953E2,1.0628129E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9646726E-1,2.3363936E-3,-1.2916008E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":66,"left_children":[1,-1,-1],"loss_changes":[5.4536743E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3363936E-3,-1.2916008E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.025226E3,1.3038242E2,1.8948436E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9111388E-1,-1.2857567E-3,2.4713196E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":67,"left_children":[1,-1,-1],"loss_changes":[2.3675874E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2857567E-3,2.4713196E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0619626E3,1.0747592E3,9.8720337E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8383967E-1,2.4709504E-3,-1.2857482E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":68,"left_children":[1,-1,-1],"loss_changes":[2.3439692E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.4709504E-3,-1.2857482E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0429504E3,9.7122955E2,1.071721E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.917272E-1,-1.2907339E-3,2.3353354E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":69,"left_children":[1,-1,-1],"loss_changes":[5.5858636E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2907339E-3,2.3353354E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[2.0185111E3,1.8843412E3,1.3417E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.03592E-1,-1.2845639E-3,2.4652788E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":70,"left_children":[1,-1,-1],"loss_changes":[2.327681E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2845639E-3,2.4652788E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0329128E3,1.0464436E3,9.8646924E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.919474E-1,2.4651599E-3,-1.284827E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":71,"left_children":[1,-1,-1],"loss_changes":[2.3402805E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4651599E-3,-1.284827E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0454208E3,9.814441E2,1.0639767E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.9818697E-1,2.3205734E-3,-1.2898578E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":72,"left_children":[1,-1,-1],"loss_changes":[5.255838E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3205734E-3,-1.2898578E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9977747E3,1.2654825E2,1.8712263E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.905455E-1,-1.2840765E-3,2.4593147E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":73,"left_children":[1,-1,-1],"loss_changes":[2.3465342E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2840765E-3,2.4593147E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0582107E3,1.0706575E3,9.875532E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9099128E-1,2.458945E-3,-1.2838632E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":74,"left_children":[1,-1,-1],"loss_changes":[2.3070012E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.458945E-3,-1.2838632E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0238575E3,9.715444E2,1.0523131E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.839181E-1,2.330975E-3,-1.2890234E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":75,"left_children":[1,-1,-1],"loss_changes":[5.791986E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.330975E-3,-1.2890234E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.008573E3,1.4039969E2,1.8681733E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.91622E-1,-1.283119E-3,2.4532308E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":76,"left_children":[1,-1,-1],"loss_changes":[2.3189429E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.283119E-3,2.4532308E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0411906E3,1.0594296E3,9.817609E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8307688E-1,2.4530196E-3,-1.2832397E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":77,"left_children":[1,-1,-1],"loss_changes":[2.3148435E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4530196E-3,-1.2832397E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0390515E3,9.726029E2,1.0664486E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8411074E-1,-1.2881844E-3,2.3244857E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":78,"left_children":[1,-1,-1],"loss_changes":[5.7398334E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2881844E-3,2.3244857E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[2.0030115E3,1.8633755E3,1.3963597E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8565967E-1,-1.2822505E-3,2.4470182E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":79,"left_children":[1,-1,-1],"loss_changes":[2.290864E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2822505E-3,2.4470182E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0247312E3,1.0552001E3,9.695311E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.949824E-1,2.4475933E-3,-1.2823932E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":80,"left_children":[1,-1,-1],"loss_changes":[2.3314973E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4475933E-3,-1.2823932E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0587031E3,9.94743E2,1.0639601E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8956E-1,2.3118637E-3,-1.2873068E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":81,"left_children":[1,-1,-1],"loss_changes":[5.450513E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3118637E-3,-1.2873068E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9823162E3,1.3290096E2,1.8494152E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8349212E-1,-1.2815231E-3,2.4412288E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":82,"left_children":[1,-1,-1],"loss_changes":[2.2944749E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2815231E-3,2.4412288E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0352343E3,1.0614501E3,9.737842E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8840825E-1,-1.2806973E-3,1.2619413E0,-1.0811267E-3,2.4415648E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":83,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.049797E3,0E0,2.612484E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1E0,-1.2806973E-3,1.7429978E9,-1.0811267E-3,2.4415648E-3],"split_indices":[3,0,6,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0560588E3,1.0042471E3,1.0518119E3,6.334337E1,9.884685E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.898345E-1,2.3075724E-3,-1.2865831E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":84,"left_children":[1,-1,-1],"loss_changes":[5.4778827E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3075724E-3,-1.2865831E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0043176E3,1.3395685E2,1.8703608E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.952806E-1,-1.2804557E-3,2.4355215E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":85,"left_children":[1,-1,-1],"loss_changes":[2.2732861E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2804557E-3,2.4355215E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0218307E3,1.0419578E3,9.79873E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8444913E-1,2.4354276E-3,-1.2807237E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":86,"left_children":[1,-1,-1],"loss_changes":[2.2872173E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.4354276E-3,-1.2807237E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0356342E3,9.761997E2,1.0594344E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.911119E-1,2.3000492E-3,-1.2857559E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":87,"left_children":[1,-1,-1],"loss_changes":[5.390906E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.3000492E-3,-1.2857559E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.999996E3,1.322616E2,1.8677344E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7666846E-1,-1.279906E-3,2.4293591E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":88,"left_children":[1,-1,-1],"loss_changes":[2.2674646E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.279906E-3,2.4293591E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0265823E3,1.060847E3,9.657353E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8238449E-1,2.4296595E-3,-1.279972E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":89,"left_children":[1,-1,-1],"loss_changes":[2.2862625E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4296595E-3,-1.279972E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0421019E3,9.78611E2,1.063491E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8670056E-1,2.2994545E-3,-1.2849463E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":90,"left_children":[1,-1,-1],"loss_changes":[5.5405225E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2994545E-3,-1.2849463E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.004694E3,1.3653297E2,1.868161E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.90422E-1,-1.2791773E-3,2.4243225E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":91,"left_children":[1,-1,-1],"loss_changes":[2.3062993E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2791773E-3,2.4243225E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.06561E3,1.0666383E3,9.989718E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.863034E-1,2.42403E-3,-1.2791458E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":92,"left_children":[1,-1,-1],"loss_changes":[2.2847485E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.42403E-3,-1.2791458E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0470985E3,9.860834E2,1.0610151E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.936173E-1,2.2855182E-3,-1.2841087E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":93,"left_children":[1,-1,-1],"loss_changes":[5.2189435E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2855182E-3,-1.2841087E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.990463E3,1.2884245E2,1.8616206E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7390373E-1,-1.2782888E-3,2.4177942E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":94,"left_children":[1,-1,-1],"loss_changes":[2.2498523E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2782888E-3,2.4177942E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0252192E3,1.060231E3,9.649883E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9322407E-1,2.4185085E-3,-1.2782713E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":95,"left_children":[1,-1,-1],"loss_changes":[2.2842412E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4185085E-3,-1.2782713E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0527014E3,9.967864E2,1.0559149E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.885733E-1,-1.2832106E-3,2.283928E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":96,"left_children":[1,-1,-1],"loss_changes":[5.327783E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2832106E-3,2.283928E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9733687E3,1.8411672E3,1.3220139E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.846265E-1,-1.2774979E-3,2.4125935E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":97,"left_children":[1,-1,-1],"loss_changes":[2.2712673E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2774979E-3,2.4125935E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0493506E3,1.0612234E3,9.8812714E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8088787E-1,2.4126375E-3,-1.277672E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":98,"left_children":[1,-1,-1],"loss_changes":[2.2845193E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.4126375E-3,-1.277672E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0617026E3,9.904349E2,1.0712677E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8388835E-1,-1.2823394E-3,2.282427E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":99,"left_children":[1,-1,-1],"loss_changes":[5.436976E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2823394E-3,2.282427E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9619512E3,1.8263917E3,1.355595E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8712887E-1,-1.2763999E-3,2.4065725E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":100,"left_children":[1,-1,-1],"loss_changes":[2.2233398E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2763999E-3,2.4065725E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0131119E3,1.0387474E3,9.743645E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.902609E-1,2.4070654E-3,-1.2766746E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":101,"left_children":[1,-1,-1],"loss_changes":[2.2681206E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.4070654E-3,-1.2766746E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0527E3,9.965304E2,1.0561696E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.915661E-1,2.2699963E-3,-1.2816271E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":102,"left_children":[1,-1,-1],"loss_changes":[5.177811E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2699963E-3,-1.2816271E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9769016E3,1.2922968E2,1.8476719E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8044474E-1,-1.2756911E-3,2.4008132E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":103,"left_children":[1,-1,-1],"loss_changes":[2.2170671E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2756911E-3,2.4008132E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0152373E3,1.0449675E3,9.702698E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[3.0860907E-1,2.401567E-3,-1.2754579E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":104,"left_children":[1,-1,-1],"loss_changes":[2.2366772E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.401567E-3,-1.2754579E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0299326E3,1.00447723E3,1.0254554E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.806049E-1,2.276639E-3,-1.2808153E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":105,"left_children":[1,-1,-1],"loss_changes":[5.574992E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.276639E-3,-1.2808153E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9863335E3,1.3998236E2,1.8463511E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8032786E-1,-1.2751577E-3,2.3956243E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":106,"left_children":[1,-1,-1],"loss_changes":[2.2540044E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2751577E-3,2.3956243E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.055313E3,1.0646289E3,9.9068414E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8757018E-1,2.3952373E-3,-1.2747516E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":107,"left_children":[1,-1,-1],"loss_changes":[2.1985554E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3952373E-3,-1.2747516E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0043485E3,9.7309955E2,1.0312489E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.840421E-1,2.2669476E-3,-1.2799967E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":108,"left_children":[1,-1,-1],"loss_changes":[5.4004065E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2669476E-3,-1.2799967E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9779625E3,1.3596228E2,1.8420002E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8574616E-1,-1.2741096E-3,2.3899109E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":109,"left_children":[1,-1,-1],"loss_changes":[2.220614E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2741096E-3,2.3899109E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0313097E3,1.0456567E3,9.8565295E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.978726E-1,2.3902226E-3,-1.2740218E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":110,"left_children":[1,-1,-1],"loss_changes":[2.226915E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3902226E-3,-1.2740218E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0357007E3,9.99547E2,1.0361537E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.863686E-1,-1.2792181E-3,2.2593427E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":111,"left_children":[1,-1,-1],"loss_changes":[5.299681E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2792181E-3,2.2593427E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9809591E3,1.8471799E3,1.3377917E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7522972E-1,-1.2727903E-3,1.2610974E0,2.3842321E-3,-1.0093059E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":112,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.0232788E3,0E0,1.8929504E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.7E1,-1.2727903E-3,5E0,2.3842321E-3,-1.0093059E-3],"split_indices":[34,0,2,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0393876E3,1.0114972E3,1.0278904E3,9.806018E2,4.7288586E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.856021E-1,2.3844596E-3,-1.2733967E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":113,"left_children":[1,-1,-1],"loss_changes":[2.2232886E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3844596E-3,-1.2733967E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0405447E3,9.912671E2,1.0492776E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8449686E-1,2.2555133E-3,-1.2783817E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":114,"left_children":[1,-1,-1],"loss_changes":[5.320789E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2555133E-3,-1.2783817E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9737529E3,1.3482948E2,1.8389235E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.800916E-1,-1.272527E-3,2.3786705E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":115,"left_children":[1,-1,-1],"loss_changes":[2.1990527E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.272527E-3,2.3786705E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0260769E3,1.045901E3,9.801759E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8758216E-1,2.3787897E-3,-1.2724404E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":116,"left_children":[1,-1,-1],"loss_changes":[2.1962615E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3787897E-3,-1.2724404E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0225862E3,9.8574585E2,1.0368405E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.862277E-1,2.2490122E-3,-1.2776286E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":117,"left_children":[1,-1,-1],"loss_changes":[5.2570087E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2490122E-3,-1.2776286E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9819875E3,1.335653E2,1.8484222E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7575472E-1,-1.2717913E-3,2.3730919E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":118,"left_children":[1,-1,-1],"loss_changes":[2.191442E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2717913E-3,2.3730919E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0264774E3,1.0490608E3,9.774166E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.80828E-1,2.3731969E-3,-1.2717497E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":119,"left_children":[1,-1,-1],"loss_changes":[2.1926353E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3731969E-3,-1.2717497E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0268494E3,9.825283E2,1.044321E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.803543E-1,2.2517373E-3,-1.2769293E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":120,"left_children":[1,-1,-1],"loss_changes":[5.5303845E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2517373E-3,-1.2769293E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0124216E3,1.4112218E2,1.8712994E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.817635E-1,-1.2709936E-3,2.3678888E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":121,"left_children":[1,-1,-1],"loss_changes":[2.1983853E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2709936E-3,2.3678888E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0387653E3,1.048311E3,9.9045416E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.822014E-1,2.3678248E-3,-1.2709685E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":122,"left_children":[1,-1,-1],"loss_changes":[2.191204E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3678248E-3,-1.2709685E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0321088E3,9.876646E2,1.0444441E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.884379E-1,2.2231424E-3,-7.147082E-1,-1.2667737E-3,-4.4268632E-5],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":123,"left_children":[1,-1,3,-1,-1],"loss_changes":[4.7014185E2,0E0,4.799011E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.08E2,2.2231424E-3,1.45496E3,-1.2667737E-3,-4.4268632E-5],"split_indices":[31,0,31,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[1.988481E3,1.21286835E2,1.8671941E3,1.8447614E3,2.243274E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.949182E-1,-1.2700935E-3,2.362846E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":124,"left_children":[1,-1,-1],"loss_changes":[2.2043555E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2700935E-3,2.362846E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.0498064E3,1.0397823E3,1.0100241E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8360176E-1,2.3624215E-3,-1.2701416E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":125,"left_children":[1,-1,-1],"loss_changes":[2.183412E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3624215E-3,-1.2701416E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0315432E3,9.900093E2,1.0415339E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.836952E-1,2.2347628E-3,-1.2751884E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":126,"left_children":[1,-1,-1],"loss_changes":[5.233626E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2347628E-3,-1.2751884E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9684668E3,1.3440332E2,1.8340635E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9297465E-1,-1.269388E-3,2.3575223E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":127,"left_children":[1,-1,-1],"loss_changes":[2.2085488E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.269388E-3,2.3575223E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0606536E3,1.0459413E3,1.0147123E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7708122E-1,2.3571234E-3,-1.2695711E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":128,"left_children":[1,-1,-1],"loss_changes":[2.1983098E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3571234E-3,-1.2695711E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0527705E3,9.9514233E2,1.0576283E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8103335E-1,2.2335094E-3,-1.2744487E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":129,"left_children":[1,-1,-1],"loss_changes":[5.3460815E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2335094E-3,-1.2744487E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.983457E3,1.3778122E2,1.8456758E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8604412E-1,-1.2685527E-3,2.351804E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":130,"left_children":[1,-1,-1],"loss_changes":[2.1789307E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2685527E-3,2.351804E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0406636E3,1.0413147E3,9.993489E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9893088E-1,2.351953E-3,-1.268309E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":131,"left_children":[1,-1,-1],"loss_changes":[2.166441E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.351953E-3,-1.268309E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0282198E3,1.0058757E3,1.0223441E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.841381E-1,2.2245587E-3,-1.2736481E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":132,"left_children":[1,-1,-1],"loss_changes":[5.1930457E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2245587E-3,-1.2736481E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9772219E3,1.34182E2,1.8430399E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.9186785E-1,-1.2677453E-3,2.3466945E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":133,"left_children":[1,-1,-1],"loss_changes":[2.1844182E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2677453E-3,2.3466945E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0520786E3,1.0401232E3,1.01195544E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8043884E-1,2.3462973E-3,-1.2678247E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":134,"left_children":[1,-1,-1],"loss_changes":[2.1660745E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3462973E-3,-1.2678247E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0359948E3,9.928165E2,1.0431782E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.86072E-1,2.2201757E-3,-1.2730203E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":135,"left_children":[1,-1,-1],"loss_changes":[5.2057245E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2201757E-3,-1.2730203E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0156677E3,1.3477098E2,1.8808967E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.846561E-1,-1.2670379E-3,2.3412246E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":136,"left_children":[1,-1,-1],"loss_changes":[2.1761436E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2670379E-3,2.3412246E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.051719E3,1.0458308E3,1.00588824E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8464392E-1,2.3411696E-3,-1.2670297E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":137,"left_children":[1,-1,-1],"loss_changes":[2.1690269E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3411696E-3,-1.2670297E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0450353E3,1.0026154E3,1.0424199E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7525235E-1,2.2232975E-3,-1.2720692E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":138,"left_children":[1,-1,-1],"loss_changes":[5.4787006E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.2232975E-3,-1.2720692E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9849025E3,1.4283188E2,1.8420707E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7597278E-1,-1.2662545E-3,2.3355717E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":139,"left_children":[1,-1,-1],"loss_changes":[2.1502095E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2662545E-3,2.3355717E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.035103E3,1.0446528E3,9.904502E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.811114E-1,2.335751E-3,-1.2662574E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":140,"left_children":[1,-1,-1],"loss_changes":[2.156138E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.335751E-3,-1.2662574E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0400345E3,9.979291E2,1.0421055E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.832811E-1,-1.2712975E-3,2.209877E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":141,"left_children":[1,-1,-1],"loss_changes":[5.155683E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2712975E-3,2.209877E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9795724E3,1.845023E3,1.345494E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.754782E-1,-1.265454E-3,2.3302832E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":142,"left_children":[1,-1,-1],"loss_changes":[2.1403682E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.265454E-3,2.3302832E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0325576E3,1.0426075E3,9.899501E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8369167E-1,1.2519392E0,-1.2582688E-3,2.3305856E-3,-9.4151223E-4],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":143,"left_children":[1,3,-1,-1,-1],"loss_changes":[1.9963575E3,1.4654321E2,0E0,0E0,0E0],"parents":[2147483647,0,0,1,1],"right_children":[2,4,-1,-1,-1],"split_conditions":[4.9965545E-3,1.742998E9,-1.2582688E-3,2.3305856E-3,-9.4151223E-4],"split_indices":[36,6,0,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.048495E3,1.0418517E3,1.00664343E3,1.00399146E3,3.7860207E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.9224606E-1,2.192356E-3,-1.2704368E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":144,"left_children":[1,-1,-1],"loss_changes":[4.7403387E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.192356E-3,-1.2704368E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.951677E3,1.2391677E2,1.8277601E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.726686E-1,-1.2628589E-3,1.2940837E0,2.3247628E-3,3.636746E-5],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":145,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.0743298E3,0E0,3.1413086E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2628589E-3,9.9122174E-2,2.3247628E-3,3.636746E-5],"split_indices":[24,0,36,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0347245E3,1.0315375E3,1.00318695E3,9.7772943E2,2.5457506E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.721592E-1,2.3249448E-3,-1.2647115E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":146,"left_children":[1,-1,-1],"loss_changes":[2.1281545E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3249448E-3,-1.2647115E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0280073E3,9.856924E2,1.042315E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8560866E-1,2.187894E-3,-1.2585361E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":147,"left_children":[1,-1,-1],"loss_changes":[4.7156396E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[2E0,2.187894E-3,-1.2585361E-3],"split_indices":[32,0,0],"split_type":[0,0,0],"sum_hessian":[1.9535138E3,1.2450113E2,1.8290127E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6911494E-1,-1.2604579E-3,1.2935517E0,2.3194263E-3,2.0570145E-5],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":148,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.0641047E3,0E0,2.8304688E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2604579E-3,9.9122174E-2,2.3194263E-3,2.0570145E-5],"split_indices":[24,0,36,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0287448E3,1.0325358E3,9.96209E2,9.729818E2,2.3227167E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.8831995E-1,2.3201536E-3,-1.2637406E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":149,"left_children":[1,-1,-1],"loss_changes":[2.1299812E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.3201536E-3,-1.2637406E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0349274E3,1.0062725E3,1.0286549E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.724981E-1,2.1956938E-3,-7.121244E-1,-1.2622672E-3,-3.263214E-4],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":150,"left_children":[1,-1,3,-1,-1],"loss_changes":[5.0833588E2,0E0,1.5385742E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.08E2,2.1956938E-3,9.98E2,-1.2622672E-3,-3.263214E-4],"split_indices":[4,0,4,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[1.9792173E3,1.3538295E2,1.8438344E3,1.8128887E3,3.0945583E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.962503E-1,-1.2629392E-3,2.3153916E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":151,"left_children":[1,-1,-1],"loss_changes":[2.1407998E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2629392E-3,2.3153916E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0514658E3,1.027885E3,1.0235808E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7906957E-1,2.3150016E-3,-1.2632307E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":152,"left_children":[1,-1,-1],"loss_changes":[2.1434397E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3150016E-3,-1.2632307E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.054916E3,1.0081061E3,1.04681E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7948464E-1,2.1940512E-3,-1.268176E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":153,"left_children":[1,-1,-1],"loss_changes":[5.193064E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1940512E-3,-1.268176E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9795382E3,1.3734755E2,1.8421907E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7675864E-1,-1.2624678E-3,2.3098793E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":154,"left_children":[1,-1,-1],"loss_changes":[2.1385283E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2624678E-3,2.3098793E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.057073E3,1.0490247E3,1.00804834E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8051072E-1,2.3100513E-3,-1.2625239E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":155,"left_children":[1,-1,-1],"loss_changes":[2.1528984E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3100513E-3,-1.2625239E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0704338E3,1.0183894E3,1.0520446E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7616025E-1,2.1799458E-3,-7.0789325E-1,-7.139194E-1,-7.161693E-7,-1.7155302E-4,-1.2654614E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0,0,0],"id":156,"left_children":[1,-1,3,5,-1,-1,-1],"loss_changes":[4.8437555E2,0E0,5.157715E0,3.3493042E0,0E0,0E0,0E0],"parents":[2147483647,0,0,2,2,3,3],"right_children":[2,-1,4,6,-1,-1,-1],"split_conditions":[1.08E2,2.1799458E-3,1.45496E3,4.78E2,-7.161693E-7,-1.7155302E-4,-1.2654614E-3],"split_indices":[31,0,31,33,0,0,0],"split_type":[0,0,0,0,0,0,0],"sum_hessian":[2.0044077E3,1.2998778E2,1.8744199E3,1.8529266E3,2.149333E1,2.576794E1,1.8271587E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"7","size_leaf_vector":"1"}},{"base_weights":[2.7858937E-1,-1.2606937E-3,1.2204845E0,-9.850053E-4,2.2968282E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":157,"left_children":[1,-1,3,-1,-1],"loss_changes":[1.917046E3,0E0,1.6923901E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.7E1,-1.2606937E-3,2E0,-9.850053E-4,2.2968282E-3],"split_indices":[34,0,32,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0296029E3,9.851389E2,1.044464E3,4.4660015E1,9.9980396E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.941132E-1,2.3052308E-3,-1.2615785E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":158,"left_children":[1,-1,-1],"loss_changes":[2.1494377E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.3052308E-3,-1.2615785E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0732136E3,1.0347969E3,1.0384169E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.878894E-1,2.1760368E-3,-1.2666926E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":159,"left_children":[1,-1,-1],"loss_changes":[4.864254E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1760368E-3,-1.2666926E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9844432E3,1.2911543E2,1.8553278E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6932302E-1,-1.260966E-3,2.299486E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":160,"left_children":[1,-1,-1],"loss_changes":[2.1183523E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.260966E-3,2.299486E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0517354E3,1.0513438E3,1.00039154E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7501148E-1,2.2993796E-3,-1.2607917E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":161,"left_children":[1,-1,-1],"loss_changes":[2.1025945E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2993796E-3,-1.2607917E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0361655E3,9.9852075E2,1.0376448E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.851167E-1,2.1697513E-3,-1.2656801E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":162,"left_children":[1,-1,-1],"loss_changes":[4.790954E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1697513E-3,-1.2656801E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9291215E3,1.2781475E2,1.8013068E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.73337E-1,-1.2600729E-3,2.294419E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":163,"left_children":[1,-1,-1],"loss_changes":[2.103582E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2600729E-3,2.294419E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0437294E3,1.0420048E3,1.00172455E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7838123E-1,2.2947222E-3,-1.2601842E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":164,"left_children":[1,-1,-1],"loss_changes":[2.13017E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2947222E-3,-1.2601842E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0688787E3,1.01915936E3,1.0497192E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7572144E-1,-1.2649173E-3,2.1756017E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":165,"left_children":[1,-1,-1],"loss_changes":[5.11987E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2649173E-3,2.1756017E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9407117E3,1.8033853E3,1.3732642E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8393257E-1,-1.2568705E-3,1.095427E0,2.2892868E-3,-1.152092E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":166,"left_children":[1,-1,3,-1,-1],"loss_changes":[1.644863E3,0E0,4.1413867E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.3E1,-1.2568705E-3,6.0002792E1,2.2892868E-3,-1.152092E-3],"split_indices":[0,0,36,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0148318E3,9.011392E2,1.1136926E3,9.99283E2,1.1440957E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.8282073E-1,2.2895364E-3,-1.2592021E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":167,"left_children":[1,-1,-1],"loss_changes":[2.100387E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2895364E-3,-1.2592021E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0466045E3,1.0139042E3,1.0327004E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.8246833E-1,1.0655017E0,-1.2488674E-3,2.1349594E-3,-0E0],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":168,"left_children":[1,3,-1,-1,-1],"loss_changes":[4.2490918E2,2.0281937E1,0E0,0E0,0E0],"parents":[2147483647,0,0,1,1],"right_children":[2,4,-1,-1,-1],"split_conditions":[4.52E2,1.6398605E-2,-1.2488674E-3,2.1349594E-3,-0E0],"split_indices":[33,35,0,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[1.9904529E3,1.3814462E2,1.8523082E3,1.1859936E2,1.954526E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.776418E-1,-1.2585893E-3,2.2846297E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":169,"left_children":[1,-1,-1],"loss_changes":[2.1105447E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2585893E-3,2.2846297E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0632244E3,1.045168E3,1.01805634E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8597596E-1,2.2844211E-3,-1.25827E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":170,"left_children":[1,-1,-1],"loss_changes":[2.07744E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2844211E-3,-1.25827E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0309135E3,1.0105175E3,1.02039594E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.77391E-1,2.1675988E-3,-1.2635726E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":171,"left_children":[1,-1,-1],"loss_changes":[5.1390936E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1675988E-3,-1.2635726E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9848776E3,1.3849037E2,1.8463872E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.66382E-1,-1.2579424E-3,2.279338E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":172,"left_children":[1,-1,-1],"loss_changes":[2.0953254E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2579424E-3,2.279338E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0559558E3,1.0516826E3,1.00427325E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7637702E-1,2.2792523E-3,-1.2576344E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":173,"left_children":[1,-1,-1],"loss_changes":[2.0699163E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2792523E-3,-1.2576344E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0305393E3,1.0019187E3,1.0286206E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.912138E-1,2.1455875E-3,-1.2627585E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":174,"left_children":[1,-1,-1],"loss_changes":[4.5657544E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1455875E-3,-1.2627585E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9582344E3,1.2304227E2,1.8351921E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.786114E-1,-1.2569771E-3,2.2746553E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":175,"left_children":[1,-1,-1],"loss_changes":[2.089579E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2569771E-3,2.2746553E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0559355E3,1.0380728E3,1.0178627E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7414787E-1,2.274274E-3,-1.2569049E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":176,"left_children":[1,-1,-1],"loss_changes":[2.0680173E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.274274E-3,-1.2569049E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0353346E3,1.0032277E3,1.0321068E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7534945E-1,2.1558134E-3,-1.261824E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":177,"left_children":[1,-1,-1],"loss_changes":[5.0197894E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1558134E-3,-1.261824E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9335099E3,1.3636334E2,1.7971466E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6867813E-1,-1.2561218E-3,2.2691519E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":178,"left_children":[1,-1,-1],"loss_changes":[2.0490557E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2561218E-3,2.2691519E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0236512E3,1.030482E3,9.931691E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.784897E-1,2.2694445E-3,-1.25609E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":179,"left_children":[1,-1,-1],"loss_changes":[2.0636055E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2694445E-3,-1.25609E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0372368E3,1.00973047E3,1.0275063E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.741635E-1,2.1533943E-3,-1.2611138E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":180,"left_children":[1,-1,-1],"loss_changes":[5.0778613E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1533943E-3,-1.2611138E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9482042E3,1.3835793E2,1.8098463E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7997112E-1,-1.2554722E-3,2.2648615E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":181,"left_children":[1,-1,-1],"loss_changes":[2.0825964E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2554722E-3,2.2648615E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.062144E3,1.0373973E3,1.0247468E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7366856E-1,2.2644233E-3,-1.2553901E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":182,"left_children":[1,-1,-1],"loss_changes":[2.0572363E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2644233E-3,-1.2553901E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.037729E3,1.0062983E3,1.0314307E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7115275E-1,2.1524136E-3,-1.2603812E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":183,"left_children":[1,-1,-1],"loss_changes":[5.1843176E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1524136E-3,-1.2603812E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9579166E3,1.417699E2,1.8161467E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.742813E-1,-1.2512921E-3,1.2594175E0,-0E0,2.2581853E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":184,"left_children":[1,-1,3,-1,-1],"loss_changes":[2.0158888E3,0E0,3.386194E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2512921E-3,2.05E2,-0E0,2.2581853E-3],"split_indices":[24,0,4,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0607192E3,1.0285741E3,1.0321451E3,2.0348951E1,1.0117962E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.711081E-1,2.2596265E-3,-1.2547586E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":185,"left_children":[1,-1,-1],"loss_changes":[2.0649658E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2596265E-3,-1.2547586E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0518857E3,1.01185046E3,1.0400354E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7248265E-1,-1.2596367E-3,2.1463386E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":186,"left_children":[1,-1,-1],"loss_changes":[5.1226697E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2596367E-3,2.1463386E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9603468E3,1.8198782E3,1.4046861E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8986275E-1,-1.253503E-3,2.254965E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":187,"left_children":[1,-1,-1],"loss_changes":[2.0310024E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.253503E-3,2.254965E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0243702E3,1.0060339E3,1.0183363E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7636376E-1,2.2547746E-3,-1.2538424E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":188,"left_children":[1,-1,-1],"loss_changes":[2.0488672E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2547746E-3,-1.2538424E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0422218E3,1.0136114E3,1.0286104E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.731431E-1,2.1399446E-3,-1.2588556E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":189,"left_children":[1,-1,-1],"loss_changes":[5.0585815E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1399446E-3,-1.2588556E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9538479E3,1.3917203E2,1.8146758E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7338034E-1,-1.2531531E-3,2.250023E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":190,"left_children":[1,-1,-1],"loss_changes":[2.0497324E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2531531E-3,2.250023E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.049556E3,1.0342013E3,1.0153547E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.749586E-1,2.2500507E-3,-1.2532079E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":191,"left_children":[1,-1,-1],"loss_changes":[2.059006E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2500507E-3,-1.2532079E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0587322E3,1.0215387E3,1.0371936E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7044834E-1,2.1374535E-3,-1.258036E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":192,"left_children":[1,-1,-1],"loss_changes":[5.094469E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1374535E-3,-1.258036E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9420884E3,1.4068779E2,1.8014005E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6432794E-1,-1.2488549E-3,1.247963E0,2.244522E-3,-0E0],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":193,"left_children":[1,-1,3,-1,-1],"loss_changes":[1.959167E3,0E0,3.599463E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2488549E-3,9.9122174E-2,2.244522E-3,-0E0],"split_indices":[24,0,36,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0289088E3,1.0177623E3,1.0111465E3,9.8720404E2,2.3942486E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.616361E-1,-1.2516228E-3,1.1683884E0,-1.038955E-3,2.2449093E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":194,"left_children":[1,-1,3,-1,-1],"loss_changes":[1.8234106E3,0E0,2.1255017E2,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1E0,-1.2516228E-3,1.7429978E9,-1.038955E-3,2.2449093E-3],"split_indices":[3,0,6,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.050996E3,9.874231E2,1.0635731E3,5.8326862E1,1.0052462E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.741961E-1,2.1302463E-3,-1.257334E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":195,"left_children":[1,-1,-1],"loss_changes":[4.970694E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1302463E-3,-1.257334E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9506554E3,1.3746556E2,1.8131898E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.764657E-1,-1.2480957E-3,1.2534814E0,2.240117E-3,1.6589041E-4],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":196,"left_children":[1,-1,3,-1,-1],"loss_changes":[1.9862668E3,0E0,1.922229E1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[5.028679E-3,-1.2480957E-3,9.9122174E-2,2.240117E-3,1.6589041E-4],"split_indices":[24,0,36,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0464554E3,1.01699774E3,1.0294576E3,1.00698505E3,2.2472622E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.7077767E-1,1.2067723E0,-1.2478904E-3,-9.1557356E-4,2.240295E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":197,"left_children":[1,3,-1,-1,-1],"loss_changes":[1.8988964E3,1.3146814E2,0E0,0E0,0E0],"parents":[2147483647,0,0,1,1],"right_children":[2,4,-1,-1,-1],"split_conditions":[4.9965545E-3,1E0,-1.2478904E-3,-9.1557356E-4,2.240295E-3],"split_indices":[36,3,0,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0543494E3,1.0525786E3,1.0017709E3,3.608691E1,1.01649164E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.801446E-1,2.1196564E-3,-1.2566305E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":198,"left_children":[1,-1,-1],"loss_changes":[4.762378E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1196564E-3,-1.2566305E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9564149E3,1.3188254E2,1.8245325E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.751949E-1,-1.25091E-3,2.2358655E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":199,"left_children":[1,-1,-1],"loss_changes":[2.0408319E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.25091E-3,2.2358655E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0598032E3,1.0339926E3,1.0258107E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7418542E-1,2.2357944E-3,-1.251005E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":200,"left_children":[1,-1,-1],"loss_changes":[2.0513486E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2357944E-3,-1.251005E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0704836E3,1.0301167E3,1.0403668E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.811744E-1,2.1141546E-3,-1.2559057E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":201,"left_children":[1,-1,-1],"loss_changes":[4.7190912E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1141546E-3,-1.2559057E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9618038E3,1.310353E2,1.8307684E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.716825E-1,-1.2500228E-3,2.23077E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":202,"left_children":[1,-1,-1],"loss_changes":[2.0053123E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2500228E-3,2.23077E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0307788E3,1.0217907E3,1.00898804E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8005663E-1,2.230706E-3,-1.2498403E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":203,"left_children":[1,-1,-1],"loss_changes":[1.9986431E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.230706E-3,-1.2498403E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0241571E3,1.0142513E3,1.0099058E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.699211E-1,2.1204185E-3,-1.2550643E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":204,"left_children":[1,-1,-1],"loss_changes":[5.0499915E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1204185E-3,-1.2550643E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9518365E3,1.4110426E2,1.8107323E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7192876E-1,-1.2494324E-3,2.2263322E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":205,"left_children":[1,-1,-1],"loss_changes":[2.0245995E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2494324E-3,2.2263322E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.0563855E3,1.0332969E3,1.0230886E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.674625E-1,2.2262805E-3,-1.2496447E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":206,"left_children":[1,-1,-1],"loss_changes":[2.0469266E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2262805E-3,-1.2496447E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0791792E3,1.029787E3,1.0493921E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7612395E-1,-1.2542368E-3,2.1075027E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":207,"left_children":[1,-1,-1],"loss_changes":[4.75565E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2542368E-3,2.1075027E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9276609E3,1.7945258E3,1.3313515E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.613526E-1,-1.2486341E-3,2.22115E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":208,"left_children":[1,-1,-1],"loss_changes":[1.9937202E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2486341E-3,2.22115E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.0322587E3,1.0307963E3,1.0014624E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7152646E-1,2.2215059E-3,-1.2487092E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":209,"left_children":[1,-1,-1],"loss_changes":[2.0247123E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2215059E-3,-1.2487092E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0631108E3,1.027228E3,1.0358828E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7476866E-1,2.0918208E-3,-7.0272964E-1,-1.2431191E-3,-7.8826655E-5],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":210,"left_children":[1,-1,3,-1,-1],"loss_changes":[4.3976233E2,0E0,2.3052368E0,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.08E2,2.0918208E-3,1.45496E3,-1.2431191E-3,-7.8826655E-5],"split_indices":[31,0,31,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[1.9269137E3,1.24703255E2,1.8022104E3,1.7831172E3,1.9093351E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.7827248E-1,-1.2477328E-3,2.2169726E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":211,"left_children":[1,-1,-1],"loss_changes":[2.0021418E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2477328E-3,2.2169726E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0464481E3,1.0194338E3,1.0270144E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6526368E-1,2.2163193E-3,-1.2477853E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":212,"left_children":[1,-1,-1],"loss_changes":[1.9797188E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2163193E-3,-1.2477853E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0242871E3,1.0027138E3,1.02157336E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7120365E-1,2.1049997E-3,-1.2528197E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":213,"left_children":[1,-1,-1],"loss_changes":[4.932859E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1049997E-3,-1.2528197E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9489884E3,1.3904028E2,1.8099481E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6791334E-1,-1.2470074E-3,2.2119177E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":214,"left_children":[1,-1,-1],"loss_changes":[1.9772537E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2470074E-3,2.2119177E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0277197E3,1.01951514E3,1.0082045E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.804341E-1,2.2122087E-3,-1.2469573E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":215,"left_children":[1,-1,-1],"loss_changes":[1.9951604E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.2122087E-3,-1.2469573E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0459023E3,1.0301869E3,1.01571545E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7098943E-1,2.1004109E-3,-1.2520531E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":216,"left_children":[1,-1,-1],"loss_changes":[4.9044348E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.1004109E-3,-1.2520531E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9438359E3,1.3866118E2,1.8051748E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7278683E-1,-1.2462637E-3,2.2075463E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":217,"left_children":[1,-1,-1],"loss_changes":[1.984199E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2462637E-3,2.2075463E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0408733E3,1.01999023E3,1.02088306E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.780555E-1,2.2075279E-3,-1.2462233E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":218,"left_children":[1,-1,-1],"loss_changes":[1.9873777E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2075279E-3,-1.2462233E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0442782E3,1.0280748E3,1.0162035E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.803137E-1,-1.251252E-3,2.084088E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":219,"left_children":[1,-1,-1],"loss_changes":[4.5155548E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.251252E-3,2.084088E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9214469E3,1.7936456E3,1.2780126E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.5825658E-1,-1.245883E-3,2.202858E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":220,"left_children":[1,-1,-1],"loss_changes":[1.9990533E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.245883E-3,2.202858E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.0627693E3,1.0449683E3,1.017801E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7467775E-1,2.2029886E-3,-1.2456218E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":221,"left_children":[1,-1,-1],"loss_changes":[1.9966288E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.2029886E-3,-1.2456218E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0599963E3,1.0336058E3,1.0263905E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.694244E-1,-1.2505841E-3,2.0931086E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":222,"left_children":[1,-1,-1],"loss_changes":[4.930699E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2505841E-3,2.0931086E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9506755E3,1.8103788E3,1.4029669E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7879494E-1,-1.2447532E-3,2.1985346E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":223,"left_children":[1,-1,-1],"loss_changes":[1.9814069E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2447532E-3,2.1985346E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0506592E3,1.01633124E3,1.034328E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6955545E-1,1.1862259E0,-1.2410807E-3,-9.006787E-4,2.1982042E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":224,"left_children":[1,3,-1,-1,-1],"loss_changes":[1.8484774E3,1.2384387E2,0E0,0E0,0E0],"parents":[2147483647,0,0,1,1],"right_children":[2,4,-1,-1,-1],"split_conditions":[4.9965545E-3,1E0,-1.2410807E-3,-9.006787E-4,2.1982042E-3],"split_indices":[36,3,0,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[2.0526135E3,1.0597834E3,9.928301E2,3.5000008E1,1.0247834E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[-5.7191336E-1,-1.2497716E-3,2.0849677E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":225,"left_children":[1,-1,-1],"loss_changes":[4.7712537E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2497716E-3,2.0849677E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9294888E3,1.7933986E3,1.3609023E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.612637E-1,-1.2441132E-3,2.1934342E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":226,"left_children":[1,-1,-1],"loss_changes":[1.9536973E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5E0,-1.2441132E-3,2.1934342E-3],"split_indices":[18,0,0],"split_type":[0,0,0],"sum_hessian":[2.028639E3,1.02230255E3,1.0063365E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6594207E-1,2.1934987E-3,-1.2441183E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":227,"left_children":[1,-1,-1],"loss_changes":[1.9639408E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.1934987E-3,-1.2441183E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.0391361E3,1.01641174E3,1.0227244E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.755086E-1,2.065858E-3,-7.01832E-1,-1.240603E-3,-1.7313637E-4],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0,0,0],"id":228,"left_children":[1,-1,3,-1,-1],"loss_changes":[4.3128485E2,0E0,8.323364E-1,0E0,0E0],"parents":[2147483647,0,0,2,2],"right_children":[2,-1,4,-1,-1],"split_conditions":[1.08E2,2.065858E-3,2.69444E3,-1.240603E-3,-1.7313637E-4],"split_indices":[4,0,33,0,0],"split_type":[0,0,0,0,0],"sum_hessian":[1.9294055E3,1.2427391E2,1.8051316E3,1.785336E3,1.9795586E1],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"5","size_leaf_vector":"1"}},{"base_weights":[2.7222052E-1,-1.2431489E-3,2.1890693E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":229,"left_children":[1,-1,-1],"loss_changes":[1.9410142E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2431489E-3,2.1890693E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0215392E3,1.00634973E3,1.0151894E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.754465E-1,2.1889093E-3,-1.2430564E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":230,"left_children":[1,-1,-1],"loss_changes":[1.93342E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.1889093E-3,-1.2430564E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0139382E3,1.01473413E3,9.9920416E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.663055E-1,2.0826575E-3,-1.2483197E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":231,"left_children":[1,-1,-1],"loss_changes":[4.9437018E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.0826575E-3,-1.2483197E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9410254E3,1.42008E2,1.7990173E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6302674E-1,-1.2426642E-3,2.1845573E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":232,"left_children":[1,-1,-1],"loss_changes":[1.9554216E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2426642E-3,2.1845573E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0426656E3,1.0253243E3,1.01734125E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7839923E-1,2.1845645E-3,-1.2423242E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":233,"left_children":[1,-1,-1],"loss_changes":[1.9364474E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.1845645E-3,-1.2423242E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0232782E3,1.0235919E3,9.996863E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7294E-1,-1.2476516E-3,2.0719767E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":234,"left_children":[1,-1,-1],"loss_changes":[4.7262042E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2476516E-3,2.0719767E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9494808E3,1.8135853E3,1.358956E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.627389E-1,-1.2419121E-3,2.1800415E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":235,"left_children":[1,-1,-1],"loss_changes":[1.9455498E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2419121E-3,2.1800415E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0385717E3,1.02244763E3,1.0161241E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.668089E-1,2.1800916E-3,-1.2419468E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":236,"left_children":[1,-1,-1],"loss_changes":[1.9585577E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.1800916E-3,-1.2419468E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0521558E3,1.0271846E3,1.0249711E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.6674355E-1,2.0711117E-3,-1.2467464E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":237,"left_children":[1,-1,-1],"loss_changes":[4.8193323E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.0711117E-3,-1.2467464E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.9162313E3,1.3934183E2,1.7768895E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7237466E-1,-1.2410459E-3,2.1757702E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":238,"left_children":[1,-1,-1],"loss_changes":[1.9425795E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1E0,-1.2410459E-3,2.1757702E-3],"split_indices":[19,0,0],"split_type":[0,0,0],"sum_hessian":[2.0416577E3,1.01282965E3,1.028828E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.8095698E-1,2.1759935E-3,-1.2410361E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":239,"left_children":[1,-1,-1],"loss_changes":[1.9588518E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.1759935E-3,-1.2410361E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.059016E3,1.0466091E3,1.01240717E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.7162666E-1,2.0626655E-3,-1.2460826E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":240,"left_children":[1,-1,-1],"loss_changes":[4.667768E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.7429978E9,2.0626655E-3,-1.2460826E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[1.926568E3,1.3512404E2,1.791444E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.5934875E-1,-1.240509E-3,2.1711246E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":241,"left_children":[1,-1,-1],"loss_changes":[1.9416788E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.240509E-3,2.1711246E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0469265E3,1.0279702E3,1.0189563E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6961803E-1,2.1714002E-3,-1.2405346E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":242,"left_children":[1,-1,-1],"loss_changes":[1.9632977E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[5.3E1,2.1714002E-3,-1.2405346E-3],"split_indices":[0,0,0],"split_type":[0,0,0],"sum_hessian":[2.069491E3,1.0410785E3,1.0284125E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[-5.673468E-1,-1.2452718E-3,2.0610893E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":243,"left_children":[1,-1,-1],"loss_changes":[4.7481128E2,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6.0002792E1,-1.2452718E-3,2.0610893E-3],"split_indices":[36,0,0],"split_type":[0,0,0],"sum_hessian":[1.9126234E3,1.7745232E3,1.3810028E2],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.6532796E-1,-1.2395413E-3,2.166615E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":244,"left_children":[1,-1,-1],"loss_changes":[1.9148049E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[1.742998E9,-1.2395413E-3,2.166615E-3],"split_indices":[6,0,0],"split_type":[0,0,0],"sum_hessian":[2.0248845E3,1.0095701E3,1.01531445E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}},{"base_weights":[2.7179497E-1,2.1668212E-3,-1.2396179E-3],"categories":[],"categories_nodes":[],"categories_segments":[],"categories_sizes":[],"default_left":[0,0,0],"id":245,"left_children":[1,-1,-1],"loss_changes":[1.936523E3,0E0,0E0],"parents":[2147483647,0,0],"right_children":[2,-1,-1],"split_conditions":[6E0,2.1668212E-3,-1.2396179E-3],"split_indices":[34,0,0],"split_type":[0,0,0],"sum_hessian":[2.0478025E3,1.0336051E3,1.0141973E3],"tree_param":{"num_deleted":"0","num_feature":"52","num_nodes":"3","size_leaf_vector":"1"}}]},"name":"gbtree"},"learner_model_param":{"base_score":"5E-1","boost_from_average":"1","num_class":"3","num_feature":"52","num_target":"1"},"objective":{"name":"multi:softmax","softmax_multiclass_param":{"num_class":"3"}}},"version":[2,1,0]}
</file>

<file path="tune_results/best_params.json">
{
  "max_depth": 5,
  "min_child_weight": 19,
  "reg_alpha": 4.000877452784171,
  "reg_lambda": 6.121688592715271,
  "eta": 0.0017574056471531343,
  "subsample": 0.7626600174065583,
  "colsample_bytree": 0.5836140774820693,
  "num_boost_round": 82
}
</file>

<file path=".gitattributes">
*.csv filter=lfs diff=lfs merge=lfs -text
data/received/UNSW-NB15_1.csv filter=lfs diff=lfs merge=lfs -text
</file>

<file path=".repomixignore">
# Ignore data directory containing large files
data/

# Common large file types
*.csv
*.parquet
*.pkl
*.model

# Ignore outputs and results
outputs/
results/

# Cache directories
__pycache__/
.cache/
</file>

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO, ERROR, WARNING
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
import importlib.util
from sklearn.utils.class_weight import compute_sample_weight
# Default XGBoost parameters for UNSW_NB15 multi-class classification
BST_PARAMS = {
    'objective': 'multi:softmax',  # Multi-class classification
    'num_class': 10,  # Classes: Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic
    'eval_metric': ['mlogloss', 'merror'],  # Multi-class metrics
    'learning_rate': 0.05,
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': 1.0  # Removed class-specific weights as it's not compatible with multi-class with >3 classes
}
# Try to import tuned parameters if available
try:
    # Check if tuned_params.py exists
    tuned_params_path = os.path.join(os.path.dirname(__file__), "tuned_params.py")
    if os.path.exists(tuned_params_path):
        # Dynamically import the tuned parameters
        spec = importlib.util.spec_from_file_location("tuned_params", tuned_params_path)
        tuned_params_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(tuned_params_module)
        # Use the tuned parameters
        TUNED_PARAMS = tuned_params_module.TUNED_PARAMS
        log(INFO, "Using tuned XGBoost parameters from Ray Tune optimization")
    else:
        TUNED_PARAMS = BST_PARAMS.copy()
except Exception as e:
    log(INFO, f"Could not load tuned parameters: {str(e)}")
    TUNED_PARAMS = BST_PARAMS.copy()
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
        cid: Client ID for logging purposes
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        cid,
        params=None,
        train_method="cyclic",
        is_prediction_only=False,
        unlabeled_dmatrix=None,
        use_tuned_params=True
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            cid: Client ID for logging purposes
            params (dict): XGBoost parameters (defaults to BST_PARAMS if None)
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
            use_tuned_params (bool): Whether to use tuned parameters if available
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.cid = cid
        # Use tuned parameters if available and requested
        if params is not None:
            self.params = params
        elif use_tuned_params:
            self.params = TUNED_PARAMS.copy()
            log(INFO, "Using tuned parameters for XGBoost training")
        else:
            self.params = BST_PARAMS.copy()
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        # --- PHASE 1: Aggressive Regularization (Overrides any loaded/tuned params) --- REMOVED
        y_train = self.train_dmatrix.get_label()
        # --- Check if labels are empty ---
        if y_train.size == 0:
            log(ERROR, f"Client {self.cid}: Training DMatrix has no labels. Cannot proceed with fit.")
            return FitRes(
                status=Status(code=Code.FIT_NOT_IMPLEMENTED, message="Training data is missing labels."),
                parameters=Parameters(tensor_type="", tensors=[]), # Return empty params
                num_examples=0,
                metrics={}
            )
        # --- End Check ---
        # Ensure labels are integers for compute_sample_weight
        y_train_int = y_train.astype(int)
        class_counts = np.bincount(y_train_int)
        # Log class distribution for all classes
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        for i, count in enumerate(class_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Training data class {class_name}: {count}")
        # --- Debugging Sample Weight Calculation ---
        log(INFO, f"Type of y_train before weight calc: {type(y_train)}")
        log(INFO, f"Shape of y_train: {y_train.shape if hasattr(y_train, 'shape') else 'N/A'}")
        try:
            log(INFO, f"Unique values in y_train: {np.unique(y_train)}")
        except Exception as e:
            log(INFO, f"Could not get unique values of y_train: {e}")
        log(INFO, f"Type of y_train_int: {type(y_train_int)}")
        log(INFO, f"Shape of y_train_int: {y_train_int.shape}")
        log(INFO, f"Unique values in y_train_int: {np.unique(y_train_int)}")
        log(INFO, f"dtype of y_train_int: {y_train_int.dtype}")
        # Only log min/max if array is not empty
        if y_train_int.size > 0:
            log(INFO, f"Min/Max values in y_train_int: {np.min(y_train_int)} / {np.max(y_train_int)}")
        else:
            log(INFO, "y_train_int is empty, cannot calculate Min/Max.")
        # --- End Debugging ---
        # Compute sample weights for class imbalance
        try:
            sample_weights = compute_sample_weight('balanced', y_train_int) # Use integer labels
            log(INFO, f"Successfully computed sample weights. Shape: {sample_weights.shape}, dtype: {sample_weights.dtype}")
        except IndexError as e:
            log(INFO, f"IndexError during compute_sample_weight: {e}")
            log(INFO, f"Unique labels causing issue: {np.unique(y_train_int)}")
            # As a fallback, use uniform weights
            log(INFO, "Falling back to uniform sample weights.")
            sample_weights = np.ones(len(y_train_int))
        except Exception as e:
            log(INFO, f"Other error during compute_sample_weight: {e}")
            log(INFO, "Falling back to uniform sample weights due to unexpected error.")
            sample_weights = np.ones(len(y_train_int))
        # Create a new DMatrix with weights for training
        dtrain_weighted = xgb.DMatrix(self.train_dmatrix.get_data(), label=y_train, weight=sample_weights, feature_names=self.train_dmatrix.feature_names)
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch with sample weights
            bst = xgb.train(
                self.params,
                dtrain_weighted,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (dtrain_weighted, "train")],
                early_stopping_rounds=20,
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions for multi-class classification
        # Since objective is multi:softprob, predict() outputs probabilities
        y_pred_proba = bst.predict(self.valid_dmatrix)
        # Get class labels from probabilities
        y_pred_labels = np.argmax(y_pred_proba, axis=1)
        # Get ground truth labels
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic']
        num_classes_actual = len(class_names) # Or get from self.params if needed
        for i, count in enumerate(true_counts):
            if i < len(class_names):
                class_name = class_names[i]
            else:
                class_name = f'unknown_{i}'
            log(INFO, f"Ground truth {class_name}: {count}")
        # Compute multi-class metrics using predicted labels
        # Add zero_division=0 to handle cases where a class might not be predicted
        precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
        accuracy = accuracy_score(y_true, y_pred_labels)
        # Calculate mlogloss using probabilities
        epsilon = 1e-15  # Small constant to avoid log(0)
        y_true_int = y_true.astype(int)
        # Ensure y_true_int does not contain labels outside the expected range [0, num_classes-1]
        valid_indices = (y_true_int >= 0) & (y_true_int < num_classes_actual)
        if not np.all(valid_indices):
            log(WARNING, f"Found {np.sum(~valid_indices)} labels outside expected range [0, {num_classes_actual-1}]. Clamping for mlogloss calculation.")
            y_true_int = np.clip(y_true_int, 0, num_classes_actual - 1)
            # Optionally filter data if clamping is not desired:
            # y_true_int = y_true_int[valid_indices]
            # y_pred_proba = y_pred_proba[valid_indices]
        y_true_one_hot = np.eye(num_classes_actual)[y_true_int]
        # Ensure y_pred_proba has the correct shape and handle potential issues
        if y_pred_proba.shape == (len(y_true_int), num_classes_actual):
            # Clip probabilities to avoid log(0)
            y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)
            mlogloss = -np.mean(np.sum(y_true_one_hot * np.log(y_pred_proba_clipped), axis=1))
        else:
            log(WARNING, f"Shape mismatch for mlogloss: y_pred_proba shape {y_pred_proba.shape}, expected ({len(y_true_int)}, {num_classes_actual}). Skipping mlogloss.")
            mlogloss = -1.0 # Indicate failure to calculate
        # Compute confusion matrix using predicted labels
        try:
            # Explicitly provide labels to ensure consistent matrix size
            conf_matrix = confusion_matrix(y_true, y_pred_labels, labels=range(num_classes_actual))
        except Exception as e:
            log(WARNING, f"Error computing confusion matrix: {str(e)}")
            # Create empty confusion matrix with the correct size
            conf_matrix = np.zeros((num_classes_actual, num_classes_actual), dtype=int)
        # Generate detailed classification report using predicted labels
        try:
            # Ensure target_names matches the actual number of classes
            unique_labels = np.unique(np.concatenate((y_true.astype(int), y_pred_labels))) # Get labels present in data
            target_names_filtered = [class_names[i] for i in range(num_classes_actual) if i in unique_labels]
            # Ensure we use labels consistent with target_names_filtered
            labels_for_report = [i for i in range(num_classes_actual) if i in unique_labels]
            class_report = classification_report(
                y_true, 
                y_pred_labels, 
                labels=labels_for_report, 
                target_names=target_names_filtered, 
                zero_division=0
            )
            log(INFO, f"Classification Report:\n{class_report}")
        except Exception as e:
            log(WARNING, f"Error generating classification report: {str(e)}")
        # Log evaluation metrics
        log(INFO, f"Precision (weighted): {precision:.4f}")
        log(INFO, f"Recall (weighted): {recall:.4f}")
        log(INFO, f"F1 Score (weighted): {f1:.4f}")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Multi-class Log Loss: {mlogloss:.4f}")
        log(INFO, f"Confusion Matrix shape: {conf_matrix.shape}")
        # Save predictions for this round
        global_round = int(ins.config["global_round"])
        from server_utils import save_predictions_to_csv
        save_predictions_to_csv(
            data=self.valid_dmatrix,
            predictions=y_pred_labels,
            round_num=global_round,
            output_dir=ins.config.get("output_dir", "results"),
            true_labels=y_true
        )
        # Format metrics in a way that Flower can handle
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "accuracy": float(accuracy),
            "mlogloss": float(mlogloss)
        }
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(mlogloss),  # Use mlogloss as the primary loss metric
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO, WARNING, ERROR
import os
import pandas as pd
import xgboost as xgb
import numpy as np
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    FeatureProcessor,
    preprocess_data
)
from utils import client_args_parser, BST_PARAMS
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    #latest_csv_path = get_latest_csv(data_directory)
    #labeled_dataset = load_csv_data(latest_csv_path)
    #unlabeled_dataset = load_csv_data(latest_csv_path)
    # Ensure data/received/ directory exists
    os.makedirs("data/received", exist_ok=True)
    # Load labeled data for training
    labeled_csv_path = "data/received/UNSW_NB15_training-set.csv"
    labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction
    unlabeled_csv_path = "data/received/UNSW_NB15_testing-set.csv"
    unlabeled_dataset = load_csv_data(unlabeled_csv_path)
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training based on partition_id
    log(INFO, "Loading training partition for client with partition_id=%d...", args.partition_id)
    # Get the entire dataset first
    full_train_data = labeled_dataset["train"] 
    full_train_data.set_format("numpy")
    # Apply the partitioner to get client-specific data partition
    # The ExponentialPartitioner doesn't have get_indices, but provides partition method
    # which returns the partition directly rather than just indices
    try:
        # First try to use get_partition method which returns the partition subset directly
        train_partition = partitioner.get_partition(full_train_data, args.partition_id)
    except AttributeError:
        # If that fails, try using the partition method (used in newer versions)
        try:
            # Newer versions use a different API
            train_partition = partitioner.partition(full_train_data)[args.partition_id]
        except (AttributeError, TypeError, IndexError):
            # As a fallback, if all partition methods fail, use a simple numerical partition
            # by getting evenly spaced indices based on partition ID
            total_samples = len(full_train_data)
            samples_per_partition = total_samples // args.num_partitions
            start_idx = args.partition_id * samples_per_partition
            end_idx = start_idx + samples_per_partition if args.partition_id < args.num_partitions - 1 else total_samples
            partition_indices = list(range(start_idx, end_idx))
            train_partition = full_train_data.select(partition_indices)
            log(INFO, "Used fallback partitioning. Partition %d: samples %d to %d", 
                args.partition_id, start_idx, end_idx)
    log(INFO, "Partition size: %d samples (out of %d total)", 
        len(train_partition), len(full_train_data))
    # Handle data splitting based on evaluation strategy
    if args.centralised_eval:
        # Use centralized test set for evaluation
        train_data = train_partition
        valid_data = labeled_dataset["test"]
        valid_data.set_format("numpy")
        num_train = train_data.shape[0]
        num_val = valid_data.shape[0]
    else:
        # Perform local train/test split using the updated function
        # This now returns the fitted processor as well
        log(INFO, "Performing local train/test split...")
        # Generate a unique seed for each client based on partition_id to ensure
        # different clients get different train/test splits
        client_specific_seed = args.seed + (args.partition_id * 1000)
        log(INFO, "Using client-specific random seed for train/test split: %d", client_specific_seed)
        train_dmatrix, valid_dmatrix, processor = train_test_split( # Capture processor
            train_partition,
            test_fraction=args.test_fraction,
            random_state=client_specific_seed  # Use client-specific seed
        )
        # Get counts from the DMatrix objects
        num_train = train_dmatrix.num_row()
        num_val = valid_dmatrix.num_row()
        log(INFO, "Local split: %d train samples, %d validation samples", num_train, num_val)
    # Transform unlabeled data for prediction using the processor from train_test_split
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    # Convert unlabeled data to pandas DataFrame first
    if not isinstance(unlabeled_data, pd.DataFrame):
        unlabeled_data = unlabeled_data.to_pandas()
    # Preprocess unlabeled data using the fitted processor from train/test split
    try:
        # Use the processor returned by train_test_split
        unlabeled_features, _ = preprocess_data(unlabeled_data, processor=processor, is_training=False)
        unlabeled_dmatrix = xgb.DMatrix(unlabeled_features, missing=np.nan)
        log(INFO, "Successfully preprocessed unlabeled data.")
    except Exception as e:
        log(ERROR, "Failed to preprocess unlabeled data or create DMatrix: %s", e)
        # Handle error appropriately, e.g., skip prediction for this client or use an empty DMatrix
        unlabeled_dmatrix = xgb.DMatrix(np.empty((0,0))) # Create an empty DMatrix as fallback
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        cid=args.partition_id,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="commit.sh">
#!/bin/bash
# Stage all changes
git add .
# Commit the changes with a commit message
git commit -m "commit.sh"
# Push the changes to the remote repository
git push
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
from sklearn.model_selection import train_test_split as train_test_split_pandas
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from flwr.common.logger import log
from logging import INFO, WARNING, ERROR
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
class FeatureProcessor:
    """Handles feature preprocessing while preventing data leakage."""
    def __init__(self):
        self.categorical_encoders = {}
        self.numerical_stats = {}
        self.is_fitted = False
        self.label_encoder = LabelEncoder()
        # Define feature groups for UNSW_NB15 dataset
        self.categorical_features = [
            'proto', 'service', 'state', 'is_ftp_login', 'is_sm_ips_ports'
        ]
        self.numerical_features = [
            'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 
            'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 
            'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 
            'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 
            'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 
            'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst'
        ]
    def fit(self, df: pd.DataFrame) -> None:
        """Fit preprocessing parameters on training data only."""
        if self.is_fitted:
            return
        # Initialize encoders for categorical features
        for col in self.categorical_features:
            if col in df.columns:
                unique_values = df[col].unique()
                # Create a mapping for each unique value to an integer
                self.categorical_encoders[col] = {
                    val: idx for idx, val in enumerate(unique_values)
                }
                # Log warning if a categorical feature is highly predictive
                if len(unique_values) > 1 and len(unique_values) < 10:
                    for val in unique_values:
                        subset = df[df[col] == val]
                        if 'attack_cat' in df.columns and len(subset) > 0:
                            most_common_label = subset['attack_cat'].value_counts().idxmax()
                            label_pct = subset['attack_cat'].value_counts()[most_common_label] / len(subset)
                            if label_pct > 0.9:  # If >90% of rows with this value have the same label
                                log(WARNING, "Potential data leakage detected: Feature '%s' value '%s' is highly predictive of label %s (%.1f%% match)",
                                    col, val, most_common_label, label_pct * 100)
        # Store numerical feature statistics
        for col in self.numerical_features:
            if col in df.columns:
                self.numerical_stats[col] = {
                    'mean': df[col].mean(),
                    'std': df[col].std(),
                    'median': df[col].median(),
                    'q99': df[col].quantile(0.99)
                }
        # Fit label encoder for attack_cat if present
        if 'attack_cat' in df.columns:
            self.label_encoder.fit(df['attack_cat'])
        self.is_fitted = True
    def transform(self, df: pd.DataFrame, is_training: bool = False) -> pd.DataFrame:
        """Transform data using fitted parameters."""
        if not self.is_fitted and is_training:
            self.fit(df)
        elif not self.is_fitted:
            raise ValueError("FeatureProcessor must be fitted before transform")
        df = df.copy()
        # Drop id column since it's just an identifier
        if 'id' in df.columns:
            df.drop(columns=['id'], inplace=True)
        # Transform categorical features
        for col in self.categorical_features:
            if col in df.columns and col in self.categorical_encoders:
                # Map known categories, set unknown to -1
                df[col] = df[col].map(self.categorical_encoders[col]).fillna(-1)
        # Handle numerical features with added noise for validation data
        for col in self.numerical_features:
            if col in df.columns and col in self.numerical_stats:
                # Replace infinities
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
                # Cap outliers using 99th percentile
                q99 = self.numerical_stats[col]['q99']
                df.loc[df[col] > q99, col] = q99  # Cap outliers
                # Fill NaN with median
                median = self.numerical_stats[col]['median']
                # Find NaN positions
                nan_mask = df[col].isna()
                # Fill with median value
                df[col] = df[col].fillna(median)
        # Explicitly drop the raw attack_cat column if it exists
        # Label encoding is handled separately in preprocess_data
        if 'attack_cat' in df.columns:
            df.drop(columns=['attack_cat'], inplace=True)
        # Also drop the binary 'label' column if it exists
        if 'label' in df.columns:
             df.drop(columns=['label'], inplace=True)
        return df
def preprocess_data(data: pd.DataFrame, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Preprocess the data by encoding categorical features and separating features and labels.
    Handles multi-class classification for the UNSW_NB15 dataset with 10 classes.
    Args:
        data (pd.DataFrame): Input DataFrame
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    if processor is None:
        processor = FeatureProcessor()
    # --- Handle labels FIRST --- 
    labels = None
    if 'attack_cat' in data.columns:
        # Extract 'attack_cat' before transforming features
        attack_labels = data['attack_cat'].copy()
        # Ensure label encoder is fitted if needed (during training)
        if is_training and not hasattr(processor.label_encoder, 'classes_'):
             log(INFO, "Fitting label encoder during training preprocessing.")
             processor.label_encoder.fit(attack_labels)
        elif not hasattr(processor.label_encoder, 'classes_') or processor.label_encoder.classes_.size == 0:
            # If not training but encoder isn't fitted, can't proceed reliably
            log(WARNING, "Label encoder not fitted, cannot transform attack_cat labels.")
            # Try fitting on the current chunk, might be incomplete
            try:
                 processor.label_encoder.fit(attack_labels)
                 log(WARNING, "Fitted label encoder on non-training data chunk.")
            except Exception as fit_err:
                 log(ERROR, f"Could not fit label encoder on non-training data: {fit_err}")
                 # Use a specific value like -1 or np.nan if XGBoost handles it, else zeros
                 labels = np.full(len(data), -1, dtype=int) # Fallback to -1
        # Transform labels if encoder is ready
        if hasattr(processor.label_encoder, 'classes_') and processor.label_encoder.classes_.size > 0:
            try:
                # Handle unseen labels during transform by mapping them to a default class (e.g., -1 or max_class_index + 1)
                # For now, let's assume fit handled all expected labels, or handle error
                labels = processor.label_encoder.transform(attack_labels)
            except ValueError as e:
                log(ERROR, f"Error transforming labels: {e}. Unseen labels might exist.")
                # Fallback: assign a default value like -1 or try to refit/update encoder
                # Ensure fallback has same length as data
                log(ERROR, f"Assigning fallback labels due to transform error.")
                labels = np.full(len(data), -1, dtype=int) # Simple fallback for now
        elif labels is None: # If fitting failed or wasn't possible earlier
             # Ensure fallback has same length as data
             log(WARNING, "Assigning fallback labels because fitting failed or was not possible.")
             labels = np.full(len(data), -1, dtype=int)
    elif 'label' in data.columns:
        # If only binary label, we might still want to return it, but tune script expects multi-class
        log(WARNING, "Only binary 'label' column found, but multi-class 'attack_cat' expected for tuning/training.")
        # Return None for labels as it's not the expected format
        labels = None 
    else:
        # No label column found
        log(INFO, "No 'attack_cat' or 'label' column found in data.")
        labels = None
    # --- Process features AFTER handling labels --- 
    # The processor's transform method will drop 'attack_cat' and 'label' if they exist
    features = processor.transform(data, is_training)
    return features, labels
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Check if this is an unlabeled test set (from filename)
    is_unlabeled = "nolabel" in file_path.lower()
    # Create appropriate dataset structure
    dataset = Dataset.from_pandas(df)
    if is_unlabeled:
        # For unlabeled data, keep the current structure (all data in both train/test)
        # This won't create issues since unlabeled data is only used for prediction
        return DatasetDict({"train": dataset, "test": dataset})
    else:
        # For labeled data, create a proper 80/20 split to avoid data leakage
        # Use a specific random seed for reproducibility
        train_test_dict = dataset.train_test_split(test_size=0.2, seed=42)
        return DatasetDict({
            "train": train_test_dict["train"],
            "test": train_test_dict["test"]
        })
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def transform_dataset_to_dmatrix(data, processor: FeatureProcessor = None, is_training: bool = False):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset (should be pandas DataFrame)
        processor (FeatureProcessor): Feature processor instance for consistent preprocessing
        is_training (bool): Whether this is training data
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    # The input 'data' should already be a pandas DataFrame in this context
    x, y = preprocess_data(data, processor=processor, is_training=is_training)
    # --- Logging before DMatrix creation ---
    log(INFO, f"[transform_dataset_to_dmatrix] is_training={is_training}")
    log(INFO, f"[transform_dataset_to_dmatrix] Features shape: {x.shape}")
    if y is not None:
        log(INFO, f"[transform_dataset_to_dmatrix] Labels type: {type(y)}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels shape: {y.shape if hasattr(y, 'shape') else 'N/A'}")
        log(INFO, f"[transform_dataset_to_dmatrix] Labels head: {y[:5] if hasattr(y, '__len__') and len(y) > 0 else 'N/A'}")
    else:
        log(INFO, "[transform_dataset_to_dmatrix] Labels are None.")
    # --- End Logging ---
    # Handle case where preprocess_data might return None for labels (e.g., unlabeled data)
    if y is None:
        log(INFO, "No labels found in data. Creating DMatrix without labels.")
        return xgb.DMatrix(x, missing=np.nan)
    # For validation data, log label distribution to help identify issues
    if not is_training:
        # Count occurrences of each label
        label_counts = np.bincount(y.astype(int))
        # Use the correct class names for UNSW_NB15 dataset
        label_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'] 
        log(INFO, "Label distribution in validation data:")
        for i, count in enumerate(label_counts):
            class_name = label_names[i] if i < len(label_names) else f'unknown_{i}'
            log(INFO, f"  {class_name}: {count}")
    return xgb.DMatrix(x, label=y, missing=np.nan)
def train_test_split(
    data,
    test_fraction: float = 0.2,
    random_state: int = 42,
) -> Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]:
    """
    Split dataset into train and test sets, preprocess, and return DMatrices and the fitted processor.
    Args:
        data: Input dataset (Hugging Face Dataset or pandas DataFrame)
        test_fraction (float): Fraction of data to use for testing
        random_state (int): Random seed for reproducibility
    Returns:
        Tuple[xgb.DMatrix, xgb.DMatrix, FeatureProcessor]: 
            - Training DMatrix
            - Test DMatrix
            - Fitted FeatureProcessor instance
    """
    # Convert to pandas if needed
    if not isinstance(data, pd.DataFrame):
        data = data.to_pandas()
    # Use sklearn's train_test_split with shuffle=True to ensure data is properly randomized
    log(INFO, "Original data shape before splitting: %s", data.shape)
    # Set random seed for consistency
    np.random.seed(random_state)
    # Check if 'label' column exists in data
    if 'label' not in data.columns:
        log(INFO, "Warning: No 'label' column found in data. Available columns: %s", data.columns.tolist())
    else:
        # Report class distribution
        label_counts = data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in original data: %s", label_counts)
    # Check for data leakage indicators
    if 'uid' in data.columns:
        uid_label_counts = data.groupby('uid')['label'].value_counts()
        uid_with_multiple_labels = uid_label_counts.index.get_level_values(0).duplicated(keep=False)
        if not any(uid_with_multiple_labels):
            log(WARNING, "CRITICAL: Each UID has only one label, indicating potential perfect data leakage through UIDs")
    # Generate a completely different random_state for validation split
    validation_random_state = (random_state * 17 + 3) % 10000
    log(INFO, "Using different random states for train/validation split: %d/%d", 
        random_state, validation_random_state)
    # Split data ensuring complete partition separation
    if 'uid' in data.columns:
        # If we have UIDs, use them to ensure no data leakage across train/test
        log(INFO, "Using UID-based splitting to ensure no data leakage")
        unique_uids = data['uid'].unique()
        np.random.seed(validation_random_state)
        np.random.shuffle(unique_uids)
        test_size = int(len(unique_uids) * test_fraction)
        test_uids = unique_uids[:test_size]
        train_uids = unique_uids[test_size:]
        # Split based on UIDs
        train_data = data[data['uid'].isin(train_uids)].copy()
        test_data = data[data['uid'].isin(test_uids)].copy()
        log(INFO, "Split by UIDs: %d train UIDs, %d test UIDs", len(train_uids), len(test_uids))
    else:
        # If no UIDs, use standard stratified split
        train_data, test_data = train_test_split_pandas(
            data,
            test_size=test_fraction,
            random_state=validation_random_state,
            shuffle=True,  # Ensure data is shuffled for a proper split
            stratify=data['label'] if 'label' in data.columns else None  # Use stratified split if possible
        )
    # Log the shapes to verify they're different sets
    log(INFO, "Train data shape: %s, Test data shape: %s", train_data.shape, test_data.shape)
    # Verify label distributions to ensure proper stratification
    if 'label' in data.columns:
        train_label_counts = train_data['label'].value_counts().to_dict()
        test_label_counts = test_data['label'].value_counts().to_dict()
        log(INFO, "Class distribution in train data: %s", train_label_counts)
        log(INFO, "Class distribution in test data: %s", test_label_counts)
    # Check for unique values in both sets to verify they're actually different
    if 'uid' in data.columns:
        train_uids = set(train_data['uid'].unique())
        test_uids = set(test_data['uid'].unique())
        common_uids = train_uids.intersection(test_uids)
        if common_uids:
            log(WARNING, "WARNING: Found %d UIDs in both train and test sets! This indicates data leakage.", 
                len(common_uids))
        else:
            log(INFO, "Good: Train and test sets have completely separate UIDs (no overlap).")
    # Initialize feature processor
    processor = FeatureProcessor()
    # Fit processor on training data and transform both sets
    # Note: transform calls fit implicitly if is_training=True and not fitted
    train_dmatrix = transform_dataset_to_dmatrix(train_data, processor=processor, is_training=True)
    test_dmatrix = transform_dataset_to_dmatrix(test_data, processor=processor, is_training=False)
    # Log number of examples for verification
    log(INFO, "Train DMatrix has %d rows, Test DMatrix has %d rows", 
        train_dmatrix.num_row(), test_dmatrix.num_row())
    # Return the fitted processor along with DMatrices
    return train_dmatrix, test_dmatrix, processor
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
# Comment out or remove ModelPredictor if not used or complete
# class ModelPredictor:
#     """
#     Handles model prediction and dataset labeling
#     """
#     def __init__(self, model_path: str):
#         self.model = xgb.Booster()
#         self.model.load_model(model_path)
#     def predict_and_save(
#         self,
#         input_data: Union[str, pd.DataFrame],
#         output_path: str,
#         include_confidence: bool = True
#     ):
#         """
#         Predict on new data and save labeled dataset
#         """
#         # Load/preprocess input data
#         # data = self._prepare_data(input_data) # Requires _prepare_data method
#         # Generate predictions
#         # predictions = self.model.predict(data)
#         # confidence = None
#         # if include_confidence:
#         #     confidence = self.model.predict(data, output_margin=True)
#         # Save labeled dataset
#         # self._save_output(data, predictions, confidence, output_path) # Requires _save_output method
</file>

<file path="go_to_work.sh">
#!/bin/bash
# Run Python scripts in the background and store their process IDs (PIDs)
python3 data/receiving_data.py &
PID1=$!
python3 data/livepreprocessing_socket.py &
PID2=$!
# Wait for 1 minute
sleep 30
# Kill the Python scripts
kill $PID1 $PID2
# Run Git commands and GitHub workflow
git pull
./commit.sh
gh workflow run cml.yaml
# Wait for 5 minutess
sleep 300
git pull
</file>

<file path="multi_class_implementation_plan.md">
# Multi-Class Classification Implementation Plan

## Current State
- Binary classification (benign vs malicious) using XGBoost
- Features include both categorical and numerical network traffic data
- Using federated learning with Flower framework
- Need to expand to classify specific types of malicious traffic

## Implementation Steps

### 1. Data Preprocessing Modifications (`dataset.py`)
- [ ] Update `preprocess_data()` function:
  ```python
  def preprocess_data(data):
      # ... existing code ...
      if 'label' in df.columns:
          features = df.drop(columns=['label'])
          
          # New label mapping for three classes
          label_mapping = {
              'benign': 0, 
              'dns_tunneling': 1, 
              'icmp_tunneling': 2
          }
          labels_series = df['label'].map(label_mapping)
          
          # Handle unmapped labels
          if labels_series.isnull().any():
              unmapped_labels = df['label'][labels_series.isnull()].unique()
              print(f"Warning: Unmapped labels found: {unmapped_labels}")
              labels_series = labels_series.fillna(-1)
          
          labels = labels_series.astype(int)
          return features, labels
      else:
          return df, None
  ```

### 2. Model Configuration Updates (`client_utils.py`)
- [ ] Update XGBoost parameters in `utils.py`:
  ```python
  BST_PARAMS = {
      'objective': 'multi:softmax',
      'num_class': 3,
      'eval_metric': ['mlogloss', 'merror'],
      'learning_rate': 0.1,
      'max_depth': 6,
      'min_child_weight': 1,
      'subsample': 0.8,
      'colsample_bytree': 0.8,
      'scale_pos_weight': [1.0, 2.0, 2.0]  # Adjust based on class distribution
  }
  ```

- [ ] Modify `evaluate()` method in `XgbClient` class:
  ```python
  def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
      # ... existing model loading code ...
      
      # Generate predictions - No thresholding needed for multi-class
      y_pred_proba = bst.predict(self.valid_dmatrix)
      y_pred_labels = np.argmax(y_pred_proba, axis=1)
      
      # Get ground truth labels
      y_true = self.valid_dmatrix.get_label()
      
      # Compute multi-class metrics
      precision = precision_score(y_true, y_pred_labels, average='weighted')
      recall = recall_score(y_true, y_pred_labels, average='weighted')
      f1 = f1_score(y_true, y_pred_labels, average='weighted')
      accuracy = accuracy_score(y_true, y_pred_labels)
      
      # Multi-class loss calculation
      epsilon = 1e-10
      log_likelihood = -np.log(y_pred_proba[np.arange(len(y_true)), y_true.astype(int)] + epsilon)
      loss = np.mean(log_likelihood)
      
      # Multi-class confusion matrix
      conf_matrix = confusion_matrix(y_true, y_pred_labels)
      
      metrics = {
          "precision": float(precision),
          "recall": float(recall),
          "f1": float(f1),
          "accuracy": float(accuracy),
          "loss": float(loss),
          "confusion_matrix": conf_matrix.tolist(),
          "num_predictions": self.num_val
      }
      
      return metrics
  ```

### 3. Server-Side Updates (`server_utils.py`)
- [ ] Update metrics aggregation:
  ```python
  def evaluate_metrics_aggregation(eval_metrics):
      total_num = sum([num for num, _ in eval_metrics])
      
      # Aggregate evaluation metrics
      metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy', 'loss']
      aggregated_metrics = {}
      
      for metric in metrics_to_aggregate:
          if all(metric in metrics for _, metrics in eval_metrics):
              aggregated_metrics[metric] = sum([metrics[metric] * num for num, metrics in eval_metrics]) / total_num
          else:
              aggregated_metrics[metric] = 0.0
      
      # Aggregate confusion matrix
      aggregated_conf_matrix = None
      for num, metrics in eval_metrics:
          conf_matrix = np.array(metrics.get("confusion_matrix", [[0, 0, 0], [0, 0, 0], [0, 0, 0]]))
          if aggregated_conf_matrix is None:
              aggregated_conf_matrix = conf_matrix
          else:
              aggregated_conf_matrix += conf_matrix
      
      aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix.tolist()
      aggregated_metrics["prediction_mode"] = eval_metrics[0][1].get("prediction_mode", False)
      
      return aggregated_metrics["loss"], aggregated_metrics
  ```

- [ ] Update prediction saving:
  ```python
  def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None):
      predictions_dict = {
          'predicted_label': predictions,
          'prediction_type': [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling') 
              for p in predictions
          ]
      }
      # ... rest of the function
  ```

### 4. Prediction Updates (`use_saved_model.py`)
- [ ] Update prediction saving:
  ```python
  def save_detailed_predictions(predictions, output_path):
      results_df = pd.DataFrame()
      
      if predictions.ndim > 1 and predictions.shape[1] > 1:
          results_df['raw_probabilities'] = predictions.tolist()
          predicted_labels = np.argmax(predictions, axis=1)
          results_df['predicted_label'] = predicted_labels
          
          results_df['prediction_type'] = [
              'benign' if p == 0 else 
              ('dns_tunneling' if p == 1 else 'icmp_tunneling')
              for p in predicted_labels
          ]
          
          results_df['prediction_score'] = predictions[
              np.arange(len(predicted_labels)), 
              predicted_labels
          ]
      
      results_df.to_csv(output_path, index=False)
      return results_df
  ```

- [ ] Update main evaluation:
  ```python
  def main():
      # ... existing code ...
      if args.has_labels:
          y_pred_labels = np.argmax(raw_predictions, axis=1)
          accuracy = accuracy_score(y_true, y_pred_labels)
          
          cm = confusion_matrix(y_true, y_pred_labels)
          report = classification_report(y_true, y_pred_labels)
          
          log(INFO, f"Accuracy: {accuracy:.4f}")
          log(INFO, f"Confusion Matrix:\n{cm}")
          log(INFO, f"Classification Report:\n{report}")
  ```

## Testing Strategy

### 1. Unit Tests
- [ ] Test label mapping in `dataset.py`
- [ ] Test multi-class metrics calculation
- [ ] Test prediction format and class probabilities
- [ ] Test confusion matrix calculation

### 2. Integration Tests
- [ ] Test full training pipeline with three classes
- [ ] Test federated learning convergence
- [ ] Test model saving and loading
- [ ] Test prediction pipeline

### 3. System Tests
- [ ] End-to-end training and evaluation
- [ ] Performance testing with large datasets
- [ ] Class imbalance handling
- [ ] Error handling and edge cases

## Success Criteria
1. Model successfully trains on three-class data
2. All evaluation metrics properly calculated and reported
3. Predictions include class probabilities
4. Federated learning pipeline works with multiple classes
5. High accuracy in distinguishing between benign and malicious traffic
6. Accurate classification of DNS vs ICMP tunneling attacks
7. Documentation is complete and accurate
8. Test cases pass successfully

## Potential Challenges and Mitigations
1. Class imbalance
   - Solution: Use class weights in model parameters
   - Monitor class distribution in training data
   - Consider data augmentation for underrepresented classes

2. Model complexity
   - Solution: Start with simpler model and gradually increase complexity
   - Monitor training metrics for overfitting
   - Use cross-validation for parameter tuning

3. Federated learning convergence
   - Solution: Adjust learning rate and number of rounds
   - Monitor loss curves across clients
   - Consider using FedAvg with momentum

4. Performance impact
   - Solution: Profile code for bottlenecks
   - Optimize prediction pipeline
   - Consider batch processing for large datasets
</file>

<file path="original_bst_params.json">
{
  "objective": "multi:softmax",
  "num_class": 3,
  "eta": 0.05,
  "max_depth": 3,
  "min_child_weight": 10,
  "gamma": 1.0,
  "subsample": 0.7,
  "colsample_bytree": 0.6,
  "colsample_bylevel": 0.6,
  "nthread": 16,
  "tree_method": "hist",
  "eval_metric": [
    "mlogloss",
    "merror"
  ],
  "max_delta_step": 5,
  "reg_alpha": 2.0,
  "reg_lambda": 5.0,
  "base_score": 0.5,
  "scale_pos_weight": [
    1.0,
    2.0,
    1.0
  ],
  "grow_policy": "lossguide",
  "normalize_type": "tree",
  "random_state": 42
}
</file>

<file path="pretrained_model_usage.md">
# Using the Saved Federated Learning Model

This document explains how to use the saved XGBoost model that was trained using the federated learning process.

## Overview

The federated learning process now saves the final trained model after the training process completes. The model is saved in two formats:

1. JSON format (`final_model.json`) - Human-readable format
2. Binary format (`final_model.bin`) - More efficient for loading

These files are saved in the output directory created for each training run, which follows the pattern:
```
outputs/YYYY-MM-DD/HH-MM-SS/
```

## Making Predictions with the Saved Model

We provide a utility script `use_saved_model.py` that demonstrates how to load the saved model and use it for making predictions on new data.

### Prerequisites

Make sure you have all the required dependencies installed:
- XGBoost
- Pandas
- NumPy
- Scikit-learn (for evaluation metrics)
- Flower (for logging utilities)

### Basic Usage

```bash
python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data> --output_path <path_for_predictions>
```

#### Arguments:

- `--model_path`: Path to the saved model file (.json or .bin)
- `--data_path`: Path to the data file (.csv)
- `--output_path`: Path to save the predictions (default: predictions.csv)
- `--has_labels`: Flag to indicate if the data file contains labels (for evaluation)
- `--info_only`: Display model information without making predictions

### Examples

#### Viewing model information:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --info_only
```

This will display information about the model such as the number of trees, feature importance, and model parameters.

#### Making predictions on unlabeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/unlabeled_data.csv --output_path predictions.csv
```

#### Evaluating the model on labeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/test_data.csv --output_path predictions.csv --has_labels
```

When using the `--has_labels` flag, the script will also calculate and display evaluation metrics such as accuracy, precision, recall, and F1 score.

## Troubleshooting

### Model File Not Found
Make sure the path to the model file is correct. The model files are saved in the output directory created for each training run.

### Model Loading Issues
If you encounter issues loading the model, the system will try multiple approaches:
1. Direct loading using `load_model`
2. Reading the file as bytes and loading into a Booster
3. Creating a new Booster with parameters and then loading the model

### Data Format Issues
The data should be in a format that can be converted to an XGBoost DMatrix. If you're having issues, check that your data has the same features that were used during training.

If you use the `--has_labels` flag but the system can't process the data as labeled, it will automatically fall back to processing it as unlabeled data.

### Memory Issues
If you're working with large datasets, you might encounter memory issues. Consider processing the data in batches or using a machine with more memory.

## Additional Resources

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Flower Documentation](https://flower.dev/docs/)
</file>

<file path="progress.md">
# FL-CML-Pipeline: Progress Summary

## Project Overview

This project implements a privacy-preserving machine learning solution using federated learning with the Flower framework. The system allows multiple clients to collaboratively train XGBoost models for network intrusion detection without sharing raw data, preserving privacy while achieving high model performance.

## Key Components

### 1. Federated Learning Architecture

- **Server Implementation (`server.py`)**
  - Controls the federated learning process
  - Implements both bagging and cyclic training strategies
  - Handles model aggregation and evaluation
  - Manages client selection and coordination

- **Client Implementation (`client.py`)**
  - Loads and processes local data
  - Trains local models based on server instructions
  - Participates in the federated learning process
  - Reports results back to the server

### 2. Data Pipeline

- **Data Processing (`dataset.py`)**
  - Loads CSV data from network traffic captures
  - Implements preprocessing for network traffic features
  - Provides multiple partitioning strategies (IID, Linear, Square, Exponential)
  - Handles data format conversions for XGBoost compatibility

- **Real-time Data Capture**
  - Support for live network traffic capture
  - Processing and conversion to training datasets

### 3. Utility Functions

- **Client Utilities (`client_utils.py`)**
  - XGBoost client implementation
  - Client-side helper functions

- **Server Utilities (`server_utils.py`)**
  - Server-side helper functions
  - Client management systems
  - Results handling and storage

### 4. Experiment Framework

- **Training Methods**
  - Bagging approach (`run_bagging.sh`): Aggregates models from multiple clients
  - Cyclic approach (`run_cyclic.sh`): Passes model sequentially through clients

- **Evaluation**
  - Supports both centralized and federated evaluation
  - Tracks multiple metrics (precision, recall, F1 score)

## Project Features

- ✅ **Privacy-Preserving Training** - True federated learning with data isolation
- ✅ **Flexible Configuration** - Support for various training strategies
- ✅ **Reproducible Experiments** - Automatic output organization
- ✅ **Custom Dataset Support** - CSV data loader with preprocessing pipeline
- ✅ **Multiple Partitioning Strategies** - IID, Linear, Square, Exponential

## Recent Developments

### Implemented Core Functionality
- Established federated learning architecture with Flower framework
- Created data processing pipeline for network traffic data
- Implemented both bagging and cyclic training approaches
- Set up experiment scripts for reproducible testing

### Technical Improvements
- Enhanced XGBoost integration with Flower framework
- Improved data partitioning strategies
- Optimized client-server communication
- Implemented comprehensive metrics tracking

### Documentation
- Created detailed README with project structure and instructions
- Documented key components and their relationships
- Added configuration guidelines and examples

### Infrastructure
- Set up project directory structure
- Created Bash scripts for easy experiment execution
- Implemented output storage and organization

## Next Steps

### Planned Enhancements
- Improve scalability for larger numbers of clients
- Enhance privacy guarantees with differential privacy techniques
- Optimize hyperparameters for better model performance
- Add support for more model architectures

### Ongoing Research
- Comparing performance of bagging vs. cyclic approaches
- Analyzing impact of different data partitioning strategies
- Evaluating model convergence across federated strategies

## Conclusion

The FL-CML-Pipeline project has established a solid foundation for privacy-preserving machine learning using federated learning. The implemented system successfully demonstrates collaborative model training across multiple clients without sharing raw data, achieving the core goal of privacy-preserving machine learning for network intrusion detection.

The project continues to evolve with ongoing research into optimal federated learning strategies and implementation improvements to enhance performance and usability.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.4.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "xgboost-comprehensive"
version = "0.1.0"
description = "Federated XGBoost with Flower (comprehensive)"
authors = ["The Flower Authors <hello@flower.ai>"]

[tool.poetry.dependencies]
python = ">=3.8,<3.11"
flwr = { extras = ["simulation"], version = ">=1.7.0,<2.0" }
flwr-datasets = ">=0.2.0,<1.0.0"
xgboost = ">=2.0.0,<3.0.0"
</file>

<file path="ray_tune_README.md">
# Ray Tune XGBoost Hyperparameter Optimization

This guide explains how to use Ray Tune for optimizing the hyperparameters of the XGBoost classifier in our federated learning pipeline.

## Overview

Ray Tune is a powerful library for hyperparameter tuning that can efficiently find optimal parameters for machine learning models. We've integrated Ray Tune with our XGBoost implementation to achieve better model performance through automated hyperparameter search.

## Requirements

Make sure you have all the required packages installed:

```bash
pip install -r requirements.txt
```

## Hyperparameter Tuning Process

### 1. Run Ray Tune Optimization

Use the `run_ray_tune.sh` script to start the optimization process:

```bash
# Basic usage
bash run_ray_tune.sh --data-file path/to/your/data.csv

# Advanced usage with more options
bash run_ray_tune.sh \
  --data-file path/to/your/data.csv \
  --num-samples 20 \
  --cpus-per-trial 2 \
  --gpu-fraction 0.2 \
  --output-dir ./my_tuning_results
```

#### Available Options

- `--data-file`: Path to the CSV data file (required)
- `--num-samples`: Number of hyperparameter combinations to try (default: 10)
- `--cpus-per-trial`: CPUs to allocate per trial (default: 1)
- `--gpu-fraction`: Fraction of GPU to use per trial (e.g., 0.1 for 10%)
- `--output-dir`: Directory to save results (default: ./tune_results)

### 2. Apply Tuned Parameters to Federated Learning

After tuning is complete, use the `use_tuned_params.py` script to integrate the best parameters into your federated learning system:

```bash
python use_tuned_params.py --params-file ./tune_results/best_params.json
```

This will:
1. Backup the original parameters to `original_bst_params.json`
2. Create a `tuned_params.py` file with the optimized parameters
3. Provide instructions on how to use these parameters

### 3. Run Federated Learning with Tuned Parameters

The federated learning system will automatically detect and use the tuned parameters if:

1. The `tuned_params.py` file exists in the project directory
2. You haven't explicitly provided custom parameters to the XgbClient

## How It Works

The Ray Tune optimization process:

1. **Search Space Definition**: We define a search space for hyperparameters like `max_depth`, `min_child_weight`, `eta`, etc.
2. **ASHA Scheduler**: We use the Asynchronous Successive Halving Algorithm (ASHA) for early stopping of poorly performing trials
3. **Parallel Execution**: Multiple hyperparameter combinations are evaluated in parallel
4. **Best Model Selection**: The best performing model is identified based on validation metrics
5. **Model Persistence**: The best model and its parameters are saved for later use

## Parameters Being Tuned

The following XGBoost parameters are optimized:

- `max_depth`: Maximum depth of a tree
- `min_child_weight`: Minimum sum of instance weight needed in a child
- `eta`: Learning rate
- `subsample`: Subsample ratio of the training instances
- `colsample_bytree`: Subsample ratio of columns when constructing each tree
- `reg_alpha`: L1 regularization term on weights
- `reg_lambda`: L2 regularization term on weights
- `num_boost_round`: Number of boosting rounds

## GPU Support

If you have a GPU available, you can use it to speed up the tuning process by specifying a `--gpu-fraction` value. This enables XGBoost's GPU acceleration via the `gpu_hist` tree method.

## Monitoring and Results

During tuning, progress is logged to the console. After completion, you'll find these files in the output directory:

- `best_params.json`: JSON file containing the best hyperparameters
- `best_model.json`: XGBoost model trained with the best hyperparameters
- `progress.csv`: CSV file with metrics for all trials
- Detailed trial information in subdirectories

## Troubleshooting

- **Memory Issues**: If you encounter memory errors, try reducing `--num-samples` or `--cpus-per-trial`
- **GPU Errors**: If you face GPU-related errors, try removing the `--gpu-fraction` option or installing the appropriate CUDA toolkit
- **Import Errors**: Ensure all dependencies are installed with `pip install -r requirements.txt`

## Advanced Usage

### Custom Search Space

To customize the hyperparameter search space, modify the `search_space` dictionary in `ray_tune_xgboost.py`.

### Integration with Existing Code

The `client_utils.py` file has been updated to automatically use tuned parameters when available. You can control this behavior by setting the `use_tuned_params` parameter when initializing the `XgbClient`.
</file>

<file path="ray_tune_xgboost_updated.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, log_loss
import logging
from ray.air.config import RunConfig
# Import existing data processing code
from dataset import load_csv_data, transform_dataset_to_dmatrix
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info(f"Starting trial with config: {config}")
    # Instantiate FeatureProcessor inside the trial
    from dataset import FeatureProcessor
    processor = FeatureProcessor()
    processor.fit(train_df)
    train_processed = processor.transform(train_df, is_training=True)
    test_processed = processor.transform(test_df, is_training=False)
    train_features = train_processed.drop(columns=['label'])
    train_labels = train_processed['label'].astype(int)
    test_features = test_processed.drop(columns=['label'])
    test_labels = test_processed['label'].astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softmax',
        'num_class': 10,  # UNSW_NB15 has 10 classes
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred = bst.predict(test_data)
    y_true = test_data.get_label()
    # Compute multi-class metrics
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    accuracy = accuracy_score(y_true, y_pred)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file=None, test_file=None, data_file=None, num_samples=100, cpus_per_trial=1, gpu_fraction=None, output_dir="./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune.
    Args:
        train_file (str): Path to the training CSV data file (required for HPO)
        test_file (str): Path to the testing CSV data file (ignored for HPO validation split)
        data_file (str): Path to a single CSV data file (fallback if train_file not provided, will be split)
        num_samples (int): Number of hyperparameter combinations to try (default: 100)
        cpus_per_trial (int): CPUs to allocate per trial
        gpu_fraction (float): Fraction of GPU to use per trial (if None, no GPU is used)
        output_dir (str): Directory to save results
    Returns:
        dict: Best hyperparameters found
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Load and prepare data
    if train_file and test_file:
        logger.info(f"Loading training data from {train_file}")
        train_data = load_csv_data(train_file)["train"].to_pandas()
        logger.info(f"Loading testing data from {test_file}")
        test_data = load_csv_data(test_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        for df in [train_data]:
            if 'label' not in df.columns and 'Label' in df.columns:
                df['label'] = df['Label']
        # Check if test data has a label column
        if 'label' not in test_data.columns and 'Label' in test_data.columns:
            test_data['label'] = test_data['Label']
        # If test data doesn't have a label column, create a dummy one
        if 'label' not in test_data.columns:
            logger.warning("Test data doesn't have a label column. Creating a dummy label column with zeros.")
            test_data['label'] = 0
        # Create feature processor and fit it on training data
        from dataset import FeatureProcessor
        # Create and fit the feature processor
        processor = FeatureProcessor()
        processor.fit(train_data)
        # Process the data, but don't create DMatrix yet (to avoid pickling issues)
        train_processed = processor.transform(train_data, is_training=True)
        test_processed = processor.transform(test_data, is_training=False)
        # Extract features and labels
        train_features = train_processed.drop(columns=['label'])
        train_labels = train_processed['label'].astype(int)
        test_features = test_processed.drop(columns=['label'])
        test_labels = test_processed['label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    else:
        # Fall back to original behavior with single file and splitting
        logger.info(f"Loading data from {data_file}")
        data = load_csv_data(data_file)["train"].to_pandas()
        # Ensure label column is correctly handled - case insensitive check
        if 'label' not in data.columns and 'Label' in data.columns:
            data['label'] = data['Label']
        # Import this function only if needed
        from dataset import train_test_split
        # Split the data, but use pandas directly
        from sklearn.model_selection import train_test_split as sklearn_split
        train_processed, test_processed = sklearn_split(data, test_size=0.2, random_state=42)
        # Process with feature processor
        processor = FeatureProcessor()
        processor.fit(train_processed)
        train_processed = processor.transform(train_processed, is_training=True)
        test_processed = processor.transform(test_processed, is_training=False)
        # Extract features and labels
        train_features = train_processed.drop(columns=['label'])
        train_labels = train_processed['label'].astype(int)
        test_features = test_processed.drop(columns=['label'])
        test_labels = test_processed['label'].astype(int)
        logger.info(f"Training data size: {len(train_features)}")
        logger.info(f"Validation data size: {len(test_features)}")
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 50, 200, 1)
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the data DataFrames
    def _train_with_data_wrapper(config):
        return train_xgboost(config, train_data.copy(), test_data.copy()) # Pass copies of FULL DataFrames with labels
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200,
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        _train_with_data_wrapper,
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=RunConfig(
            local_dir=output_dir,
            name="xgboost_tune"
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info(f"  mlogloss: {best_metrics['mlogloss']:.4f}")
    logger.info(f"  merror: {best_metrics['merror']:.4f}")
    logger.info(f"  precision: {best_metrics['precision']:.4f}")
    logger.info(f"  recall: {best_metrics['recall']:.4f}")
    logger.info(f"  f1: {best_metrics['f1']:.4f}")
    logger.info(f"  accuracy: {best_metrics['accuracy']:.4f}")
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    with open(best_params_file, 'w') as f:
        json.dump(best_config, f, indent=2)
    logger.info(f"Best parameters saved to {best_params_file}")
    # Train a final model with the best parameters
    train_final_model(best_config, train_features, train_labels, test_features, test_labels, output_dir)
    return best_config
def train_final_model(config, train_features, train_labels, test_features, test_labels, output_dir):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softmax',
        'num_class': 10,  # UNSW_NB15 has 10 classes
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info(f"Final model saved to {model_path}")
    # Evaluate the model
    y_pred = final_model.predict(test_data)
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    accuracy = accuracy_score(y_true, y_pred)
    # Log final performance
    logger.info("Final model performance:")
    logger.info(f"  Precision: {precision:.4f}")
    logger.info(f"  Recall: {recall:.4f}")
    logger.info(f"  F1 Score: {f1:.4f}")
    logger.info(f"  Accuracy: {accuracy:.4f}")
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Ray Tune for XGBoost hyperparameter optimization")
    parser.add_argument("--data-file", type=str, help="Path to single CSV data file (optional if train and test files are provided)")
    parser.add_argument("--train-file", type=str, help="Path to training CSV data file")
    parser.add_argument("--test-file", type=str, help="Path to testing CSV data file")
    parser.add_argument("--num-samples", type=int, default=10, help="Number of hyperparameter combinations to try")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="CPUs per trial")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="GPU fraction per trial (0.1 for 10%)")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Output directory for results")
    args = parser.parse_args()
    # Validate arguments
    if not args.data_file and not (args.train_file and args.test_file):
        parser.error("Either --data-file or both --train-file and --test-file must be provided")
    # Run the hyperparameter tuning
    tune_xgboost(
        train_file=args.train_file,
        test_file=args.test_file,
        data_file=args.data_file,
        num_samples=args.num_samples,
        cpus_per_trial=args.cpus_per_trial,
        gpu_fraction=args.gpu_fraction,
        output_dir=args.output_dir
    )
if __name__ == "__main__":
    main()
</file>

<file path="ray_tune_xgboost.py">
"""
ray_tune_xgboost.py
This script implements Ray Tune for hyperparameter optimization of XGBoost models in the
federated learning pipeline. It leverages the existing data processing pipeline while
adding a tuning layer to find optimal hyperparameters.
Key Components:
- Ray Tune integration for hyperparameter search
- XGBoost parameter space definition
- Multi-class evaluation metrics (precision, recall, F1)
- Optimal model selection and persistence
"""
import os
import argparse
import json
import xgboost as xgb
import pandas as pd
import numpy as np
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.hyperopt import HyperOptSearch
from hyperopt import hp
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import logging
from ray import air
# Import existing data processing code
from dataset import load_csv_data, preprocess_data, FeatureProcessor
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def train_xgboost(config, train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    Training function for XGBoost that can be used with Ray Tune.
    Args:
        config (dict): Hyperparameters to use for training
        train_df (pd.DataFrame): Training data as DataFrame
        test_df (pd.DataFrame): Test data as DataFrame
    """
    logger.info("Starting trial with config: %s", config)
    # Use preprocess_data to get features and encoded labels
    processor = FeatureProcessor()
    train_features, train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    test_features, test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle cases where test data might be unlabeled
    if test_labels is None:
        logger.warning("Test data has no labels. Creating dummy labels for DMatrix.")
        test_labels = np.zeros(len(test_features))
    else:
        test_labels = test_labels.astype(int) # Ensure labels are integers
    # Ensure train labels are integers
    train_labels = train_labels.astype(int)
    # Create DMatrix objects inside the function to avoid pickling issues
    # FeatureProcessor already handles feature types and drops unnecessary columns
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 10,  # UNSW_NB15 has 10 classes
        'eval_metric': ['mlogloss', 'merror'],
        # Tunable parameters from config - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Fixed parameters for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Store evaluation results
    results = {}
    # Train the model
    bst = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        evals_result=results,
        verbose_eval=False
    )
    # Get the final evaluation metrics
    final_iteration = len(results['eval']['mlogloss']) - 1
    eval_mlogloss = results['eval']['mlogloss'][final_iteration]
    eval_merror = results['eval']['merror'][final_iteration]
    # Make predictions for more detailed metrics
    y_pred_proba = bst.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Compute multi-class metrics using predicted labels
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Return metrics to Ray Tune instead of using tune.report
    return {
        "mlogloss": eval_mlogloss,
        "merror": eval_merror,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy
    }
def tune_xgboost(train_file: str, test_file: str, num_samples: int = 100, cpus_per_trial: int = 1, gpu_fraction: float = None, output_dir: str = "./tune_results"):
    """
    Run hyperparameter tuning for XGBoost using Ray Tune.
    Args:
        train_file (str): Path to the training CSV data file.
        test_file (str): Path to the testing CSV data file.
        num_samples (int): Number of hyperparameter combinations to try.
        cpus_per_trial (int): Number of CPUs to allocate per trial.
        gpu_fraction (float): Fraction of GPU to use per trial (if None, no GPU is used).
        output_dir (str): Directory to save results.
    Returns:
        dict: Best hyperparameters found.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    # Load and prepare data *once* before tuning starts
    logger.info("Loading training data from %s", train_file)
    train_df = load_csv_data(train_file)["train"].to_pandas()
    logger.info("Loading testing data from %s", test_file)
    test_df = load_csv_data(test_file)["train"].to_pandas() # Use train split of test file for validation
    # Preprocess data *once* to get features/labels for the final model training
    # and to fit the processor
    logger.info("Preprocessing data for final model training and fitting processor...")
    processor = FeatureProcessor()
    # We need the original dataframes (train_df, test_df) to pass to the trial wrapper
    # But we also need the processed features/labels for the final model training step
    final_train_features, final_train_labels = preprocess_data(train_df, processor=processor, is_training=True)
    final_test_features, final_test_labels = preprocess_data(test_df, processor=processor, is_training=False)
    # Handle potential missing labels in the test set for final model evaluation
    if final_test_labels is None:
        logger.warning("Test data has no labels. Using zeros for final model evaluation.")
        final_test_labels = np.zeros(len(final_test_features))
    else:
        final_test_labels = final_test_labels.astype(int) # Ensure labels are integers
    final_train_labels = final_train_labels.astype(int)
    logger.info("Training data size for final model: %d", len(final_train_features))
    logger.info("Validation data size for final model: %d", len(final_test_features))
    # Define the search space using hyperopt's hp module
    search_space = {
        "max_depth": hp.quniform("max_depth", 3, 10, 1),
        "min_child_weight": hp.quniform("min_child_weight", 1, 20, 1),
        "reg_alpha": hp.loguniform("reg_alpha", np.log(1e-3), np.log(10.0)),
        "reg_lambda": hp.loguniform("reg_lambda", np.log(1e-3), np.log(10.0)),
        "eta": hp.loguniform("eta", np.log(1e-3), np.log(0.3)),
        "subsample": hp.uniform("subsample", 0.5, 1.0),
        "colsample_bytree": hp.uniform("colsample_bytree", 0.5, 1.0),
        "num_boost_round": hp.quniform("num_boost_round", 1, 10, 1)  # Modified range for FL compatibility
    }
    if gpu_fraction is not None and gpu_fraction > 0:
        # NOTE: Ray Tune handles GPU allocation via resources_per_trial typically
        # Setting tree_method here might be sufficient, but review Ray Tune docs if issues persist
        search_space["tree_method"] = hp.choice("tree_method", ["gpu_hist"])
    # Create a wrapper function that includes the *original* data DataFrames
    # The train_xgboost function inside the trial will handle preprocessing
    def _train_with_data_wrapper(config):
        # Pass copies of the original dataframes to the training function
        return train_xgboost(config, train_df.copy(), test_df.copy()) 
    # Set up HyperOptSearch
    algo = HyperOptSearch(
        search_space,
        metric="mlogloss",
        mode="min"
    )
    scheduler = ASHAScheduler(
        max_t=200, # Corresponds roughly to num_boost_round max
        grace_period=10,
        reduction_factor=2,
        metric="mlogloss",
        mode="min"
    )
    # Wrap trainable with resource specification
    trainable_with_resources = tune.with_resources(
        _train_with_data_wrapper, 
        resources={"cpu": cpus_per_trial, "gpu": gpu_fraction if gpu_fraction else 0}
    )
    # Initialize the tuner
    logger.info("Starting hyperparameter tuning")
    # Run the tuning with updated API
    tuner = tune.Tuner(
        trainable_with_resources, # Use wrapped trainable
        tune_config=tune.TuneConfig(
            scheduler=scheduler,
            num_samples=num_samples,
            search_alg=algo
        ),
        param_space={},  # search space is handled by HyperOptSearch
        run_config=air.RunConfig( # Use air.RunConfig
            local_dir=output_dir,
            name="xgboost_tune",
            # resources_per_trial is handled by tune.with_resources
        )
    )
    # Execute the hyperparameter search
    results = tuner.fit()
    # Get the best trial
    best_result = results.get_best_result(metric="mlogloss", mode="min")
    best_config = best_result.config
    best_metrics = best_result.metrics
    # Log the best configuration and metrics
    logger.info("Best hyperparameters found:")
    logger.info(json.dumps(best_config, indent=2))
    logger.info("Best metrics:")
    logger.info("  mlogloss: %.4f", best_metrics['mlogloss'])
    logger.info("  merror: %.4f", best_metrics['merror'])
    logger.info("  precision: %.4f", best_metrics['precision'])
    logger.info("  recall: %.4f", best_metrics['recall'])
    logger.info("  f1: %.4f", best_metrics['f1'])
    logger.info("  accuracy: %.4f", best_metrics['accuracy'])
    # Save the best hyperparameters to a file
    best_params_file = os.path.join(output_dir, "best_params.json")
    # Use utf-8 encoding when writing JSON
    with open(best_params_file, 'w', encoding='utf-8') as f:
        json.dump(best_config, f, indent=2)
    logger.info("Best parameters saved to %s", best_params_file)
    # Train a final model with the best parameters using the preprocessed data
    train_final_model(best_config, final_train_features, final_train_labels, final_test_features, final_test_labels, output_dir)
    return best_config
def train_final_model(config: dict, 
                      train_features: pd.DataFrame, train_labels: pd.Series, 
                      test_features: pd.DataFrame, test_labels: pd.Series, 
                      output_dir: str):
    """
    Train a final model using the best hyperparameters found.
    Args:
        config (dict): Best hyperparameters
        train_features (pd.DataFrame): Training features
        train_labels (pd.Series): Training labels (encoded)
        test_features (pd.DataFrame): Test features
        test_labels (pd.Series): Test labels (encoded)
        output_dir (str): Directory to save the model
    """
    # Create DMatrix objects
    train_data = xgb.DMatrix(train_features, label=train_labels, missing=np.nan)
    test_data = xgb.DMatrix(test_features, label=test_labels, missing=np.nan)
    # Prepare the XGBoost parameters
    params = {
        # Fixed parameters
        'objective': 'multi:softprob',
        'num_class': 10,  # UNSW_NB15 has 10 classes
        'eval_metric': ['mlogloss', 'merror'],
        # Best parameters from tuning - convert float values to integers where needed
        'max_depth': int(config['max_depth']),
        'min_child_weight': int(config['min_child_weight']),
        'eta': config['eta'],
        'subsample': config['subsample'],
        'colsample_bytree': config['colsample_bytree'],
        'reg_alpha': config['reg_alpha'],
        'reg_lambda': config['reg_lambda'],
        # Set the seed for reproducibility
        'seed': 42
    }
    # Optional GPU support if available
    if config.get('tree_method') == 'gpu_hist':
        params['tree_method'] = 'gpu_hist'
    # Train the final model
    logger.info("Training final model with best parameters")
    final_model = xgb.train(
        params,
        train_data,
        num_boost_round=int(config['num_boost_round']),
        evals=[(test_data, 'eval'), (train_data, 'train')],
        verbose_eval=True # Show progress for final model
    )
    # Save the model
    model_path = os.path.join(output_dir, "best_model.json")
    final_model.save_model(model_path)
    logger.info("Final model saved to %s", model_path)
    # Evaluate the model
    y_pred_proba = final_model.predict(test_data) # Renamed from y_pred
    y_pred_labels = np.argmax(y_pred_proba, axis=1) # Get predicted labels from probabilities
    y_true = test_data.get_label()
    # Generate performance metrics
    precision = precision_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    recall = recall_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred_labels, average='weighted', zero_division=0)
    accuracy = accuracy_score(y_true, y_pred_labels)
    # Log final performance
    logger.info("Final model performance:")
    logger.info("  Precision: %.4f", precision)
    logger.info("  Recall: %.4f", recall)
    logger.info("  F1 Score: %.4f", f1)
    logger.info("  Accuracy: %.4f", accuracy)
    return final_model
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="XGBoost Hyperparameter Tuning with Ray Tune")
    parser.add_argument("--train-file", type=str, required=True, help="Path to the training data CSV file.")
    parser.add_argument("--test-file", type=str, required=True, help="Path to the testing data CSV file.")
    parser.add_argument("--num-samples", type=int, default=100, help="Number of hyperparameter samples to try.")
    parser.add_argument("--cpus-per-trial", type=int, default=1, help="Number of CPUs to allocate per trial.")
    parser.add_argument("--gpu-fraction", type=float, default=None, help="Fraction of GPU resources per trial (e.g., 0.5). Default is None (CPU only).")
    parser.add_argument("--output-dir", type=str, default="./tune_results", help="Directory to save Ray Tune results and best parameters.")
    args = parser.parse_args()
    logger.info("===== XGBoost Hyperparameter Tuning with Ray Tune ====")
    logger.info("Training file: %s", args.train_file)
    logger.info("Testing file: %s", args.test_file)
    logger.info("Number of hyperparameter samples: %d", args.num_samples)
    logger.info("CPUs per trial: %d", args.cpus_per_trial)
    logger.info("GPU: %s", f"{args.gpu_fraction * 100}% per trial" if args.gpu_fraction else "Not used")
    logger.info("Output directory: %s", args.output_dir)
    logger.info("=======================================================")
    try:
        # Run the tuning process
        best_params = tune_xgboost(
            train_file=args.train_file,
            test_file=args.test_file,
            num_samples=args.num_samples,
            cpus_per_trial=args.cpus_per_trial,
            gpu_fraction=args.gpu_fraction,
            output_dir=args.output_dir
        )
        # Train the final model with the best hyperparameters
        # Load data again for final training (could be optimized if memory is a concern)
        train_df_final = load_csv_data(args.train_file)["train"].to_pandas()
        test_df_final = load_csv_data(args.test_file)["train"].to_pandas()
        # Preprocess data using the same processor logic used during tuning
        processor_final = FeatureProcessor()
        final_train_features, final_train_labels = preprocess_data(train_df_final, processor=processor_final, is_training=True)
        final_test_features, final_test_labels = preprocess_data(test_df_final, processor=processor_final, is_training=False)
        # Ensure labels are integers
        final_train_labels = final_train_labels.astype(int)
        if final_test_labels is not None:
            final_test_labels = final_test_labels.astype(int)
        else:
            # Handle case where test set might still be unlabeled after tuning run
            final_test_labels = np.zeros(len(final_test_features)) # Dummy labels if needed
        train_final_model(
            config=best_params, 
            train_features=final_train_features, train_labels=final_train_labels, 
            test_features=final_test_features, test_labels=final_test_labels, 
            output_dir=args.output_dir
        )
        logger.info("===== Hyperparameter tuning finished successfully ====")
    except Exception as e:
        logger.error("Hyperparameter tuning failed: %s", str(e), exc_info=True)
        print("===== Hyperparameter tuning failed ====") # Also print to stderr for visibility in GH Actions
if __name__ == "__main__":
    main()
</file>

<file path="README.md">
# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## 🛠️ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture → 2. Clean Capture and Convert to Dataset → 3. Train/Test → 4.️Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## 🗺️ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## 🎯 What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **📚 Table of Contents**
- [✨ Features](#-features)  
- [📂 Project Structure](#-project-structure)  
- [🚀 Getting Started](#-getting-started)  
- [⚙️ Configuration](#-configuration)
- [📂 Output Structure](#-output-structure)
- [🧪 Running Experiments](#-running-experiments)  
- [⚖️ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## ✨ Features  
✅ **Privacy-Preserving Training** - Federated learning implementation with data isolation  
✅ **Flexible Configuration** - Hydra-powered experiment management  
✅ **Reproducible Experiments** - ⚠️Automatic output organization   
✅ **CI/CD Integration** - GitHub Actions workflow with CML reporting  
✅ **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **📂 Project Structure**
```bash
├── github/
│   └── workflows/
│       └── cml.yaml      # CI/CD workflow definition
├── pyecache/             # Python cache directory
├── data/                 # Dataset files, data capture script, and data cleaning script
├   └── received/         # Data from Zeek/Kafka stream
├── outputs/              # Model, Predictions, Eval; Output files
├── results/              # Latest aggregated metrics
├── client.py             # Flower client logic
├── client_utils.py       # Client helper functions
├── dataset.py            # Data loading/preprocessing
├── poetry.lock           # Poetry dependency lockfile - 🔍 exploring (research phase) 🔍
├── pyproject.toml        # Poetry project configuration - 🔍 exploring (research phase) 🔍
├── requirements.txt      # Python dependencies
├── run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - 🚧 under construction (implementation phase) 🚧
├── run_bagging.sh        # Bagging experiment script - runs script.py + client.py
├── run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
├── server.py             # Flower server logic
├── server_utils.py       # Server helper functions
├── sim.py                # Start simulation - ⚠️ deprecated soon ⚠️
├── utils.py              # Shared utilities
└── README.md             # Project documentation
```

---

## **🚀 Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **⚠️⚙️ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **⚠📂 Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
└── YYYY-MM-DD/                  # Run date
    └── HH-MM-SS/                # Run time
        ├── .hydra/              # ⚠️Config snapshots
        │   ├── config.yaml
        │   └── hydra.yaml
        ├── results.pkl          # Training history
        ├── predictions/         # Model predictions
            ├── predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **🧪 Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# ⚖️ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## 🔄 **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## 🎒 **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## 📊 Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- ༼ つ ◕_◕ ༽つ R&D ZONE ༼ つ ◕_◕ ༽つ -->
<div align="center">

## 🔥 **R&D Led By** 🔥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  🧑💻 Coded with 100% chaos-driven curiosity    !
!  ☕ Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
🔐 Cyber Alchemy Brewing For 🏛️ Indiana University of Pennsylvania's ARMZTA Project

🔗 https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- ༼ つ ◕_◕ ༽つ R&D ZONE ༼ つ ◕_◕ ༽つ -->
</file>

<file path="requirements.txt">
flwr[simulation]>=1.7.0, <2.0
flwr-datasets>=0.2.0, <1.0.0
xgboost>=2.0.0, <3.0.0
ray[tune]>=2.9.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.2.0
matplotlib
seaborn
</file>

<file path="run_bagging copy.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=2 --num-rounds=20 --num-clients-per-round=2 &
sleep 30  # Sleep for 30s to give the server enough time to start
for i in `seq 0 1`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --num-partitions=2 --partitioner-type=exponential &
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_bagging.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=5 --num-rounds=5 --num-clients-per-round=5 &
sleep 30  # Sleep for 30s to give the server enough time to start
# Start regular client (partition 0)
echo "Starting regular client (partition 0)"
python3 client.py --partition-id=0 --num-partitions=5 --partitioner-type=exponential &
# Start regular client (partition 1)
echo "Starting regular client (partition 1)"
python3 client.py --partition-id=1 --num-partitions=5 --partitioner-type=exponential &
# Start regular client (partition 2)
echo "Starting regular client (partition 2)"
python3 client.py --partition-id=2 --num-partitions=5 --partitioner-type=exponential &
# Start regular client (partition 3)
echo "Starting regular client (partition 3)"
python3 client.py --partition-id=3 --num-partitions=5 --partitioner-type=exponential &
# Start regular client (partition 4)
echo "Starting regular client (partition 4)"
python3 client.py --partition-id=4 --num-partitions=5 --partitioner-type=exponential &
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_cyclic.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --train-method=cyclic --pool-size=20 --num-rounds=10 &
sleep 30  # Sleep for 15s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --train-method=cyclic --num-partitions=5 --partitioner-type=uniform &
    sleep 5
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_ray_tune.sh">
#!/bin/bash
# This script runs XGBoost hyperparameter tuning using Ray Tune
# Default values
DATA_FILE=""
TRAIN_FILE=""
TEST_FILE=""
NUM_SAMPLES=20
CPUS_PER_TRIAL=1
GPU_FRACTION=""
OUTPUT_DIR="./tune_results"
# Parse command line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --data-file)
      DATA_FILE="$2"
      shift 2
      ;;
    --train-file)
      TRAIN_FILE="$2"
      shift 2
      ;;
    --test-file)
      TEST_FILE="$2"
      shift 2
      ;;
    --num-samples)
      NUM_SAMPLES="$2"
      shift 2
      ;;
    --cpus-per-trial)
      CPUS_PER_TRIAL="$2"
      shift 2
      ;;
    --gpu-fraction)
      GPU_FRACTION="--gpu-fraction $2"
      shift 2
      ;;
    --output-dir)
      OUTPUT_DIR="$2"
      shift 2
      ;;
    *)
      echo "Unknown option: $1"
      exit 1
      ;;
  esac
done
# Check if proper data sources are provided
if [ -z "$DATA_FILE" ] && [ -z "$TRAIN_FILE" -o -z "$TEST_FILE" ]; then
  echo "Error: Either --data-file or both --train-file and --test-file must be provided"
  echo "Usage: $0 [--data-file <path_to_csv_file>] [--train-file <path_to_train_csv>] [--test-file <path_to_test_csv>] [--num-samples <number>] [--cpus-per-trial <number>] [--gpu-fraction <float>] [--output-dir <path>]"
  exit 1
fi
# Print hyperparameter tuning settings
echo "===== XGBoost Hyperparameter Tuning with Ray Tune ====="
if [ -n "$DATA_FILE" ]; then
  echo "Data file: $DATA_FILE"
else
  echo "Training file: $TRAIN_FILE"
  echo "Testing file: $TEST_FILE"
fi
echo "Number of hyperparameter samples: $NUM_SAMPLES"
echo "CPUs per trial: $CPUS_PER_TRIAL"
if [ -n "$GPU_FRACTION" ]; then
  echo "GPU fraction: $GPU_FRACTION"
else
  echo "GPU: Not used"
fi
echo "Output directory: $OUTPUT_DIR"
echo "======================================================="
# Run the Ray Tune script
if [ -n "$DATA_FILE" ]; then
  python ray_tune_xgboost.py \
    --data-file "$DATA_FILE" \
    --num-samples "$NUM_SAMPLES" \
    --cpus-per-trial "$CPUS_PER_TRIAL" \
    $GPU_FRACTION \
    --output-dir "$OUTPUT_DIR"
else
  python ray_tune_xgboost.py \
    --train-file "$TRAIN_FILE" \
    --test-file "$TEST_FILE" \
    --num-samples "$NUM_SAMPLES" \
    --cpus-per-trial "$CPUS_PER_TRIAL" \
    $GPU_FRACTION \
    --output-dir "$OUTPUT_DIR"
fi
# Check if the tuning completed successfully
if [ $? -eq 0 ]; then
  echo "===== Hyperparameter tuning completed successfully ====="
  echo "Best parameters saved to: $OUTPUT_DIR/best_params.json"
  echo "Best model saved to: $OUTPUT_DIR/best_model.json"
  echo "======================================================="
else
  echo "===== Hyperparameter tuning failed ====="
fi
</file>

<file path="run.py">
import subprocess
import time
import os
def run_script(script_path):
    """Run a Python script from the given path."""
    return subprocess.Popen(['python3', script_path])
def wait_for_new_file(directory):
    """Wait for a new file to be created in the directory."""
    existing_files = set(os.listdir(directory))
    while True:
        time.sleep(5)  # Check every 5 seconds
        current_files = set(os.listdir(directory))
        new_files = current_files - existing_files
        if new_files:
            print(f"New file detected: {new_files}")
            return new_files.pop()  # Return the first detected new file
def main():
    # Paths to your scripts
    preprocessing_script = '/home/mohamed/Desktop/test_repo/data/livepreprocessing_socket.py'
    receiving_script = '/home/mohamed/Desktop/test_repo/data/receiving_data.py'
    data_directory = '/home/mohamed/Desktop/test_repo/data'
    # Start both scripts
    preprocessing_process = run_script(preprocessing_script)
    receiving_process = run_script(receiving_script)
    print("Both scripts are running...")
    try:
        # Wait for a new file to appear in the /data directory
        new_file = wait_for_new_file(data_directory)
        # Run the 'act -j run' command once the new file is detected
        subprocess.run(['act', '-j', 'run', '--container-architecture', 'linux/amd64', '-v'])
        print(f"'act -j run' executed after detecting {new_file}")
    except KeyboardInterrupt:
        print("Process interrupted by user.")
    finally:
        # Terminate the running processes
        preprocessing_process.terminate()
        receiving_process.terminate()
        print("Processes terminated.")
if __name__ == "__main__":
    main()
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, log_loss, accuracy_score
from logging import INFO, WARNING
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
import numpy as np
# Assuming visualization_utils.py is in the same directory or accessible via PYTHONPATH
from visualization_utils import (
    plot_confusion_matrix,
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution
)
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """
    Aggregate evaluation metrics from multiple clients for multi-class classification.
    Args:
        eval_metrics: List of tuples (num_examples, metrics_dict) from each client
    Returns:
        tuple: (loss, aggregated_metrics)
    """
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, "Received metrics from %d clients", len(eval_metrics))
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, "Client %d metrics: %s", i+1, metrics.keys())
        if "mlogloss" in metrics:
            log(INFO, "Client %d mlogloss: %f", i+1, metrics["mlogloss"])
    # Initialize aggregated metrics dictionary
    metrics_to_aggregate = ['precision', 'recall', 'f1', 'accuracy']
    aggregated_metrics = {}
    # Aggregate weighted metrics
    for metric in metrics_to_aggregate:
        if all(metric in metrics for _, metrics in eval_metrics):
            weighted_sum = sum([metrics[metric] * num for num, metrics in eval_metrics])
            aggregated_metrics[metric] = weighted_sum / total_num
        else:
            aggregated_metrics[metric] = 0.0
            log(INFO, "Metric %s not available in all client metrics", metric)
    # Aggregate loss (using mlogloss)
    if all("mlogloss" in metrics for _, metrics in eval_metrics):
        client_losses = [metrics["mlogloss"] for _, metrics in eval_metrics]
        log(INFO, "Individual client losses (mlogloss): %s", client_losses)
        loss = sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]) / total_num
        log(INFO, "Aggregated loss calculation: sum(mlogloss*num)=%f, total_num=%d, result=%f",
            sum([metrics["mlogloss"] * num for num, metrics in eval_metrics]), total_num, loss)
    else:
        loss = 0.0
        log(INFO, "Mlogloss not available in all client metrics")
    # aggregated_metrics["loss"] = loss  # REMOVED - Keep as "loss" for compatibility
    aggregated_metrics["mlogloss"] = loss  # Store as mlogloss
    # Aggregate confusion matrix
    aggregated_conf_matrix = None
    for num, metrics in eval_metrics:
        if "confusion_matrix" in metrics:
            conf_matrix = metrics["confusion_matrix"]
            if aggregated_conf_matrix is None:
                aggregated_conf_matrix = [[0 for _ in range(len(conf_matrix[0]))] for _ in range(len(conf_matrix))]
            # Add weighted confusion matrix
            for i in range(len(conf_matrix)):
                for j in range(len(conf_matrix[0])):
                    aggregated_conf_matrix[i][j] += conf_matrix[i][j] * num
    # Normalize confusion matrix by total examples
    if aggregated_conf_matrix is not None:
        for i in range(len(aggregated_conf_matrix)):
            for j in range(len(aggregated_conf_matrix[0])):
                aggregated_conf_matrix[i][j] /= total_num
    aggregated_metrics["confusion_matrix"] = aggregated_conf_matrix
    # Log aggregated metrics
    log(INFO, "Aggregated metrics:")
    log(INFO, "  Precision (weighted): %f", aggregated_metrics["precision"])
    log(INFO, "  Recall (weighted): %f", aggregated_metrics["recall"])
    log(INFO, "  F1 Score (weighted): %f", aggregated_metrics["f1"])
    log(INFO, "  Accuracy: %f", aggregated_metrics["accuracy"])
    log(INFO, "  Loss (mlogloss): %f", aggregated_metrics["mlogloss"])
    if aggregated_conf_matrix is not None:
        log(INFO, "  Confusion Matrix:\n%s", aggregated_conf_matrix)
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    if not (isinstance(loss, (int, float)) and isinstance(aggregated_metrics, dict)):
        log(INFO, "[ERROR] Output of evaluate_metrics_aggregation is not (loss, dict): %s, %s", type(loss), type(aggregated_metrics))
        raise TypeError("evaluate_metrics_aggregation must return (loss, dict)")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None, prediction_types=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data
        predictions: Prediction labels (class indices)
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
        prediction_types (list, optional): List of prediction type strings (e.g., 'Normal', 'Reconnaissance', etc.)
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predictions,
    }
    # Add prediction types if provided
    if prediction_types is not None:
        predictions_dict['prediction_type'] = prediction_types
    else:
        # Default mapping for UNSW_NB15 multi-class predictions
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic'
        }
        predictions_dict['prediction_type'] = [label_mapping.get(int(p), 'unknown') for p in predictions]
    # Add true labels if available
    if true_labels is not None:
        predictions_dict['true_label'] = true_labels
        # Generate and save visualizations if we have true labels to compare with
        try:
            class_names = list(label_mapping.values())
            num_classes = len(class_names)
            # Convert to numpy arrays if they're not already
            y_true = np.array(true_labels) if not isinstance(true_labels, np.ndarray) else true_labels
            y_pred = np.array(predictions) if not isinstance(predictions, np.ndarray) else predictions
            # Create confusion matrix
            cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{round_num}.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{round_num}.png")
            plot_class_distribution(y_true, y_pred, class_names, dist_path)
            log(INFO, f"Visualizations saved for round {round_num}")
        except Exception as e:
            log(WARNING, f"Error generating visualizations: {e}")
    predictions_df = pd.DataFrame(predictions_dict)
    # Save predictions
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions: %s", raw_predictions)
    # Log distribution of scores
    log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
        np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # For multi-class, the raw predictions should already be class indices
    # For binary, we might need to convert scores to binary predictions
    if len(raw_predictions.shape) == 1:  # Binary case
        probabilities = 1 / (1 + np.exp(-raw_predictions))  # Sigmoid transformation
        predicted_labels = (probabilities >= 0.5).astype(int)
        # Save predictions to CSV
        predictions_df = pd.DataFrame({
            'predicted_label': predicted_labels,
            'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
            'prediction_score': probabilities
        })
    else:  # Multi-class case
        predicted_labels = np.argmax(raw_predictions, axis=1)
        # Default mapping for UNSW_NB15 multi-class predictions
        label_mapping = {
            0: 'Normal', 
            1: 'Reconnaissance', 
            2: 'Backdoor', 
            3: 'DoS', 
            4: 'Exploits', 
            5: 'Analysis', 
            6: 'Fuzzers', 
            7: 'Worms', 
            8: 'Shellcode', 
            9: 'Generic'
        }
        # Save predictions to CSV with class names
        predictions_df = pd.DataFrame({
            'predicted_label': predicted_labels,
            'prediction_type': [label_mapping.get(int(p), 'unknown') for p in predicted_labels],
        })
        # Add probability columns for each class
        for i in range(raw_predictions.shape[1]):
            predictions_df[f'prob_class_{i}'] = raw_predictions[:, i]
    # Log predicted class distribution
    unique, counts = np.unique(predicted_labels, return_counts=True)
    log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Generate visualizations if true labels are available
    try:
        true_labels = dmatrix.get_label()
        if true_labels is not None:
            output_dir = os.path.dirname(output_path)
            class_names = list(label_mapping.values())
            num_classes = len(class_names)
            # Create confusion matrix
            cm = confusion_matrix(true_labels, predicted_labels, labels=range(num_classes))
            cm_path = os.path.join(output_dir, "final_confusion_matrix.png")
            plot_confusion_matrix(cm, class_names, cm_path)
            # Plot class distribution
            dist_path = os.path.join(output_dir, "final_class_distribution.png")
            plot_class_distribution(true_labels, predicted_labels, class_names, dist_path)
            # Plot ROC and Precision-Recall Curves (for multi-class)
            if len(raw_predictions.shape) > 1 and raw_predictions.shape[1] == num_classes:
                roc_path = os.path.join(output_dir, "final_roc_curves.png")
                plot_roc_curves(true_labels, raw_predictions, class_names, roc_path)
                pr_path = os.path.join(output_dir, "final_precision_recall_curves.png")
                plot_precision_recall_curves(true_labels, raw_predictions, class_names, pr_path)
            log(INFO, "Visualizations saved with final model predictions")
    except Exception as e:
        log(WARNING, f"Error generating visualizations: {e}")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_model(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Predict on test data
            y_pred = bst.predict(test_data)
            y_pred_labels = y_pred.astype(int)
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_labels, server_round, "results", y_true)
            # Evaluate
            predictions = bst.predict(test_data)
            pred_proba = bst.predict(test_data, output_margin=False) # Need probabilities for ROC/PR
            # Ensure pred_proba has the correct shape for multi-class
            if len(pred_proba.shape) == 1 or pred_proba.shape[1] == 1:
                 # If predict gives labels or single class proba, try predict_proba if available
                 try:
                     # Note: XGBoost predict() with multi:softmax directly gives labels.
                     # To get probabilities, the objective might need to be multi:softprob
                     log(WARNING, "Predict output seems 1D, attempting to handle for multi-class probability plots...")
                     if BST_PARAMS.get('objective') == 'multi:softmax':
                         # Create dummy probabilities centered around the predicted class
                         num_classes = BST_PARAMS.get('num_class', 10) # Default to 10 if not set
                         pred_proba = np.zeros((len(predictions), num_classes))
                         for i, label in enumerate(predictions):
                             if 0 <= int(label) < num_classes: # Check bounds
                                pred_proba[i, int(label)] = 0.9 # Assign high prob to predicted
                                other_prob = 0.1 / max(1, (num_classes - 1))
                                for j in range(num_classes):
                                    if j != int(label):
                                        pred_proba[i,j] = other_prob
                             else:
                                 log(WARNING, f"Prediction label {label} out of bounds [0, {num_classes-1}]")
                                 # Assign uniform probability as fallback if label is invalid
                                 pred_proba[i, :] = 1.0 / num_classes
                         log(WARNING, "Reconstructed dummy probabilities for multi:softmax. Plots may be inaccurate. Consider using 'multi:softprob' objective for better probability estimates.")
                     else: # Cannot determine probabilities
                         pred_proba = None
                 except AttributeError:
                     log(WARNING, "Could not get probabilities, ROC and PR curves will not be generated.")
                     pred_proba = None
                 except Exception as e:
                     log(WARNING, f"Error processing probabilities: {e}. ROC/PR plots skipped.")
                     pred_proba = None
            elif pred_proba.shape[1] != BST_PARAMS.get('num_class', 10):
                 log(WARNING, f"Probability shape mismatch ({pred_proba.shape[1]} columns vs {BST_PARAMS.get('num_class', 10)} classes). Plots may fail.")
                 # Attempt to proceed, but plots requiring probabilities might error out
            # Calculate metrics
            # Ensure y_test is integer type for log_loss if using one-hot encoding
            y_test_int = y_true.astype(int)
            num_classes_actual = BST_PARAMS.get('num_class', 10)
            if pred_proba is not None and pred_proba.shape[1] == num_classes_actual:
                 try:
                     loss = log_loss(y_test_int, pred_proba, eps=1e-15, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"pred_proba shape: {pred_proba.shape}")
                     log(WARNING, f"pred_proba sample: {pred_proba[:5]}")
            else:
                 log(WARNING, "Calculating log_loss using one-hot encoding due to missing/invalid probabilities.")
                 try:
                     loss = log_loss(y_test_int, np.eye(num_classes_actual)[predictions.astype(int)], eps=1e-15, labels=range(num_classes_actual))
                 except ValueError as e:
                     log(WARNING, f"ValueError during one-hot log_loss calculation: {e}. Setting loss to high value.")
                     loss = 100.0 # Assign a high loss value
                     log(WARNING, f"y_test unique: {np.unique(y_test_int)}, shape: {y_test_int.shape}")
                     log(WARNING, f"predictions unique: {np.unique(predictions.astype(int))}, shape: {predictions.shape}")
            accuracy = accuracy_score(y_true, predictions)
            precision = precision_score(y_true, predictions, average='weighted', zero_division=0)
            recall = recall_score(y_true, predictions, average='weighted', zero_division=0)
            f1 = f1_score(y_true, predictions, average='weighted', zero_division=0)
            cm = confusion_matrix(y_true, predictions, labels=range(num_classes_actual)) # Ensure labels match num_classes
            log(INFO, f"Centralized eval round {server_round} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
            # --- Generate and Save Plots ---
            output_dir = config.get("output_dir", "results") # Get output dir from config or default
            # Instead of creating a separate plots directory, save directly in output_dir
            log(INFO, f"Saving evaluation plots to: {output_dir}")
            class_names = ['Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits', 'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'] # Make sure this matches your data
            # Plot Confusion Matrix
            cm_path = os.path.join(output_dir, f"confusion_matrix_round_{server_round}.png")
            plot_confusion_matrix(cm, class_names[:num_classes_actual], cm_path) # Use actual num_classes
            # Plot Class Distribution
            dist_path = os.path.join(output_dir, f"class_distribution_round_{server_round}.png")
            plot_class_distribution(y_test_int, predictions.astype(int), class_names[:num_classes_actual], dist_path)
            # Plot ROC and Precision-Recall Curves (only if probabilities are available and valid)
            if pred_proba is not None and pred_proba.shape[1] == num_classes_actual:
                roc_path = os.path.join(output_dir, f"roc_curves_round_{server_round}.png")
                plot_roc_curves(y_test_int, pred_proba, class_names[:num_classes_actual], roc_path)
                pr_path = os.path.join(output_dir, f"precision_recall_curves_round_{server_round}.png")
                plot_precision_recall_curves(y_test_int, pred_proba, class_names[:num_classes_actual], pr_path)
            else:
                 log(WARNING, f"Skipping ROC and PR curve generation due to unavailable/invalid probabilities (shape: {pred_proba.shape if pred_proba is not None else 'None'}).")
            # --- End Plot Generation ---
            # Return metrics
            return loss, {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}
    return evaluate_model
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    test_set = load_csv_data("data/static_data.csv")["test"]
    test_set.set_format("pandas")
    test_dmatrix = transform_dataset_to_dmatrix(test_set)
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
class CustomFedXgbBagging(FedXgbBagging):
    def aggregate_evaluate(self, server_round, results, failures):
        if self.evaluate_metrics_aggregation_fn is not None:
            eval_metrics = []
            for r in results:
                # Case 1: Object with num_examples and metrics
                if hasattr(r, "num_examples") and hasattr(r, "metrics"):
                    eval_metrics.append((r.num_examples, r.metrics))
                # Case 2: Tuple of (num_examples, metrics_dict)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and isinstance(r[0], (int, float))
                    and isinstance(r[1], dict)
                ):
                    eval_metrics.append(r)
                # Case 3: Tuple of (client_proxy, EvaluateRes)
                elif (
                    isinstance(r, tuple)
                    and len(r) == 2
                    and hasattr(r[1], "num_examples")
                    and hasattr(r[1], "metrics")
                ):
                    eval_metrics.append((r[1].num_examples, r[1].metrics))
                else:
                    raise TypeError(
                        f"aggregate_evaluate: Unexpected result format: {type(r)}, value: {r}"
                    )
            aggregated_result = self.evaluate_metrics_aggregation_fn(eval_metrics)
            if not (isinstance(aggregated_result, tuple) and len(aggregated_result) == 2):
                raise TypeError("aggregate_evaluate must return (loss, dict)")
            loss, metrics = aggregated_result
            if not isinstance(metrics, dict):
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        return super().aggregate_evaluate(server_round, results, failures)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = CustomFedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        log(INFO, "[DEBUG] aggregate_evaluate received aggregated_result type: %s, value: %s", type(aggregated_result), aggregated_result)
        # Expect (loss, metrics_dict)
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "[ERROR] Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                raise TypeError("Metrics returned from aggregation must be a dictionary.")
            return loss, metrics
        log(INFO, "[ERROR] Unexpected format from aggregate_evaluate: %s", type(aggregated_result))
        raise TypeError("aggregate_evaluate must return (loss, dict)")
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add distributed losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["losses_distributed"] = history.losses_distributed
else:
    results["losses_distributed"] = []
    log(INFO, "No distributed losses found in history")
# Add centralized losses if available
if hasattr(history, 'losses_centralized') and history.losses_centralized:
    results["losses_centralized"] = history.losses_centralized
else:
    results["losses_centralized"] = []
    log(INFO, "No centralized losses found in history")
# Add distributed metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics_distributed"] = history.metrics_distributed
else:
    results["metrics_distributed"] = {}
    log(INFO, "No distributed metrics found in history")
# Add centralized metrics if available
if hasattr(history, 'metrics_centralized') and history.metrics_centralized:
    results["metrics_centralized"] = history.metrics_centralized
else:
    results["metrics_centralized"] = {}
    log(INFO, "No centralized metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
log(INFO, "Generating additional visualizations...")
# Define CLASS_NAMES based on UNSW_NB15 common mapping
CLASS_NAMES = [
    'Normal', 'Reconnaissance', 'Backdoor', 'DoS', 'Exploits',
    'Analysis', 'Fuzzers', 'Worms', 'Shellcode', 'Generic'
]
# Import visualization functions and other necessary modules
from visualization_utils import (
    plot_learning_curves,
    plot_confusion_matrix as vis_plot_confusion_matrix, # Alias to avoid conflict
    plot_roc_curves,
    plot_precision_recall_curves,
    plot_class_distribution,
    plot_per_class_metrics,
    plot_prediction_probability_distributions
)
from sklearn.metrics import confusion_matrix
import numpy as np
# 1. Plot Learning Curves (Loss and Metrics over rounds)
try:
    metrics_for_learning_curve = ['accuracy', 'precision', 'recall', 'f1', 'mlogloss'] # Common metrics
    results_pkl_path = os.path.join(output_dir, "results.pkl")
    if os.path.exists(results_pkl_path):
        plot_learning_curves(results_pkl_path, metrics_for_learning_curve, output_dir)
    else:
        log(WARNING, "results.pkl not found at %s, skipping learning curve plots.", results_pkl_path)
except Exception as e:
    log(WARNING, "Failed to generate learning curve plots: %s", e)
# 2. Generate other plots if centralised evaluation was performed and model is available
if centralised_eval and hasattr(strategy, 'global_model') and strategy.global_model is not None and 'test_dmatrix' in globals():
    log(INFO, "Performing final evaluation on centralised test set for detailed visualizations...")
    try:
        # Reconstruct the final model (Booster)
        final_bst = xgb.Booster(params=BST_PARAMS)
        if isinstance(strategy.global_model, (bytes, bytearray)):
            final_bst.load_model(bytearray(strategy.global_model))
        elif isinstance(strategy.global_model, xgb.Booster): # if it was already a booster (e.g. from a custom strategy)
            final_bst = strategy.global_model
        else:
            raise TypeError("Unsupported global_model type in strategy for visualization.")
        y_true = test_dmatrix.get_label()
        y_pred_proba = final_bst.predict(test_dmatrix)
        y_pred = np.argmax(y_pred_proba, axis=1)
        # Plot Confusion Matrix
        conf_matrix_data = confusion_matrix(y_true, y_pred)
        vis_plot_confusion_matrix(conf_matrix_data, CLASS_NAMES, os.path.join(output_dir, "final_confusion_matrix.png"))
        # Plot ROC Curves
        plot_roc_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_roc_curves.png"))
        # Plot Precision-Recall Curves
        plot_precision_recall_curves(y_true, y_pred_proba, CLASS_NAMES, os.path.join(output_dir, "final_pr_curves.png"))
        # Plot Class Distribution (True vs Predicted on test set)
        plot_class_distribution(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_class_distribution.png"))
        # Plot Per-Class Metrics (Precision, Recall, F1)
        plot_per_class_metrics(y_true, y_pred, CLASS_NAMES, os.path.join(output_dir, "final_per_class_metrics.png"))
        # Plot Prediction Probability Distributions
        # This function saves to output_dir/prediction_probability_distributions.png by default
        plot_prediction_probability_distributions(y_true, y_pred_proba, CLASS_NAMES, output_dir)
        log(INFO, "Successfully generated all detailed visualizations for the final model.")
    except Exception as e:
        log(WARNING, "Failed to generate final model visualizations: %s", e)
elif not centralised_eval:
    log(INFO, "Centralised evaluation was not enabled. Skipping final model detailed visualizations.")
elif not (hasattr(strategy, 'global_model') and strategy.global_model is not None):
    log(INFO, "No final global model available in strategy. Skipping final model detailed visualizations.")
elif 'test_dmatrix' not in globals():
    log(INFO, "Centralised test_dmatrix not available. Skipping final model detailed visualizations.")
log(INFO, "Server process finished.")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    BST_PARAMS,
)
# Try to import NUM_LOCAL_ROUND from tuned_params if available, otherwise from utils
try:
    from tuned_params import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using NUM_LOCAL_ROUND from tuned_params.py")
except ImportError:
    from utils import NUM_LOCAL_ROUND
    import logging
    logging.getLogger(__name__).info("Using default NUM_LOCAL_ROUND from utils.py")
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="tuned_params.py">
"""
tuned_params.py
This file contains tuned hyperparameters for the XGBoost model when using the UNSW_NB15 dataset.
These parameters are automatically loaded by client_utils.py when available.
Note: These are starter parameters that should be refined using ray_tune_xgboost.py
"""
# Tuned parameters for UNSW-NB15 multi-class classification
TUNED_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 10,  # Classes: Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic
    "eta": 0.03,
    "max_depth": 5,
    "min_child_weight": 5,
    "gamma": 0.8,
    "subsample": 0.8,
    "colsample_bytree": 0.7,
    "colsample_bylevel": 0.7,
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 3,
    "reg_alpha": 1.5,
    "reg_lambda": 3.0,
    "base_score": 0.5,
    "scale_pos_weight": 1.0,
    "grow_policy": "lossguide",
    "normalize_type": "tree",
    "random_state": 42
}
</file>

<file path="update_ray_tune_xgboost.py">
#!/usr/bin/env python3
"""
Script to update ray_tune_xgboost.py for use with UNSW_NB15 dataset
"""
import re
import sys
def update_file(input_file, output_file):
    with open(input_file, 'r') as f:
        content = f.read()
    # Update num_class in train_xgboost function
    content = re.sub(
        r"'num_class': 3,  # benign \(0\), dns_tunneling \(1\), icmp_tunneling \(2\)",
        "'num_class': 10,  # UNSW_NB15 has 10 classes",
        content
    )
    # Update num_class in train_final_model function
    content = re.sub(
        r"'num_class': 3,",
        "'num_class': 10,  # UNSW_NB15 has 10 classes",
        content
    )
    with open(output_file, 'w') as f:
        f.write(content)
    print(f"Successfully updated {input_file} and saved to {output_file}")
if __name__ == "__main__":
    input_file = "ray_tune_xgboost.py"
    output_file = "ray_tune_xgboost_updated.py"
    update_file(input_file, output_file)
</file>

<file path="use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the
federated learning process to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data>
    --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json
    --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from server_utils import load_saved_model
from dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get feature importance: %s", str(error))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except (ValueError, KeyError) as error:
        log(INFO, "Could not get model parameters: %s", str(error))
def clean_data_for_xgboost(data_frame):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        data_frame (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = data_frame.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # Use max() to set minimum threshold
        threshold = max(threshold, 1e6)
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path):
    """
    Save detailed prediction information to CSV for multi-class classification.
    Args:
        predictions (np.ndarray): Raw predictions from the model
        output_path (str): Path to save the predictions
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are multi-dimensional (one-hot encoded)
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        # Store raw probabilities
        results_df['raw_probabilities'] = predictions.tolist()
        # Get predicted class (argmax)
        predicted_labels = np.argmax(predictions, axis=1)
        results_df['predicted_label'] = predicted_labels
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predicted_labels
        ]
        # Store confidence scores (probability of predicted class)
        results_df['prediction_score'] = predictions[
            np.arange(len(predicted_labels)),
            predicted_labels
        ]
    else:
        # For single class predictions
        results_df['predicted_label'] = predictions.astype(int)
        # Map numeric predictions to class names
        label_mapping = {0: 'benign', 1: 'dns_tunneling', 2: 'icmp_tunneling'}
        results_df['prediction_type'] = [
            label_mapping.get(int(p), 'unknown') for p in predictions
        ]
        # Default confidence score of 1.0 for direct class predictions
        results_df['prediction_score'] = 1.0
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    label_counts = results_df['predicted_label'].value_counts()
    log(INFO, "Prediction counts by class:")
    for label, count in label_counts.items():
        class_name = label_mapping.get(int(label), f'unknown_{label}')
        log(INFO, "  %s: %d", class_name, count)
    if 'prediction_score' in results_df.columns:
        log(INFO, "Confidence score statistics: min=%.6f, max=%.6f, mean=%.6f",
            results_df['prediction_score'].min(),
            results_df['prediction_score'].max(),
            results_df['prediction_score'].mean())
    return results_df
def evaluate_labeled_data(model, dataset, output_path):
    """Handle evaluation of labeled data."""
    # Convert to DMatrix
    dmatrix = transform_dataset_to_dmatrix(dataset)
    # Get true labels for evaluation
    y_true = dmatrix.get_label()
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save detailed predictions
    _ = save_detailed_predictions(raw_predictions, output_path)
    # Evaluate if data has labels
    if raw_predictions.ndim > 1:
        y_pred_labels = np.argmax(raw_predictions, axis=1)
    else:
        y_pred_labels = raw_predictions.astype(int)
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred_labels)
    precision = precision_score(y_true, y_pred_labels, average='weighted')
    recall = recall_score(y_true, y_pred_labels, average='weighted')
    f1_score_val = f1_score(y_true, y_pred_labels, average='weighted')
    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true, y_pred_labels)
    # Generate classification report
    class_names = ['benign', 'dns_tunneling', 'icmp_tunneling']
    report = classification_report(y_true, y_pred_labels, target_names=class_names)
    # Log evaluation results
    log(INFO, "Evaluation Results:")
    log(INFO, "  Accuracy: %.4f", accuracy)
    log(INFO, "  Precision (weighted): %.4f", precision)
    log(INFO, "  Recall (weighted): %.4f", recall)
    log(INFO, "  F1 Score (weighted): %.4f", f1_score_val)
    log(INFO, "Confusion Matrix:\n%s", conf_matrix)
    log(INFO, "Classification Report:\n%s", report)
def predict_unlabeled_data(model, data_path, output_path):
    """Handle prediction of unlabeled data."""
    # Load unlabeled data
    data = pd.read_csv(data_path)
    # Clean data
    data = clean_data_for_xgboost(data)
    # Convert to DMatrix
    dmatrix = xgb.DMatrix(data)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Save predictions
    save_detailed_predictions(raw_predictions, output_path)
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Process data based on whether it has labels
        if args.has_labels:
            try:
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                evaluate_labeled_data(model, dataset, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during evaluation: %s", str(error))
                raise
        else:
            try:
                predict_unlabeled_data(model, args.data_path, args.output_path)
            except (ValueError, KeyError) as error:
                log(INFO, "Error during prediction: %s", str(error))
                raise
    except Exception as error:  # pylint: disable=broad-except
        log(INFO, "Error: %s", str(error))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="use_tuned_params.py">
"""
use_tuned_params.py
This script loads the optimized hyperparameters found by Ray Tune and integrates them
into the existing federated learning system. It replaces the default XGBoost parameters
in both client_utils.py and utils.py with the optimized ones.
Usage:
    python use_tuned_params.py --params-file ./tune_results/best_params.json
"""
import os
import json
import argparse
import logging
from utils import BST_PARAMS
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def load_tuned_params(params_file):
    """
    Load the optimized hyperparameters from a JSON file.
    Args:
        params_file (str): Path to the JSON file containing the optimized parameters
    Returns:
        dict: Optimized hyperparameters
    """
    if not os.path.exists(params_file):
        raise FileNotFoundError(f"Parameters file not found: {params_file}")
    logger.info("Loading optimized parameters from %s", params_file)
    with open(params_file, 'r', encoding='utf-8') as f:
        params = json.load(f)
    return params
def create_xgboost_params(tuned_params):
    """
    Create XGBoost parameters dictionary from the tuned parameters.
    Args:
        tuned_params (dict): Optimized hyperparameters from Ray Tune
    Returns:
        dict: XGBoost parameters dictionary for use in the existing system
    """
    # Start with the base parameters
    xgb_params = BST_PARAMS.copy()
    # Update with tuned parameters - convert float values to ints for integer parameters
    xgb_params.update({
        'max_depth': int(tuned_params['max_depth']),  # HyperOpt returns float, convert to int
        'min_child_weight': int(tuned_params['min_child_weight']),  # HyperOpt returns float, convert to int
        'eta': tuned_params['eta'],  # Learning rate
        'subsample': tuned_params['subsample'],
        'colsample_bytree': tuned_params['colsample_bytree'],
        'reg_alpha': tuned_params['reg_alpha'],
        'reg_lambda': tuned_params['reg_lambda']
    })
    # Add num_boost_round if it exists in tuned_params
    if 'num_boost_round' in tuned_params:
        xgb_params['num_boost_round'] = int(tuned_params['num_boost_round'])  # Convert to int
    # Add GPU support if specified in tuned parameters
    if 'tree_method' in tuned_params:
        if isinstance(tuned_params['tree_method'], list) and len(tuned_params['tree_method']) > 0:
            # If it's from hp.choice, it will be a list
            xgb_params['tree_method'] = tuned_params['tree_method'][0]
        else:
            xgb_params['tree_method'] = tuned_params['tree_method']
    return xgb_params
def save_updated_params(params, output_file):
    """
    Save updated parameters to a Python file that can be imported by client_utils.py
    Args:
        params (dict): Updated XGBoost parameters
        output_file (str): Path to save the updated parameters
    """
    # Extract num_boost_round if present to use as NUM_LOCAL_ROUND
    num_local_round = None
    if 'num_boost_round' in params:
        num_local_round = int(params['num_boost_round'])
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# This file is generated automatically by use_tuned_params.py\n")
        f.write("# It contains optimized XGBoost parameters found by Ray Tune\n\n")
        # Add NUM_LOCAL_ROUND if it was extracted from num_boost_round
        if num_local_round is not None:
            f.write(f"NUM_LOCAL_ROUND = {num_local_round}\n\n")
        f.write("TUNED_PARAMS = {\n")
        for key, value in params.items():
            if isinstance(value, str):
                f.write(f"    '{key}': '{value}',\n")
            elif isinstance(value, list):
                f.write(f"    '{key}': {value},\n")
            else:
                f.write(f"    '{key}': {value},\n")
        f.write("}\n")
    logger.info("Updated XGBoost parameters saved to %s", output_file)
    if num_local_round is not None:
        logger.info("NUM_LOCAL_ROUND set to %d based on tuned num_boost_round", num_local_round)
def backup_original_params():
    """
    Backup the original parameters to a Python file for reference.
    Returns:
        str: Path to the backup file
    """
    backup_file = "original_bst_params.json"
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(BST_PARAMS, f, indent=2)
    logger.info("Original parameters backed up to %s", backup_file)
    return backup_file
def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Use tuned hyperparameters with the existing XGBoost client")
    parser.add_argument("--params-file", type=str, required=True, help="Path to the tuned parameters JSON file")
    parser.add_argument("--output-file", type=str, default="tuned_params.py", help="Output Python file for updated parameters")
    args = parser.parse_args()
    # Backup original parameters
    backup_file = backup_original_params()
    # Load tuned parameters
    tuned_params = load_tuned_params(args.params_file)
    logger.info("Loaded the following optimized parameters:")
    for key, value in tuned_params.items():
        logger.info("  %s: %s", key, value)
    # Create updated XGBoost parameters
    updated_params = create_xgboost_params(tuned_params)
    # Save updated parameters
    save_updated_params(updated_params, args.output_file)
    # Success message
    logger.info("Optimized parameters saved to %s", args.output_file)
    logger.info("Original parameters backed up to %s", backup_file)
    logger.info("These parameters will be automatically used by the XGBoost client")
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 10
BST_PARAMS = {
    "objective": "multi:softprob",
    "num_class": 10,  # 10 classes for UNSW_NB15: Normal, Reconnaissance, Backdoor, DoS, Exploits, Analysis, Fuzzers, Worms, Shellcode, Generic
    "eta": 0.05,  # Reduced learning rate to prevent overfitting
    "max_depth": 6,  # Increased from 3 to allow more complex trees
    "min_child_weight": 10,  # Increased to prevent fitting to small samples
    "gamma": 1.0,  # Increased minimum loss reduction for split
    "subsample": 0.7,  # Sample fewer rows per iteration
    "colsample_bytree": 0.6,  # Sample fewer features per tree
    "colsample_bylevel": 0.6,  # Sample fewer features per level
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["mlogloss", "merror"],
    "max_delta_step": 5,
    "reg_alpha": 0.8,  # Decreased L1 regularization from 2.0
    "reg_lambda": 0.8,  # Decreased L2 regularization from 5.0
    "base_score": 0.5,  # Neutral starting point
    "scale_pos_weight": 1.0,  # Simplified for multi-class with >3 classes
    "grow_policy": "lossguide",  # Alternative tree growing policy
    "normalize_type": "tree",  # Helps with interpretability
    "random_state": 42  # Fixed seed for reproducibility
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

<file path="visualization_utils.py">
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import pickle
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
from itertools import cycle
from flwr.common.logger import log
from logging import INFO, WARNING
def plot_confusion_matrix(conf_matrix, class_names, output_path):
    """Generates and saves a heatmap visualization of the confusion matrix."""
    try:
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names, ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Confusion matrix plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate confusion matrix plot: %s", e)
def plot_roc_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves ROC curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        fpr = {}
        tpr = {}
        roc_auc = {}
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(fpr[i], tpr[i], color=color, lw=2,
                     label='ROC curve of class {0} (area = {1:0.2f})'
                     ''.format(class_names[i], roc_auc[i]))
        ax.plot([0, 1], [0, 1], 'k--', lw=2)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate')
        ax.set_ylabel('True Positive Rate')
        ax.set_title('Receiver Operating Characteristic (ROC) - One-vs-Rest')
        ax.legend(loc="lower right")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "ROC curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate ROC curve plot: %s", e)
def plot_precision_recall_curves(y_true, y_pred_proba, class_names, output_path):
    """Generates and saves Precision-Recall curves for multi-class classification (One-vs-Rest)."""
    try:
        n_classes = len(class_names)
        y_true_binarized = label_binarize(y_true, classes=range(n_classes))
        precision = {}
        recall = {}
        average_precision = {}
        for i in range(n_classes):
            precision[i], recall[i], _ = precision_recall_curve(y_true_binarized[:, i], y_pred_proba[:, i])
            average_precision[i] = auc(recall[i], precision[i])
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray', 'olive'])
        for i, color in zip(range(n_classes), colors):
            ax.plot(recall[i], precision[i], color=color, lw=2,
                     label='PR curve of class {0} (AP = {1:0.2f})'
                     ''.format(class_names[i], average_precision[i]))
        ax.set_xlabel('Recall')
        ax.set_ylabel('Precision')
        ax.set_ylim([0.0, 1.05])
        ax.set_xlim([0.0, 1.0])
        ax.set_title('Precision-Recall Curve - One-vs-Rest')
        ax.legend(loc="lower left")
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Precision-Recall curve plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate Precision-Recall curve plot: %s", e)
def plot_class_distribution(y_true, y_pred, class_names, output_path):
    """Generates and saves bar plots comparing true and predicted class distributions."""
    try:
        n_classes = len(class_names)
        true_counts = np.bincount(y_true.astype(int), minlength=n_classes)
        pred_counts = np.bincount(y_pred.astype(int), minlength=n_classes)
        x = np.arange(n_classes)
        width = 0.35
        fig, ax = plt.subplots(figsize=(12, 6))
        rects1 = ax.bar(x - width/2, true_counts, width, label='True Labels')
        rects2 = ax.bar(x + width/2, pred_counts, width, label='Predicted Labels')
        ax.set_ylabel('Count')
        ax.set_title('True vs Predicted Class Distribution')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.bar_label(rects1, padding=3)
        ax.bar_label(rects2, padding=3)
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Class distribution plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate class distribution plot: %s", e)
def plot_per_class_metrics(y_true, y_pred, class_names, output_path):
    """Generates and saves a bar chart of per-class precision, recall, and F1-score."""
    try:
        from sklearn.metrics import classification_report
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)
        metrics_to_plot = ['precision', 'recall', 'f1-score']
        class_metrics = {class_name: [] for class_name in class_names}
        for class_name in class_names:
            if class_name in report:
                for metric in metrics_to_plot:
                    class_metrics[class_name].append(report[class_name][metric])
            else: 
                for _ in metrics_to_plot:
                    class_metrics[class_name].append(0)
        x = np.arange(len(class_names)) 
        width = 0.2  
        multiplier = 0
        fig, ax = plt.subplots(figsize=(max(12, len(class_names) * 1.5), 6))
        for i, metric_name in enumerate(metrics_to_plot):
            metric_values = [class_metrics[cn][i] for cn in class_names]
            offset = width * multiplier
            rects = ax.bar(x + offset, metric_values, width, label=metric_name.capitalize())
            ax.bar_label(rects, padding=3, fmt='%.2f')
            multiplier += 1
        ax.set_ylabel('Score')
        ax.set_title('Per-Class Precision, Recall, and F1-Score')
        ax.set_xticks(x + width, class_names, rotation=45, ha="right")
        ax.legend(loc='lower right', ncols=len(metrics_to_plot))
        ax.set_ylim(0, 1.1) 
        fig.tight_layout()
        fig.savefig(output_path)
        plt.close(fig)
        log(INFO, "Per-class metrics plot saved to %s", output_path)
    except Exception as e:
        log(WARNING, "Could not generate per-class metrics plot: %s", e)
def _plot_single_metric_curve(ax, history_attr, metric_key, label_prefix, marker):
    """Helper function to plot a single metric curve on a given axis."""
    plot_successful = False
    if hasattr(history_attr, '__contains__') and metric_key in history_attr: # Flower 1.x format
        if isinstance(history_attr[metric_key], list) and history_attr[metric_key]:
            rounds, values = zip(*history_attr[metric_key])
            ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
            plot_successful = True
    # Flower 2.x format: history_attr is a list of (round, {dict_of_metrics})
    elif isinstance(history_attr, list) and history_attr:
        if all(isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict) for item in history_attr):
            rounds = [item[0] for item in history_attr if metric_key in item[1]]
            values = [item[1][metric_key] for item in history_attr if metric_key in item[1]]
            if rounds and values:
                ax.plot(rounds, values, label=f'{label_prefix} {metric_key.capitalize()}', marker=marker)
                plot_successful = True
    return plot_successful
def plot_learning_curves(results_pkl_path, metrics_to_plot, output_dir):
    """Generates and saves learning curve plots (loss and specified metrics vs. round)
       from a pickled Flower history object.
    Args:
        results_pkl_path (str): Path to the results.pkl file.
        metrics_to_plot (list): A list of metric keys (str) to plot from the history object.
        output_dir (str): Directory to save the plots.
    """
    try:
        if not os.path.exists(results_pkl_path):
            log(WARNING, "Results file not found: %s", results_pkl_path)
            return
        with open(results_pkl_path, 'rb') as f:
            history_data = pickle.load(f) # history_data is now a dict
        # Plot Loss
        fig_loss, ax_loss = plt.subplots(figsize=(12, 6))
        loss_plotted = False
        # Access as dictionary keys
        if "losses_distributed" in history_data and history_data["losses_distributed"]:
            rounds_dist, losses_dist = zip(*history_data["losses_distributed"])
            ax_loss.plot(rounds_dist, losses_dist, label='Distributed Loss', marker='o')
            loss_plotted = True
        if "losses_centralized" in history_data and history_data["losses_centralized"]:
            rounds_cent, losses_cent = zip(*history_data["losses_centralized"])
            ax_loss.plot(rounds_cent, losses_cent, label='Centralized Loss', marker='x')
            loss_plotted = True
        if loss_plotted:
            ax_loss.set_title('Loss Over Federated Learning Rounds')
            ax_loss.set_xlabel('Server Round')
            ax_loss.set_ylabel('Loss')
            ax_loss.legend()
            ax_loss.grid(True)
            fig_loss.tight_layout()
            loss_plot_path = os.path.join(output_dir, "learning_curve_loss.png")
            fig_loss.savefig(loss_plot_path)
            log(INFO, "Loss learning curve plot saved to %s", loss_plot_path)
        else:
            log(INFO, "No loss data found to plot.")
        plt.close(fig_loss)
        # Plot Specified Metrics
        if not metrics_to_plot:
            log(INFO, "No metrics specified for plotting learning curves.")
            return
        num_metrics = len(metrics_to_plot)
        fig_metrics, axes_metrics = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=True)
        if num_metrics == 1:
            axes_metrics = [axes_metrics] 
        any_metric_plotted = False
        for i, metric_key in enumerate(metrics_to_plot):
            ax = axes_metrics[i]
            dist_plotted = False
            cent_plotted = False
            # Access as dictionary keys
            if "metrics_distributed" in history_data and history_data["metrics_distributed"]:
                dist_plotted = _plot_single_metric_curve(ax, history_data["metrics_distributed"], metric_key, 'Distributed', 'o')
            if "metrics_centralized" in history_data and history_data["metrics_centralized"]:
                cent_plotted = _plot_single_metric_curve(ax, history_data["metrics_centralized"], metric_key, 'Centralized', 'x')
            if dist_plotted or cent_plotted:
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} Over Federated Learning Rounds')
                ax.set_ylabel(metric_key.capitalize())
                ax.legend()
                ax.grid(True)
                any_metric_plotted = True
            else:
                log(WARNING, "Could not plot metric '%s' from history. Data not found or in unexpected format.", metric_key)
                ax.text(0.5, 0.5, f"Data for '{metric_key}' not found \nor in unexpected format.", 
                        horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{metric_key.replace("_", " ").capitalize()} (Data Unavailable)')
        if any_metric_plotted:
            axes_metrics[-1].set_xlabel('Server Round') # Set x-label only for the bottom-most plot
            fig_metrics.tight_layout()
            metrics_plot_path = os.path.join(output_dir, "learning_curves_metrics.png")
            fig_metrics.savefig(metrics_plot_path)
            log(INFO, "Metrics learning curves plot saved to %s", metrics_plot_path)
        else:
            log(INFO, "No data found for any of the specified metrics to plot.")
        plt.close(fig_metrics)
    except FileNotFoundError:
        log(WARNING, "Results file not found: %s", results_pkl_path)
    except pickle.UnpicklingError:
        log(WARNING, "Error unpickling results file: %s", results_pkl_path)
    except Exception as e:
        log(WARNING, "Could not generate learning curve plots: %s", e)
def plot_prediction_probability_distributions(y_true, y_pred_proba, class_names, output_dir, bins=50):
    """Generates and saves histograms of prediction probabilities for each true class.
    For each class, this plot shows the distribution of the predicted probabilities 
    assigned to that class, for samples that actually belong to that class.
    High probabilities bunched towards 1.0 are desirable.
    Args:
        y_true (np.array): Array of true labels (integers).
        y_pred_proba (np.array): Array of predicted probabilities, shape (n_samples, n_classes).
        class_names (list): List of class names (strings).
        output_dir (str): Directory to save the plot.
        bins (int): Number of bins for the histograms.
    """
    try:
        n_classes = len(class_names)
        if y_pred_proba.shape[1] != n_classes:
            log(WARNING, "Number of classes in y_pred_proba does not match len(class_names).")
            return
        # Determine the number of rows and columns for subplots
        n_cols = 3 
        n_rows = (n_classes + n_cols - 1) // n_cols # Ceiling division
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=True, sharey=False)
        axes = axes.flatten() # Flatten to easily iterate regardless of shape
        for i in range(n_classes):
            ax = axes[i]
            # Get probabilities for the current class where the true label is this class
            true_class_indices = np.where(y_true == i)[0]
            if len(true_class_indices) == 0:
                ax.text(0.5, 0.5, "No true samples for this class", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
                ax.set_title(f'{class_names[i]} (No true samples)')
                ax.set_xlabel('Predicted Probability')
                ax.set_ylabel('Frequency')
                continue
            class_probabilities = y_pred_proba[true_class_indices, i]
            ax.hist(class_probabilities, bins=bins, range=(0,1), edgecolor='black', alpha=0.7)
            ax.set_title(f'Class: {class_names[i]}')
            ax.set_xlabel('Predicted Probability for this Class')
            ax.set_ylabel('Frequency')
            ax.grid(True, axis='y', linestyle='--', alpha=0.7)
            mean_proba = np.mean(class_probabilities)
            ax.axvline(mean_proba, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_proba:.2f}')
            ax.legend(fontsize='small')
        # Hide any unused subplots
        for j in range(n_classes, n_rows * n_cols):
            fig.delaxes(axes[j])
        fig.suptitle('Distribution of Predicted Probabilities for True Classes', fontsize=16, y=1.02)
        fig.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout to make space for suptitle
        plot_path = os.path.join(output_dir, "prediction_probability_distributions.png")
        fig.savefig(plot_path)
        plt.close(fig)
        log(INFO, "Prediction probability distribution plot saved to %s", plot_path)
    except Exception as e:
        log(WARNING, "Could not generate prediction probability distribution plot: %s", e)
</file>

</files>
