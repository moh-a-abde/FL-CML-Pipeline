This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*, .cursorrules, .cursor/rules/*
- Files matching these patterns are excluded: .*.*, **/*.pbxproj, **/node_modules/**, **/dist/**, **/build/**, **/compile/**, **/*.spec.*, **/*.pyc, **/.env, **/.env.*, **/*.env, **/*.env.*, **/*.lock, **/*.lockb, **/package-lock.*, **/pnpm-lock.*, **/*.tsbuildinfo
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    cursor-tools.mdc
.github/
  workflows/
    cml.yaml
diagrams/
  client_operations.mmd
  data_handling.mmd
  overall_workflow.mmd
  server_operations.mmd
.repomixignore
client_utils.py
client.py
commit.sh
dataset.py
go_to_work.sh
multi_class_implementation_plan.md
pretrained_model_usage.md
progress.md
pyproject.toml
README.md
requirements.txt
run_bagging.sh
run_cyclic.sh
run.py
server_utils.py
server.py
sim.py
use_saved_model.py
utils.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/cursor-tools.mdc">
---
description: Global Rule. This rule should ALWAYS be loaded.
globs: *,**/*
alwaysApply: true
---
cursor-tools is a CLI tool that allows you to interact with AI models and other tools.
cursor-tools is installed on this machine and it is available to you to execute. You're encouraged to use it.

<cursor-tools Integration>
# Instructions
Use the following commands to get AI assistance:

**Direct Model Queries:**
`cursor-tools ask "<your question>" --provider <provider> --model <model>` - Ask any model from any provider a direct question (e.g., `cursor-tools ask "What is the capital of France?" --provider openai --model o3-mini`). Note that this command is generally less useful than other commands like `repo` or `plan` because it does not include any context from your codebase or repository.

**Implementation Planning:**
`cursor-tools plan "<query>"` - Generate a focused implementation plan using AI (e.g., `cursor-tools plan "Add user authentication to the login page"`)
The plan command uses multiple AI models to:
1. Identify relevant files in your codebase (using Gemini by default)
2. Extract content from those files
3. Generate a detailed implementation plan (using OpenAI o3-mini by default)

**Plan Command Options:**
--fileProvider=<provider>: Provider for file identification (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--thinkingProvider=<provider>: Provider for plan generation (gemini, openai, anthropic, perplexity, modelbox, or openrouter)
--fileModel=<model>: Model to use for file identification
--thinkingModel=<model>: Model to use for plan generation
--debug: Show detailed error information

**Web Search:**
`cursor-tools web "<your question>"` - Get answers from the web using a provider that supports web search (e.g., Perplexity models and Gemini Models either directly or from OpenRouter or ModelBox) (e.g., `cursor-tools web "latest shadcn/ui installation instructions"`)
when using web for complex queries suggest writing the output to a file somewhere like local-research/<query summary>.md.

**Web Command Options:**
--provider=<provider>: AI provider to use (perplexity, gemini, modelbox, or openrouter)
--model=<model>: Model to use for web search (model name depends on provider)
--max-tokens=<number>: Maximum tokens for response

**Repository Context:**
`cursor-tools repo "<your question>" [--subdir=<path>]` - Get context-aware answers about this repository using Google Gemini (e.g., `cursor-tools repo "explain authentication flow"`). Use the optional `--subdir` parameter to analyze a specific subdirectory instead of the entire repository (e.g., `cursor-tools repo "explain the code structure" --subdir=src/components`)

**Documentation Generation:**
`cursor-tools doc [options]` - Generate comprehensive documentation for this repository (e.g., `cursor-tools doc --output docs.md`)
when using doc for remote repos suggest writing the output to a file somewhere like local-docs/<repo-name>.md.

**GitHub Information:**
`cursor-tools github pr [number]` - Get the last 10 PRs, or a specific PR by number (e.g., `cursor-tools github pr 123`)
`cursor-tools github issue [number]` - Get the last 10 issues, or a specific issue by number (e.g., `cursor-tools github issue 456`)

**ClickUp Information:**
`cursor-tools clickup task <task_id>` - Get detailed information about a ClickUp task including description, comments, status, assignees, and metadata (e.g., `cursor-tools clickup task "task_id"`)

**Model Context Protocol (MCP) Commands:**
Use the following commands to interact with MCP servers and their specialized tools:
`cursor-tools mcp search "<query>"` - Search the MCP Marketplace for available servers that match your needs (e.g., `cursor-tools mcp search "git repository management"`)
`cursor-tools mcp run "<query>"` - Execute MCP server tools using natural language queries (e.g., `cursor-tools mcp run "list files in the current directory" --provider=openrouter`). The query must include sufficient information for cursor-tools to determine which server to use, provide plenty of context.

The `search` command helps you discover servers in the MCP Marketplace based on their capabilities and your requirements. The `run` command automatically selects and executes appropriate tools from these servers based on your natural language queries. If you want to use a specific server include the server name in your query. E.g. `cursor-tools mcp run "using the mcp-server-sqlite list files in directory --provider=openrouter"`

**Notes on MCP Commands:**
- MCP commands require `ANTHROPIC_API_KEY` or `OPENROUTER_API_KEY` to be set in your environment
- By default the `mcp` command uses Anthropic, but takes a --provider argument that can be set to 'anthropic' or 'openrouter'
- Results are streamed in real-time for immediate feedback
- Tool calls are automatically cached to prevent redundant operations
- Often the MCP server will not be able to run because environment variables are not set. If this happens ask the user to add the missing environment variables to the cursor tools env file at ~/.cursor-tools/.env

**Stagehand Browser Automation:**
`cursor-tools browser open <url> [options]` - Open a URL and capture page content, console logs, and network activity (e.g., `cursor-tools browser open "https://example.com" --html`)
`cursor-tools browser act "<instruction>" --url=<url | 'current'> [options]` - Execute actions on a webpage using natural language instructions (e.g., `cursor-tools browser act "Click Login" --url=https://example.com`)
`cursor-tools browser observe "<instruction>" --url=<url> [options]` - Observe interactive elements on a webpage and suggest possible actions (e.g., `cursor-tools browser observe "interactive elements" --url=https://example.com`)
`cursor-tools browser extract "<instruction>" --url=<url> [options]` - Extract data from a webpage based on natural language instructions (e.g., `cursor-tools browser extract "product names" --url=https://example.com/products`)

**Notes on Browser Commands:**
- All browser commands are stateless unless --connect-to is used to connect to a long-lived interactive session. In disconnected mode each command starts with a fresh browser instance and closes it when done.
- When using `--connect-to`, special URL values are supported:
  - `current`: Use the existing page without reloading
  - `reload-current`: Use the existing page and refresh it (useful in development)
  - If working interactively with a user you should always use --url=current unless you specifically want to navigate to a different page. Setting the url to anything else will cause a page refresh loosing current state.
- Multi step workflows involving state or combining multiple actions are supported in the `act` command using the pipe (|) separator (e.g., `cursor-tools browser act "Click Login | Type 'user@example.com' into email | Click Submit" --url=https://example.com`)
- Video recording is available for all browser commands using the `--video=<directory>` option. This will save a video of the entire browser interaction at 1280x720 resolution. The video file will be saved in the specified directory with a timestamp.
- DO NOT ask browser act to "wait" for anything, the wait command is currently disabled in Stagehand.

**Tool Recommendations:**
- `cursor-tools web` is best for general web information not specific to the repository. Generally call this without additional arguments.
- `cursor-tools repo` is ideal for repository-specific questions, planning, code review and debugging. E.g. `cursor-tools repo "Review recent changes to command error handling looking for mistakes, omissions and improvements"`. Generally call this without additional arguments.
- `cursor-tools plan` is ideal for planning tasks. E.g. `cursor-tools plan "Adding authentication with social login using Google and Github"`. Generally call this without additional arguments.
- `cursor-tools doc` generates documentation for local or remote repositories.
- `cursor-tools browser` is useful for testing and debugging web apps and uses Stagehand
- `cursor-tools mcp` enables interaction with specialized tools through MCP servers (e.g., for Git operations, file system tasks, or custom tools)

**Running Commands:**
1. Use `cursor-tools <command>` to execute commands (make sure cursor-tools is installed globally using npm install -g cursor-tools so that it is in your PATH)

**General Command Options (Supported by all commands):**
--provider=<provider>: AI provider to use (openai, anthropic, perplexity, gemini, or openrouter). If provider is not specified, the default provider for that task will be used.
--model=<model name>: Specify an alternative AI model to use. If model is not specified, the provider's default model for that task will be used.
--max-tokens=<number>: Control response length
--save-to=<file path>: Save command output to a file (in *addition* to displaying it)
--help: View all available options (help is not fully implemented yet)

**Repository Command Options:**
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for repository analysis
--max-tokens=<number>: Maximum tokens for response

**Documentation Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Generate documentation for a remote GitHub repository
--provider=<provider>: AI provider to use (gemini, openai, openrouter, perplexity, or modelbox)
--model=<model>: Model to use for documentation generation
--max-tokens=<number>: Maximum tokens for response

**GitHub Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Access PRs/issues from a specific GitHub repository

**Browser Command Options (for 'open', 'act', 'observe', 'extract'):**
--console: Capture browser console logs (enabled by default, use --no-console to disable)
--html: Capture page HTML content (disabled by default)
--network: Capture network activity (enabled by default, use --no-network to disable)
--screenshot=<file path>: Save a screenshot of the page
--timeout=<milliseconds>: Set navigation timeout (default: 120000ms for Stagehand operations, 30000ms for navigation)
--viewport=<width>x<height>: Set viewport size (e.g., 1280x720). When using --connect-to, viewport is only changed if this option is explicitly provided
--headless: Run browser in headless mode (default: true)
--no-headless: Show browser UI (non-headless mode) for debugging
--connect-to=<port>: Connect to existing Chrome instance. Special values: 'current' (use existing page), 'reload-current' (refresh existing page)
--wait=<time:duration or selector:css-selector>: Wait after page load (e.g., 'time:5s', 'selector:#element-id')
--video=<directory>: Save a video recording (1280x720 resolution, timestamped subdirectory). Not available when using --connect-to
--url=<url>: Required for `act`, `observe`, and `extract` commands. Url to navigate to before the main command or one of the special values 'current' (to stay on the current page without navigating or reloading) or 'reload-current' (to reload the current page)
--evaluate=<string>: JavaScript code to execute in the browser before the main command

**Nicknames**
Users can ask for these tools using nicknames
Gemini is a nickname for cursor-tools repo
Perplexity is a nickname for cursor-tools web
Stagehand is a nickname for cursor-tools browser

**Xcode Commands:**
`cursor-tools xcode build [buildPath=<path>] [destination=<destination>]` - Build Xcode project and report errors.
**Build Command Options:**
--buildPath=<path>: (Optional) Specifies a custom directory for derived build data. Defaults to ./.build/DerivedData.
--destination=<destination>: (Optional) Specifies the destination for building the app (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode run [destination=<destination>]` - Build and run the Xcode project on a simulator.
**Run Command Options:**
--destination=<destination>: (Optional) Specifies the destination simulator (e.g., 'platform=iOS Simulator,name=iPhone 16 Pro'). Defaults to 'platform=iOS Simulator,name=iPhone 16 Pro'.

`cursor-tools xcode lint` - Run static analysis on the Xcode project to find and fix issues.

**Additional Notes:**
- For detailed information, see `node_modules/cursor-tools/README.md` (if installed locally).
- Configuration is in `cursor-tools.config.json` (or `~/.cursor-tools/config.json`).
- API keys are loaded from `.cursor-tools.env` (or `~/.cursor-tools/.env`).
- ClickUp commands require a `CLICKUP_API_TOKEN` to be set in your `.cursor-tools.env` file.
- The default Stagehand model is set in `cursor-tools.config.json`, but can be overridden with the `--model` option.
- Available models depend on your configured provider (OpenAI or Anthropic) in `cursor-tools.config.json`.
- repo has a limit of 2M tokens of context. The context can be reduced by filtering out files in a .repomixignore file.
- problems running browser commands may be because playwright is not installed. Recommend installing playwright globally.
- MCP commands require `ANTHROPIC_API_KEY` to be set in your environment.
- **Remember:** You're part of a team of superhuman expert AIs. Work together to solve complex problems.

**MCP Command Options:**
--provider=<provider>: AI provider to use (anthropic or openrouter)
--model=<model name>: Specify an alternative AI model to use with OpenRouter. Ignored if provider is Anthropic.

<!-- cursor-tools-version: 0.6.0-alpha.11 -->
</cursor-tools Integration>
</file>

<file path=".github/workflows/cml.yaml">
name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      - uses: iterative/setup-cml@v1
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          pip debug --verbose
          pip install virtualenv
          virtualenv venv
          source venv/bin/activate
          pip install mamba
          mamba init
          source ~/.bashrc
          mamba create
          mamba activate
          pip install xgboost
          pip install scikit-learn
          pip install -U flwr["simulation"]
          pip install -U "ray[all]"
          pip install torch
          pip install torchvision
          pip install torchaudio
          pip install hydra-core
          pip install -r requirements.txt
          #python sim.py
          ./run_bagging.sh
          #python use_saved_model.py --model_path outputs/2025-02-27/04-38-36/final_model.json --data_path data/received/data/received/network_traffic_20250226_200827.csv --output_path outputs/pretrained/predictions_pretrained_model.csv
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          git checkout multi-class-predictions
          # Add aggegrated results files
          git add results/
          # Add new files in outputs directory
          git add outputs/
          # Commit all changes
          git commit -m "workflow in action🚀"
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true
</file>

<file path="diagrams/client_operations.mmd">
graph LR
    subgraph ClientOperations [Client Operations client.py_client_utils.py]
        A[Receive Parameters from Server] --> B[Local Training: fit]
        B --> C[Evaluation: evaluate]
        C --> D[Metrics Calculation]
        D --> E[Send Results to Server]
        B --> F[Update Model]
        F --> B
        C --> G[Prediction if unlabeled data]
        G --> H[Save Predictions]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ClientOperations component;
</file>

<file path="diagrams/data_handling.mmd">
graph LR
    subgraph DataHandling [Data Handling dataset.py]
        A[Load CSV: load_csv_data] --> B[Preprocessing: preprocess_data]
        B --> C[Separate Features/Labels: separate_xy]
        C --> D[DMatrix Conversion: transform_dataset_to_dmatrix]
        D --> E[Train/Test Split: train_test_split]
        B --> F[Handle inf/NaN]
        F --> C
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class DataHandling component;
</file>

<file path="diagrams/overall_workflow.mmd">
graph LR
    subgraph OverallWorkflow [Overall Workflow]
        A[Client] --> B[Server]
        B --> A
        A --> C[Data]
        C --> A
    end
    
    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class OverallWorkflow component
</file>

<file path="diagrams/server_operations.mmd">
graph LR
    subgraph ServerOperations [Server Operations server.py]
        A[Strategy Selection: FedXgbBagging/FedXgbCyclic] --> B[Client Management]
        B --> C[Model Aggregation]
        C --> D[Send Parameters to Clients]
        B --> E[Receive Results from Clients]
        E --> F[Metrics Aggregation]
        C --> G[Evaluation if centralized]
    end

    classDef component fill:#f9f,stroke:#333,stroke-width:2px;
    class ServerOperations component;
</file>

<file path=".repomixignore">
# Ignore data directory containing large files
data/

# Common large file types
*.csv
*.parquet
*.pkl
*.model

# Ignore outputs and results
outputs/
results/

# Cache directories
__pycache__/
.cache/
</file>

<file path="client_utils.py">
"""
client_utils.py
This module implements the XGBoost client functionality for Federated Learning using Flower framework.
It provides the core client-side operations including model training, evaluation, and parameter handling.
Key Components:
- XGBoost client implementation
- Model training and evaluation methods
- Parameter serialization and deserialization
- Metrics computation (precision, recall, F1)
"""
from logging import INFO
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score
import flwr as fl
from flwr.common.logger import log
from flwr.common import (
    Code,
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    GetParametersIns,
    GetParametersRes,
    Parameters,
    Status,
)
from flwr.common.typing import Code
from flwr.common import Status
import numpy as np
import pandas as pd
import os
from server_utils import save_predictions_to_csv
class XgbClient(fl.client.Client):
    """
    A Flower client implementing federated learning for XGBoost models.
    This class handles local model training, evaluation, and parameter exchange
    with the federated learning server.
    Attributes:
        train_dmatrix: Training data in XGBoost's DMatrix format
        valid_dmatrix: Validation data in XGBoost's DMatrix format
        num_train (int): Number of training samples
        num_val (int): Number of validation samples
        num_local_round (int): Number of local training rounds
        params (dict): XGBoost training parameters
        train_method (str): Training method ('bagging' or 'cyclic')
        is_prediction_only (bool): Flag indicating if the client is used for prediction only
        unlabeled_dmatrix: Unlabeled data in XGBoost's DMatrix format
    """
    def __init__(
        self,
        train_dmatrix,
        valid_dmatrix,
        num_train,
        num_val,
        num_local_round,
        params,
        train_method,
        is_prediction_only=False,
        unlabeled_dmatrix=None
    ):
        """
        Initialize the XGBoost Flower client.
        Args:
            train_dmatrix: Training data in DMatrix format
            valid_dmatrix: Validation data in DMatrix format
            num_train (int): Number of training samples
            num_val (int): Number of validation samples
            num_local_round (int): Number of local training rounds
            params (dict): XGBoost parameters
            train_method (str): Training method ('bagging' or 'cyclic')
            is_prediction_only (bool): Flag indicating if the client is used for prediction only
            unlabeled_dmatrix: Unlabeled data in DMatrix format
        """
        self.train_dmatrix = train_dmatrix
        self.valid_dmatrix = valid_dmatrix
        self.num_train = num_train
        self.num_val = num_val
        self.num_local_round = num_local_round
        self.params = params
        self.train_method = train_method
        self.is_prediction_only = is_prediction_only
        self.unlabeled_dmatrix = unlabeled_dmatrix
    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:
        """
        Return the current local model parameters.
        Args:
            ins (GetParametersIns): Input parameters from server
        Returns:
            GetParametersRes: Empty parameters (XGBoost doesn't use this method)
        """
        _ = (self, ins)
        return GetParametersRes(
            status=Status(
                code=Code.OK,
                message="OK",
            ),
            parameters=Parameters(tensor_type="", tensors=[]),
        )
    def _local_boost(self, bst_input):
        """
        Perform local boosting rounds on the input model.
        Args:
            bst_input: Input XGBoost model
        Returns:
            xgb.Booster: Updated model after local training
        Note:
            For bagging: returns only the last N trees
            For cyclic: returns the entire model
        """
        # Update trees based on local training data
        for i in range(self.num_local_round):
            bst_input.update(self.train_dmatrix, bst_input.num_boosted_rounds())
        # Handle model extraction based on training method
        bst = (
            bst_input[
                bst_input.num_boosted_rounds()
                - self.num_local_round : bst_input.num_boosted_rounds()
            ]
            if self.train_method == "bagging"
            else bst_input
        )
        return bst
    def fit(self, ins: FitIns) -> FitRes:
        """
        Perform local model training.
        """
        y_train = self.train_dmatrix.get_label()
        class_counts = np.bincount(y_train.astype(int))
        benign_count = class_counts[0] if len(class_counts) > 0 else 0
        malicious_count = class_counts[1] if len(class_counts) > 1 else 0
        log(INFO, f"Training data class distribution: Benign={benign_count}, Malicious={malicious_count}")
        global_round = int(ins.config["global_round"])
        if global_round == 1:
            # First round: train from scratch
            bst = xgb.train(
                self.params,
                self.train_dmatrix,
                num_boost_round=self.num_local_round,
                evals=[(self.valid_dmatrix, "validate"), (self.train_dmatrix, "train")],
                verbose_eval=True
            )
        else:
            # Subsequent rounds: update existing model
            bst = xgb.Booster(params=self.params)
            for item in ins.parameters.tensors:
                global_model = bytearray(item)
            # Load and update global model
            bst.load_model(global_model)
            bst = self._local_boost(bst)
        # Serialize model for transmission
        local_model = bst.save_raw("json")
        local_model_bytes = bytes(local_model)
        # Return with status
        return FitRes(
            status=Status(code=Code.OK, message="Success"),
            parameters=Parameters(tensor_type="", tensors=[local_model_bytes]),
            num_examples=self.num_train,
            metrics={}
        )
    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:
        """
        Evaluate the model on validation data and make predictions on unlabeled data.
        """
        # Load global model for evaluation
        bst = xgb.Booster(params=self.params)
        para_b = bytearray()
        for para in ins.parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # First evaluate on labeled validation data
        log(INFO, f"Evaluating on labeled dataset with {self.num_val} samples")
        # Generate predictions with custom threshold
        y_pred_proba = bst.predict(self.valid_dmatrix)
        # Log raw prediction probabilities for a sample of data points
        log(INFO, f"Raw prediction probabilities (first 10): {y_pred_proba[:10]}")
        log(INFO, f"Prediction probability histogram: {np.histogram(y_pred_proba, bins=10)[0]}")
        # Get ground truth labels before threshold selection
        y_true = self.valid_dmatrix.get_label()
        # Log ground truth distribution
        true_counts = np.bincount(y_true.astype(int))
        log(INFO, f"Ground truth distribution: Benign={true_counts[0]}, Malicious={true_counts[1]}")
        # Try different thresholds
        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
        best_threshold = 0.5
        best_balance = float('inf')
        # Get true class distribution
        true_benign = true_counts[0] if len(true_counts) > 0 else 0
        true_malicious = true_counts[1] if len(true_counts) > 1 else 0
        true_ratio = true_malicious / true_benign if true_benign > 0 else float('inf')
        for threshold in thresholds:
            temp_labels = (y_pred_proba > threshold).astype(int)
            temp_counts = np.bincount(temp_labels.astype(int))
            benign_count = temp_counts[0] if len(temp_counts) > 0 else 0
            malicious_count = temp_counts[1] if len(temp_counts) > 1 else 0
            # Calculate predicted ratio, handling edge cases
            pred_ratio = malicious_count / benign_count if benign_count > 0 else float('inf')
            # Calculate how well this threshold preserves the true class distribution
            if true_ratio == float('inf') and pred_ratio == float('inf'):
                # Both have only malicious samples
                balance_score = 0
            elif true_ratio == 0 and pred_ratio == 0:
                # Both have only benign samples
                balance_score = 0
            else:
                # Calculate absolute difference between ratios
                balance_score = abs(pred_ratio - true_ratio)
            log(INFO, f"Threshold {threshold}: Benign={benign_count}, Malicious={malicious_count}, Balance Score={balance_score:.4f}")
            # Update best threshold if this one gives better class balance
            if balance_score < best_balance:
                best_balance = balance_score
                best_threshold = threshold
        log(INFO, f"Selected best threshold: {best_threshold} with balance score: {best_balance:.4f}")
        # Use the best threshold for actual predictions
        THRESHOLD = best_threshold
        y_pred_labels = (y_pred_proba > THRESHOLD).astype(int)
        # Log prediction distribution
        pred_counts = np.bincount(y_pred_labels.astype(int))
        benign_pred = pred_counts[0] if len(pred_counts) > 0 else 0
        malicious_pred = pred_counts[1] if len(pred_counts) > 1 else 0
        log(INFO, f"Prediction distribution: Benign={benign_pred}, Malicious={malicious_pred}")
        log(INFO, f"Prediction probabilities range: [{y_pred_proba.min():.3f}, {y_pred_proba.max():.3f}]")
        # Compute metrics for labeled data
        precision = precision_score(y_true, y_pred_labels, average='weighted')
        recall = recall_score(y_true, y_pred_labels, average='weighted')
        f1 = f1_score(y_true, y_pred_labels, average='weighted')
        # Calculate and log detailed loss information
        log(INFO, "LOSS CALCULATION DETAILS:")
        log(INFO, f"y_true shape: {y_true.shape}, y_pred_proba shape: {y_pred_proba.shape}")
        log(INFO, f"y_true min/max: {y_true.min()}/{y_true.max()}, y_pred_proba min/max: {y_pred_proba.min():.4f}/{y_pred_proba.max():.4f}")
        # Calculate binary cross-entropy loss
        epsilon = 1e-10  # To avoid log(0)
        loss_terms = y_true * np.log(y_pred_proba + epsilon) + (1 - y_true) * np.log(1 - y_pred_proba + epsilon)
        loss = -np.mean(loss_terms)
        # Log loss calculation components
        log(INFO, f"Loss calculation - epsilon: {epsilon}")
        log(INFO, f"Loss calculation - first 5 loss terms: {loss_terms[:5]}")
        log(INFO, f"Loss calculation - mean of loss terms: {np.mean(loss_terms)}")
        log(INFO, f"Loss calculation - final loss value: {loss}")
        # Log detailed metrics
        log(INFO, f"Evaluation metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Loss: {loss:.4f}")
        # Generate confusion matrix
        conf_matrix = confusion_matrix(y_true, y_pred_labels)
        tn, fp, fn, tp = conf_matrix.ravel()
        log(INFO, f"Confusion matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}")
        # Create base metrics dictionary
        metrics = {
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "loss": float(loss),  # Add loss to metrics dictionary
            "true_negatives": int(tn),
            "false_positives": int(fp),
            "false_negatives": int(fn),
            "true_positives": int(tp),
            "num_predictions": self.num_val
        }
        # Check if prediction mode is enabled
        prediction_mode = ins.config.get("prediction_mode", "true").lower() == "true"
        log(INFO, f"Prediction mode is {'enabled' if prediction_mode else 'disabled'} for this round")
        # Add unlabeled predictions if available and prediction mode is enabled
        if self.unlabeled_dmatrix is not None and prediction_mode:
            # Get predictions
            unlabeled_pred_proba = bst.predict(self.unlabeled_dmatrix)
            # Log unlabeled prediction distribution
            log(INFO, f"Unlabeled prediction probabilities range: [{unlabeled_pred_proba.min():.3f}, {unlabeled_pred_proba.max():.3f}]")
            log(INFO, f"Unlabeled prediction probability histogram: {np.histogram(unlabeled_pred_proba, bins=10)[0]}")
            # Use the same threshold determined for validation data
            unlabeled_pred_labels = (unlabeled_pred_proba > THRESHOLD).astype(int)
            # Log prediction distribution
            unlabeled_counts = np.bincount(unlabeled_pred_labels.astype(int))
            benign_count = unlabeled_counts[0] if len(unlabeled_counts) > 0 else 0
            malicious_count = unlabeled_counts[1] if len(unlabeled_counts) > 1 else 0
            log(INFO, f"Unlabeled predictions with threshold {THRESHOLD}: Benign={benign_count}, Malicious={malicious_count}")
            # Check if unlabeled data affects loss calculation
            log(INFO, "CHECKING IF UNLABELED DATA AFFECTS LOSS:")
            if hasattr(self.unlabeled_dmatrix, 'get_label'):
                try:
                    unlabeled_true_labels = self.unlabeled_dmatrix.get_label()
                    if len(unlabeled_true_labels) > 0:
                        log(INFO, f"Unlabeled data has labels, shape: {unlabeled_true_labels.shape}")
                        # Calculate potential loss including unlabeled data
                        unlabeled_loss_terms = unlabeled_true_labels * np.log(unlabeled_pred_proba + epsilon) + (1 - unlabeled_true_labels) * np.log(1 - unlabeled_pred_proba + epsilon)
                        unlabeled_loss = -np.mean(unlabeled_loss_terms)
                        log(INFO, f"Potential unlabeled loss: {unlabeled_loss:.4f}")
                        # Calculate combined loss
                        combined_loss_terms = np.concatenate([loss_terms, unlabeled_loss_terms])
                        combined_loss = -np.mean(combined_loss_terms)
                        log(INFO, f"Potential combined loss: {combined_loss:.4f}")
                    else:
                        log(INFO, "Unlabeled data has empty labels array")
                except Exception as e:
                    log(INFO, f"Error getting labels for unlabeled data: {str(e)}")
            else:
                log(INFO, "Unlabeled data has no labels attribute")
            # Save predictions using the server_utils function
            round_num = ins.config.get("global_round", "final")
            # Check if output directory is provided in config
            output_dir = ins.config.get("output_dir", "results")
            # Get true labels if available for unlabeled data
            unlabeled_true_labels = None
            if hasattr(self.unlabeled_dmatrix, 'get_label'):
                try:
                    unlabeled_true_labels = self.unlabeled_dmatrix.get_label()
                    if len(unlabeled_true_labels) > 0:
                        log(INFO, "Found true labels for unlabeled data, shape: %s", unlabeled_true_labels.shape)
                    else:
                        log(INFO, "Found empty true labels array for unlabeled data, not using for output")
                        unlabeled_true_labels = None
                except Exception as e:
                    log(INFO, "Error getting true labels for unlabeled data: %s", str(e))
                    unlabeled_true_labels = None
            output_path = save_predictions_to_csv(
                None,  # We don't need the data anymore since we simplified save_predictions_to_csv
                unlabeled_pred_labels,
                round_num,
                output_dir=output_dir,
                true_labels=unlabeled_true_labels  # Pass true labels if available and non-empty
            )
            # Add prediction metrics
            metrics.update({
                "total_predictions": len(unlabeled_pred_labels),
                "malicious_predictions": int(np.sum(unlabeled_pred_labels == 1)),
                "benign_predictions": int(np.sum(unlabeled_pred_labels == 0)),
                "predictions_file": output_path
            })
        # Always include prediction_mode in metrics
        metrics["prediction_mode"] = prediction_mode
        return EvaluateRes(
            status=Status(code=Code.OK, message="Success"),
            loss=float(loss),
            num_examples=self.num_val,
            metrics=metrics
        )
</file>

<file path="client.py">
"""
client.py
This module implements the Federated Learning client functionality for distributed XGBoost training.
It handles data loading, preprocessing, and client-side model training operations.
Key Components:
- Data loading and partitioning
- Client initialization
- Model training configuration
- Connection to FL server
"""
import warnings
from logging import INFO
import os
import flwr as fl
from flwr.common.logger import log
from dataset import (
    load_csv_data,
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    resplit,
)
from utils import client_args_parser, BST_PARAMS, NUM_LOCAL_ROUND
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    """
    Retrieves the most recently modified CSV file from the specified directory.
    Args:
        directory (str): Path to the directory containing CSV files
    Returns:
        str: Full path to the most recent CSV file
    Example:
        latest_file = get_latest_csv("/path/to/data/directory")
    """
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
if __name__ == "__main__":
    # Parse command line arguments for experimental settings
    args = client_args_parser()
    data_directory = "data/received"
    latest_csv_path = get_latest_csv(data_directory)
    labeled_dataset = load_csv_data(latest_csv_path)
    unlabeled_dataset = load_csv_data(latest_csv_path)
    # Load labeled data for training
    #labeled_csv_path = "data/received/network_traffic_20250227_194147.csv"
    #labeled_dataset = load_csv_data(labeled_csv_path)
    # Load unlabeled data for prediction
    #unlabeled_csv_path = "data/received/network_traffic_20250227_194147.csv"
    #unlabeled_dataset = load_csv_data(unlabeled_csv_path)
    # Initialize data partitioner based on specified strategy
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type,
        num_partitions=args.num_partitions
    )
    # Load the specific partition for training
    log(INFO, "Loading training partition...")
    train_partition = labeled_dataset["train"]
    train_partition.set_format("numpy")
    # Handle data splitting based on evaluation strategy
    if args.centralised_eval:
        # Use centralized test set for evaluation
        train_data = train_partition
        valid_data = labeled_dataset["test"]
        valid_data.set_format("numpy")
        num_train = train_data.shape[0]
        num_val = valid_data.shape[0]
    else:
        # Perform local train/test split
        train_data, valid_data, num_train, num_val = train_test_split(
            train_partition,
            test_fraction=args.test_fraction,
            seed=args.seed
        )
    # Transform training data into XGBoost's DMatrix format
    log(INFO, "Reformatting training data...")
    train_dmatrix = transform_dataset_to_dmatrix(train_data)
    valid_dmatrix = transform_dataset_to_dmatrix(valid_data)
    # Transform unlabeled data for prediction
    log(INFO, "Reformatting unlabeled data...")
    unlabeled_data = unlabeled_dataset["train"]
    unlabeled_data.set_format("numpy")
    unlabeled_dmatrix = transform_dataset_to_dmatrix(unlabeled_data)
    # Configure training parameters
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Adjust learning rate for bagging method if specified
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.num_partitions
        params.update({"eta": new_lr})
    # Create client with both training and prediction data
    client = XgbClient(
        train_dmatrix=train_dmatrix,
        valid_dmatrix=valid_dmatrix,
        num_train=num_train,
        num_val=num_val,
        num_local_round=num_local_round,
        params=params,
        train_method=args.train_method,
        is_prediction_only=False,  # Set to False for training
        unlabeled_dmatrix=unlabeled_dmatrix  # Add unlabeled data for prediction
    )
    # Initialize and start Flower client
    fl.client.start_client(
        server_address="127.0.0.1:8080",
        client=client,
    )
</file>

<file path="commit.sh">
#!/bin/bash
# Stage all changes
git add .
# Commit the changes with a commit message
git commit -m "commit.sh"
# Push the changes to the remote repository
git push
</file>

<file path="dataset.py">
"""
dataset.py
This module handles all dataset-related operations for the federated learning system.
It provides functionality for loading, preprocessing, partitioning, and transforming
network traffic data for XGBoost training.
Key Components:
- Data loading and preprocessing
- Feature engineering (numerical and categorical)
- Dataset partitioning strategies
- Data format conversions
"""
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset, DatasetDict, concatenate_datasets
from flwr_datasets.partitioner import (
    IidPartitioner,
    LinearPartitioner,
    SquarePartitioner,
    ExponentialPartitioner,
)
from typing import Union, Tuple
# Mapping between partitioning strategy names and their implementations
CORRELATION_TO_PARTITIONER = {
    "uniform": IidPartitioner,
    "linear": LinearPartitioner,
    "square": SquarePartitioner,
    "exponential": ExponentialPartitioner,
}
def load_csv_data(file_path: str) -> DatasetDict:
    """
    Load and prepare CSV data into a Hugging Face DatasetDict format.
    Args:
        file_path (str): Path to the CSV file containing network traffic data
    Returns:
        DatasetDict: Dataset dictionary containing train and test splits
    Example:
        dataset = load_csv_data("path/to/network_data.csv")
    """
    print("Loading dataset from:", file_path)
    df = pd.read_csv(file_path)
    # print dataset statistics
    print("Dataset Statistics:")
    print(f"Total samples: {len(df)}")
    print(f"Features: {df.columns.tolist()}")
    # Convert to Dataset format
    dataset = Dataset.from_pandas(df)
    return DatasetDict({"train": dataset, "test": dataset})
def instantiate_partitioner(partitioner_type: str, num_partitions: int):
    """
    Create a data partitioner based on specified strategy and number of partitions.
    Args:
        partitioner_type (str): Type of partitioning strategy 
            ('uniform', 'linear', 'square', 'exponential')
        num_partitions (int): Number of partitions to create
    Returns:
        Partitioner: Initialized partitioner object
    """
    partitioner = CORRELATION_TO_PARTITIONER[partitioner_type](
        num_partitions=num_partitions
    )
    return partitioner
def preprocess_data(data):
    """/
    Preprocess the data by encoding categorical features and separating features and labels.
    Args:
        data (pd.DataFrame): Input DataFrame
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Define categorical and numerical features
    categorical_features = ['id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history', 'validation_status', 'method', 'status_msg', 'is_orig']
    numerical_features = ['id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
                      'local_orig', 'local_resp', 'missed_bytes', 'orig_pkts', 
                      'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'ts_delta', 
                      'rtt', 'acks', 'percent_lost', 'request_body_len', 
                      'response_body_len', 'seen_bytes', 'missing_bytes', 'overflow_bytes']
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Convert categorical features to category type
    for col in categorical_features:
        df[col] = df[col].astype('category')
        # Get numerical codes for categories
        df[col] = df[col].cat.codes
    # Ensure numerical features are float type
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Check if this is labeled or unlabeled data
    if 'label' in df.columns:
        # For labeled data
        features = df.drop(columns=['label'])
        labels = df['label'].astype(float)
        return features, labels
    else:
        # For unlabeled data
        return df, None
def preprocess_data_deprec2(data):
    """/
    Preprocess the data by encoding categorical features and separating features and labels.
    Args:
        data (pd.DataFrame): Input DataFrame
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Define categorical and numerical features
    categorical_features = ['id.orig_h', 'id.resp_h', 'proto', 'conn_state', 'history']
    numerical_features = ['id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',
                         'local_orig', 'local_resp', 'missed_bytes', 'orig_pkts', 
                         'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Convert categorical features to category type
    for col in categorical_features:
        df[col] = df[col].astype('category')
        # Get numerical codes for categories
        df[col] = df[col].cat.codes
    # Ensure numerical features are float type
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Check if this is labeled or unlabeled data
    if 'label' in df.columns:
        # For labeled data
        features = df.drop(columns=['label'])
        labels = df['label'].astype(float)
        return features, labels
    else:
        # For unlabeled data
        return df, None
def preprocess_data_deprec(data):
    """
    Preprocess the static_data.csv dataset by:
      - Dropping the 'Timestamp' column.
      - Converting 'Dst Port' and 'Protocol' to categorical features.
      - Converting remaining features (except 'Label') to numerical (float).
      - Separating features and target (Label), and encoding the target if necessary.
    Args:
        filepath (str): Path to the static_data.csv file.
    Returns:
        tuple: (features DataFrame, labels Series or None if unlabeled)
    """
    # Create a copy to avoid modifying original data
    df = data.copy()
    # Drop 'Timestamp' as it is not used for training directly
    if 'Timestamp' in df.columns:
        df.drop(columns=['Timestamp'], inplace=True)
    # Define which columns to treat as categorical based on domain knowledge
    categorical_features = []
    if 'Dst Port' in df.columns:
        categorical_features.append('Dst Port')
    if 'Protocol' in df.columns:
        categorical_features.append('Protocol')
    # Convert categorical features to type 'category' and then to numerical codes
    for col in categorical_features:
        df[col] = df[col].astype('category').cat.codes
    # The numerical features are all columns except the ones we have categorized or the target
    numerical_features = [col for col in df.columns if col not in categorical_features + ['Label']]
    # Convert these numerical features to float
    for col in numerical_features:
        df[col] = df[col].astype(float)
    # Replace inf with NaN and cap large values
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    for col in numerical_features:
        max_val = 1e15  # Example max value, adjust as needed
        df[col] = np.where(df[col] > max_val, np.nan, df[col])
    # Process the target variable if present
    if 'Label' in df.columns:
        # If the label column is non-numeric (object), encode it as categorical codes.
        if df['Label'].dtype == object:
            labels = df['Label'].astype('category').cat.codes
        else:
            labels = df['Label']
        features = df.drop(columns=['Label'])
        return features, labels
    else:
        # If no label column, return the processed DataFrame and None for labels
        return df, None
def separate_xy(data):
    """
    Separate features and labels from the dataset.
    Args:
        data: Input dataset
    Returns:
        tuple: (features, labels or None if unlabeled)
    """
    return preprocess_data(data.to_pandas())
def transform_dataset_to_dmatrix(data):
    """
    Transform dataset to DMatrix format.
    Args:
        data: Input dataset
    Returns:
        xgb.DMatrix: Transformed dataset
    """
    x, y = separate_xy(data)
    x_encoded = x.copy()
    # Encode object columns
    object_columns = ['uid', 'client_initial_dcid', 'server_scid']
    for col in object_columns:
        if col in x_encoded.columns:
            le = LabelEncoder()
            x_encoded[col] = le.fit_transform(x_encoded[col].astype(str))
    return xgb.DMatrix(x_encoded, label=y, missing=np.nan)
def train_test_split(
    partition: Dataset, 
    test_fraction: float, 
    seed: int
) -> Tuple[Dataset, Dataset, int, int]:
    """
    Split dataset into training and validation sets.
    Args:
        partition (Dataset): Input dataset to split
        test_fraction (float): Fraction of data to use for testing
        seed (int): Random seed for reproducibility
    Returns:
        Tuple containing:
            - Training dataset
            - Test dataset
            - Number of training samples
            - Number of test samples
    """
    train_test = partition.train_test_split(test_size=test_fraction, seed=seed)
    partition_train = train_test["train"]
    partition_test = train_test["test"]
    num_train = len(partition_train)
    num_test = len(partition_test)
    return partition_train, partition_test, num_train, num_test
def resplit(dataset: DatasetDict) -> DatasetDict:
    """
    Increase the quantity of centralized test samples by reallocating from training set.
    Args:
        dataset (DatasetDict): Input dataset with train/test splits
    Returns:
        DatasetDict: Dataset with adjusted train/test split sizes
    Note:
        Moves 10K samples from training to test set (if available)
    """
    train_size = dataset["train"].num_rows
    # test_size = dataset["test"].num_rows  # Removed unused variable
    # Ensure we don't exceed the number of samples in the training set
    additional_test_samples = min(10000, train_size)
    return DatasetDict(
        {
            "train": dataset["train"].select(
                range(0, train_size - additional_test_samples)
            ),
            "test": concatenate_datasets(
                [
                    dataset["train"].select(
                        range(
                            train_size - additional_test_samples,
                            train_size,
                        )
                    ),
                    dataset["test"],
                ]
            ),
        }
    )
class ModelPredictor:
    """
    Handles model prediction and dataset labeling
    """
    def __init__(self, model_path: str):
        self.model = xgb.Booster()
        self.model.load_model(model_path)
    def predict_and_save(
        self,
        input_data: Union[str, pd.DataFrame],
        output_path: str,
        include_confidence: bool = True
    ):
        """
        Predict on new data and save labeled dataset
        """
        # Load/preprocess input data
        data = self._prepare_data(input_data)
        # Generate predictions
        predictions = self.model.predict(data)
        confidence = None
        if include_confidence:
            confidence = self.model.predict(data, output_margin=True)
        # Save labeled dataset
        self._save_output(data, predictions, confidence, output_path)
</file>

<file path="go_to_work.sh">
#!/bin/bash
# Run Python scripts in the background and store their process IDs (PIDs)
python3 data/receiving_data.py &
PID1=$!
python3 data/livepreprocessing_socket.py &
PID2=$!
# Wait for 1 minute
sleep 30
# Kill the Python scripts
kill $PID1 $PID2
# Run Git commands and GitHub workflow
git pull
./commit.sh
gh workflow run cml.yaml
# Wait for 5 minutess
sleep 300
git pull
</file>

<file path="multi_class_implementation_plan.md">
# Multi-Class Classification Implementation Plan

## Current State
- Binary classification (benign vs malicious) using XGBoost
- Features include both categorical and numerical network traffic data
- Using federated learning with Flower framework
- Need to expand to classify specific types of malicious traffic

## Implementation Steps

### 1. Data Preprocessing Modifications
- [ ] Update `preprocess_data()` in `dataset.py`:
  - Modify label handling to support three classes
  - Update categorical feature encoding to handle new class labels
  - Ensure numerical features remain unchanged
  - Add class label mapping for human-readable output
  - Add specific feature engineering for DNS and ICMP traffic patterns

### 2. Model Configuration Updates
- [ ] Modify XGBoost parameters in `client_utils.py`:
  - Change objective from 'binary:logistic' to 'multi:softmax'
  - Add num_class parameter (3 classes)
  - Update evaluation metrics for multi-class
  - Adjust learning rate and other hyperparameters if needed
  - Add class weights to handle potential imbalance

### 3. Training Pipeline Updates
- [ ] Update `client_utils.py`:
  - Modify fit() method to handle three-class labels
  - Update evaluation metrics calculation
  - Adjust prediction threshold handling
  - Update model serialization/deserialization
  - Add specific handling for DNS and ICMP traffic patterns

### 4. Prediction and Evaluation Updates
- [ ] Modify `use_saved_model.py`:
  - Update prediction handling for three classes
  - Modify confidence score calculation
  - Update output format to include class probabilities
  - Add multi-class specific metrics (confusion matrix, per-class metrics)
  - Add specific evaluation for DNS and ICMP detection accuracy

### 5. Server-Side Updates
- [ ] Update `server_utils.py`:
  - Modify model aggregation for three-class classification
  - Update evaluation metrics aggregation
  - Adjust prediction handling for multiple classes
  - Add specific handling for DNS and ICMP traffic patterns

### 6. Testing and Validation
- [ ] Create test cases for:
  - Three-class data loading and preprocessing
  - Model training with multiple classes
  - Prediction with multiple classes
  - Evaluation metrics for multi-class scenario
  - Specific tests for DNS and ICMP tunneling detection
  - False positive/negative analysis for each class

### 7. Documentation Updates
- [ ] Update README.md with:
  - New three-class classification capabilities
  - Updated usage instructions
  - New evaluation metrics explanation
  - Example outputs and interpretations
  - Specific documentation for DNS and ICMP tunneling detection

## Technical Details

### Class Labels
Based on the classification task, we'll use the following classes:
- Benign (0)
- Malicious DNS Tunneling (1)
- Malicious ICMP Tunneling (2)

### Model Parameters
```python
params = {
    'objective': 'multi:softmax',
    'num_class': 3,
    'eval_metric': ['mlogloss', 'merror'],
    'learning_rate': 0.1,
    'max_depth': 6,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': [1.0, 2.0, 2.0]  # Adjust based on class distribution
}
```

### Evaluation Metrics
- Multi-class accuracy
- Per-class precision, recall, and F1-score
- Confusion matrix
- Multi-class log loss
- Classification report with per-class metrics
- Specific metrics for DNS and ICMP tunneling detection
- False positive/negative rates for each class

### Output Format
```python
{
    'predicted_class': int,
    'class_probabilities': [float],
    'confidence_score': float,
    'class_names': ['benign', 'malicious_dns_tunneling', 'malicious_icmp_tunneling'],
    'traffic_type': str,  # Additional field for traffic type identification
    'risk_level': str     # Additional field for risk assessment
}
```

## Implementation Order
1. Data preprocessing modifications
2. Model configuration updates
3. Training pipeline updates
4. Prediction and evaluation updates
5. Server-side updates
6. Testing and validation
7. Documentation updates

## Success Criteria
1. Model successfully trains on three-class data
2. All evaluation metrics properly calculated and reported
3. Predictions include class probabilities
4. Federated learning pipeline works with multiple classes
5. High accuracy in distinguishing between benign and malicious traffic
6. Accurate classification of DNS vs ICMP tunneling attacks
7. Documentation is complete and accurate
8. Test cases pass successfully

## Potential Challenges
1. Class imbalance (benign traffic likely outnumbers malicious)
2. Model complexity increase with three classes
3. Federated learning convergence with multiple classes
4. Performance impact of multi-class predictions
5. Distinguishing between different types of malicious traffic
6. False positives in benign traffic classification

## Mitigation Strategies
1. Implement class weights or sampling techniques
2. Monitor model complexity and adjust parameters
3. Increase communication rounds if needed
4. Optimize prediction pipeline for efficiency
5. Add specific feature engineering for DNS and ICMP patterns
6. Implement ensemble methods for better classification
7. Use threshold-based classification for high-confidence predictions
</file>

<file path="pretrained_model_usage.md">
# Using the Saved Federated Learning Model

This document explains how to use the saved XGBoost model that was trained using the federated learning process.

## Overview

The federated learning process now saves the final trained model after the training process completes. The model is saved in two formats:

1. JSON format (`final_model.json`) - Human-readable format
2. Binary format (`final_model.bin`) - More efficient for loading

These files are saved in the output directory created for each training run, which follows the pattern:
```
outputs/YYYY-MM-DD/HH-MM-SS/
```

## Making Predictions with the Saved Model

We provide a utility script `use_saved_model.py` that demonstrates how to load the saved model and use it for making predictions on new data.

### Prerequisites

Make sure you have all the required dependencies installed:
- XGBoost
- Pandas
- NumPy
- Scikit-learn (for evaluation metrics)
- Flower (for logging utilities)

### Basic Usage

```bash
python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data> --output_path <path_for_predictions>
```

#### Arguments:

- `--model_path`: Path to the saved model file (.json or .bin)
- `--data_path`: Path to the data file (.csv)
- `--output_path`: Path to save the predictions (default: predictions.csv)
- `--has_labels`: Flag to indicate if the data file contains labels (for evaluation)
- `--info_only`: Display model information without making predictions

### Examples

#### Viewing model information:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --info_only
```

This will display information about the model such as the number of trees, feature importance, and model parameters.

#### Making predictions on unlabeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/unlabeled_data.csv --output_path predictions.csv
```

#### Evaluating the model on labeled data:

```bash
python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/test_data.csv --output_path predictions.csv --has_labels
```

When using the `--has_labels` flag, the script will also calculate and display evaluation metrics such as accuracy, precision, recall, and F1 score.

## Troubleshooting

### Model File Not Found
Make sure the path to the model file is correct. The model files are saved in the output directory created for each training run.

### Model Loading Issues
If you encounter issues loading the model, the system will try multiple approaches:
1. Direct loading using `load_model`
2. Reading the file as bytes and loading into a Booster
3. Creating a new Booster with parameters and then loading the model

### Data Format Issues
The data should be in a format that can be converted to an XGBoost DMatrix. If you're having issues, check that your data has the same features that were used during training.

If you use the `--has_labels` flag but the system can't process the data as labeled, it will automatically fall back to processing it as unlabeled data.

### Memory Issues
If you're working with large datasets, you might encounter memory issues. Consider processing the data in batches or using a machine with more memory.

## Additional Resources

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Flower Documentation](https://flower.dev/docs/)
</file>

<file path="progress.md">
# FL-CML-Pipeline: Progress Summary

## Project Overview

This project implements a privacy-preserving machine learning solution using federated learning with the Flower framework. The system allows multiple clients to collaboratively train XGBoost models for network intrusion detection without sharing raw data, preserving privacy while achieving high model performance.

## Key Components

### 1. Federated Learning Architecture

- **Server Implementation (`server.py`)**
  - Controls the federated learning process
  - Implements both bagging and cyclic training strategies
  - Handles model aggregation and evaluation
  - Manages client selection and coordination

- **Client Implementation (`client.py`)**
  - Loads and processes local data
  - Trains local models based on server instructions
  - Participates in the federated learning process
  - Reports results back to the server

### 2. Data Pipeline

- **Data Processing (`dataset.py`)**
  - Loads CSV data from network traffic captures
  - Implements preprocessing for network traffic features
  - Provides multiple partitioning strategies (IID, Linear, Square, Exponential)
  - Handles data format conversions for XGBoost compatibility

- **Real-time Data Capture**
  - Support for live network traffic capture
  - Processing and conversion to training datasets

### 3. Utility Functions

- **Client Utilities (`client_utils.py`)**
  - XGBoost client implementation
  - Client-side helper functions

- **Server Utilities (`server_utils.py`)**
  - Server-side helper functions
  - Client management systems
  - Results handling and storage

### 4. Experiment Framework

- **Training Methods**
  - Bagging approach (`run_bagging.sh`): Aggregates models from multiple clients
  - Cyclic approach (`run_cyclic.sh`): Passes model sequentially through clients

- **Evaluation**
  - Supports both centralized and federated evaluation
  - Tracks multiple metrics (precision, recall, F1 score)

## Project Features

- ✅ **Privacy-Preserving Training** - True federated learning with data isolation
- ✅ **Flexible Configuration** - Support for various training strategies
- ✅ **Reproducible Experiments** - Automatic output organization
- ✅ **Custom Dataset Support** - CSV data loader with preprocessing pipeline
- ✅ **Multiple Partitioning Strategies** - IID, Linear, Square, Exponential

## Recent Developments

### Implemented Core Functionality
- Established federated learning architecture with Flower framework
- Created data processing pipeline for network traffic data
- Implemented both bagging and cyclic training approaches
- Set up experiment scripts for reproducible testing

### Technical Improvements
- Enhanced XGBoost integration with Flower framework
- Improved data partitioning strategies
- Optimized client-server communication
- Implemented comprehensive metrics tracking

### Documentation
- Created detailed README with project structure and instructions
- Documented key components and their relationships
- Added configuration guidelines and examples

### Infrastructure
- Set up project directory structure
- Created Bash scripts for easy experiment execution
- Implemented output storage and organization

## Next Steps

### Planned Enhancements
- Improve scalability for larger numbers of clients
- Enhance privacy guarantees with differential privacy techniques
- Optimize hyperparameters for better model performance
- Add support for more model architectures

### Ongoing Research
- Comparing performance of bagging vs. cyclic approaches
- Analyzing impact of different data partitioning strategies
- Evaluating model convergence across federated strategies

## Conclusion

The FL-CML-Pipeline project has established a solid foundation for privacy-preserving machine learning using federated learning. The implemented system successfully demonstrates collaborative model training across multiple clients without sharing raw data, achieving the core goal of privacy-preserving machine learning for network intrusion detection.

The project continues to evolve with ongoing research into optimal federated learning strategies and implementation improvements to enhance performance and usability.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.4.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "xgboost-comprehensive"
version = "0.1.0"
description = "Federated XGBoost with Flower (comprehensive)"
authors = ["The Flower Authors <hello@flower.ai>"]

[tool.poetry.dependencies]
python = ">=3.8,<3.11"
flwr = { extras = ["simulation"], version = ">=1.7.0,<2.0" }
flwr-datasets = ">=0.2.0,<1.0.0"
xgboost = ">=2.0.0,<3.0.0"
</file>

<file path="README.md">
# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## 🛠️ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture → 2. Clean Capture and Convert to Dataset → 3. Train/Test → 4.️Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## 🗺️ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## 🎯 What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **📚 Table of Contents**
- [✨ Features](#-features)  
- [📂 Project Structure](#-project-structure)  
- [🚀 Getting Started](#-getting-started)  
- [⚙️ Configuration](#-configuration)
- [📂 Output Structure](#-output-structure)
- [🧪 Running Experiments](#-running-experiments)  
- [⚖️ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## ✨ Features  
✅ **Privacy-Preserving Training** - Federated learning implementation with data isolation  
✅ **Flexible Configuration** - Hydra-powered experiment management  
✅ **Reproducible Experiments** - ⚠️Automatic output organization   
✅ **CI/CD Integration** - GitHub Actions workflow with CML reporting  
✅ **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **📂 Project Structure**
```bash
├── github/
│   └── workflows/
│       └── cml.yaml      # CI/CD workflow definition
├── pyecache/             # Python cache directory
├── data/                 # Dataset files, data capture script, and data cleaning script
├   └── received/         # Data from Zeek/Kafka stream
├── outputs/              # Model, Predictions, Eval; Output files
├── results/              # Latest aggregated metrics
├── client.py             # Flower client logic
├── client_utils.py       # Client helper functions
├── dataset.py            # Data loading/preprocessing
├── poetry.lock           # Poetry dependency lockfile - 🔍 exploring (research phase) 🔍
├── pyproject.toml        # Poetry project configuration - 🔍 exploring (research phase) 🔍
├── requirements.txt      # Python dependencies
├── run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - 🚧 under construction (implementation phase) 🚧
├── run_bagging.sh        # Bagging experiment script - runs script.py + client.py
├── run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
├── server.py             # Flower server logic
├── server_utils.py       # Server helper functions
├── sim.py                # Start simulation - ⚠️ deprecated soon ⚠️
├── utils.py              # Shared utilities
└── README.md             # Project documentation
```

---

## **🚀 Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **⚠️⚙️ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **⚠📂 Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
└── YYYY-MM-DD/                  # Run date
    └── HH-MM-SS/                # Run time
        ├── .hydra/              # ⚠️Config snapshots
        │   ├── config.yaml
        │   └── hydra.yaml
        ├── results.pkl          # Training history
        ├── predictions/         # Model predictions
            ├── predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **🧪 Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# ⚖️ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## 🔄 **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## 🎒 **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## 📊 Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- ༼ つ ◕_◕ ༽つ R&D ZONE ༼ つ ◕_◕ ༽つ -->
<div align="center">

## 🔥 **R&D Led By** 🔥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  🧑💻 Coded with 100% chaos-driven curiosity    !
!  ☕ Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
🔐 Cyber Alchemy Brewing For 🏛️ Indiana University of Pennsylvania's ARMZTA Project

🔗 https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- ༼ つ ◕_◕ ༽つ R&D ZONE ༼ つ ◕_◕ ༽つ -->
</file>

<file path="requirements.txt">
flwr[simulation]>=1.7.0, <2.0
flwr-datasets>=0.2.0, <1.0.0
xgboost>=2.0.0, <3.0.0
</file>

<file path="run_bagging.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --pool-size=5 --num-rounds=20 --num-clients-per-round=5 &
sleep 30  # Sleep for 30s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --num-partitions=5 --partitioner-type=exponential &
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run_cyclic.sh">
#!/bin/bash
set -e
cd "$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"/
echo "Starting server"
python3 server.py --train-method=cyclic --pool-size=20 --num-rounds=10 &
sleep 30  # Sleep for 15s to give the server enough time to start
for i in `seq 0 4`; do
    echo "Starting client $i"
    python3 client.py --partition-id=$i --train-method=cyclic --num-partitions=5 --partitioner-type=uniform &
    sleep 5
done
# Enable CTRL+C to stop all background processes
trap "trap - SIGTERM && kill -- -$$" SIGINT SIGTERM
# Wait for all background processes to complete
wait
</file>

<file path="run.py">
import subprocess
import time
import os
def run_script(script_path):
    """Run a Python script from the given path."""
    return subprocess.Popen(['python3', script_path])
def wait_for_new_file(directory):
    """Wait for a new file to be created in the directory."""
    existing_files = set(os.listdir(directory))
    while True:
        time.sleep(5)  # Check every 5 seconds
        current_files = set(os.listdir(directory))
        new_files = current_files - existing_files
        if new_files:
            print(f"New file detected: {new_files}")
            return new_files.pop()  # Return the first detected new file
def main():
    # Paths to your scripts
    preprocessing_script = '/home/mohamed/Desktop/test_repo/data/livepreprocessing_socket.py'
    receiving_script = '/home/mohamed/Desktop/test_repo/data/receiving_data.py'
    data_directory = '/home/mohamed/Desktop/test_repo/data'
    # Start both scripts
    preprocessing_process = run_script(preprocessing_script)
    receiving_process = run_script(receiving_script)
    print("Both scripts are running...")
    try:
        # Wait for a new file to appear in the /data directory
        new_file = wait_for_new_file(data_directory)
        # Run the 'act -j run' command once the new file is detected
        subprocess.run(['act', '-j', 'run', '--container-architecture', 'linux/amd64', '-v'])
        print(f"'act -j run' executed after detecting {new_file}")
    except KeyboardInterrupt:
        print("Process interrupted by user.")
    finally:
        # Terminate the running processes
        preprocessing_process.terminate()
        receiving_process.terminate()
        print("Processes terminated.")
if __name__ == "__main__":
    main()
</file>

<file path="server_utils.py">
from typing import Dict, List, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from logging import INFO
import xgboost as xgb
import pandas as pd
from flwr.common.logger import log
from flwr.common import Parameters, Scalar
from flwr.server.client_manager import SimpleClientManager
from flwr.server.client_proxy import ClientProxy
from flwr.server.criterion import Criterion
from utils import BST_PARAMS
import os
import json
import shutil
from datetime import datetime
import pickle
def setup_output_directory():
    """
    Creates a date and time-based directory structure for outputs.
    Returns:
        str: Path to the created output directory
    """
    # Create base outputs directory if it doesn't exist
    base_dir = "outputs"
    os.makedirs(base_dir, exist_ok=True)
    # Create date directory
    date_str = datetime.now().strftime("%Y-%m-%d")
    date_dir = os.path.join(base_dir, date_str)
    os.makedirs(date_dir, exist_ok=True)
    # Create time directory
    time_str = datetime.now().strftime("%H-%M-%S")
    output_dir = os.path.join(date_dir, time_str)
    os.makedirs(output_dir, exist_ok=True)
    # Create .hydra directory
    hydra_dir = os.path.join(output_dir, ".hydra")
    os.makedirs(hydra_dir, exist_ok=True)
    # Copy existing .hydra files if they exist
    if os.path.exists(".hydra"):
        for file in os.listdir(".hydra"):
            if file.endswith(".yaml"):
                src_path = os.path.join(".hydra", file)
                dst_path = os.path.join(hydra_dir, file)
                shutil.copy2(src_path, dst_path)
    log(INFO, "Created output directory: %s", output_dir)
    return output_dir
def save_results_pickle(results, output_dir):
    """
    Save results dictionary to a pickle file.
    Args:
        results (dict): Results to save
        output_dir (str): Directory to save to
    """
    output_path = os.path.join(output_dir, "results.pkl")
    with open(output_path, 'wb') as f:
        pickle.dump(results, f)
    log(INFO, "Saved results to: %s", output_path)
def eval_config(rnd: int, output_dir: str = None) -> Dict[str, str]:
    """
    Return a configuration with global round and output directory.
    Args:
        rnd (int): Current round number
        output_dir (str, optional): Output directory path
    Returns:
        Dict[str, str]: Configuration dictionary
    """
    # Set prediction_mode to false for rounds 1-10 and true for rounds 11-20
    prediction_mode = "false" if rnd <= 10 else "true"
    config = {
        "global_round": str(rnd),
        "prediction_mode": prediction_mode,
    }
    # Add output directory if provided
    if output_dir is not None:
        config["output_dir"] = output_dir
    return config
def save_evaluation_results(eval_metrics: Dict, round_num: int, output_dir: str = None):
    """
    Save evaluation results for each round.
    Args:
        eval_metrics (Dict): Evaluation metrics to save
        round_num (int or str): Round number or identifier
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Format results
    results = {
        'round': round_num,
        'timestamp': datetime.now().isoformat(),
        'metrics': eval_metrics
    }
    # Save to file
    output_path = os.path.join(output_dir, f"eval_results_round_{round_num}.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=4)
    log(INFO, "Evaluation results saved to: %s", output_path)
def fit_config(rnd: int) -> Dict[str, str]:
    """Return a configuration with global epochs."""
    config = {
        "global_round": str(rnd),
    }
    return config
def evaluate_metrics_aggregation(eval_metrics):
    """Return aggregated metrics for evaluation."""
    total_num = sum([num for num, _ in eval_metrics])
    # Log the raw metrics received from clients
    log(INFO, f"Received metrics from {len(eval_metrics)} clients")
    for i, (num, metrics) in enumerate(eval_metrics):
        log(INFO, f"Client {i+1} metrics: {metrics.keys()}")
        if "loss" in metrics:
            log(INFO, f"Client {i+1} loss: {metrics['loss']}")
    # Check if we're in prediction mode or evaluation mode
    first_metrics = eval_metrics[0][1]
    is_prediction_mode = first_metrics.get("prediction_mode", False)
    # Add detailed logging about prediction mode
    log(INFO, f"PREDICTION MODE STATUS: {'ENABLED' if is_prediction_mode else 'DISABLED'}")
    # Log individual client prediction mode status
    prediction_modes = [metrics.get("prediction_mode", False) for _, metrics in eval_metrics]
    log(INFO, f"Client prediction modes: {prediction_modes}")
    if is_prediction_mode:
        # Aggregate prediction statistics
        total_predictions = sum([metrics.get("total_predictions", 0)for num, metrics in eval_metrics])
        malicious_predictions = sum([metrics.get("malicious_predictions", 0)for num, metrics in eval_metrics])
        benign_predictions = sum([metrics.get("benign_predictions", 0)for num, metrics in eval_metrics])
        # Calculate loss if available
        if all("loss" in metrics for _, metrics in eval_metrics):
            # Log individual client losses for analysis
            client_losses = [metrics["loss"] for _, metrics in eval_metrics]
            log(INFO, f"Individual client losses: {client_losses}")
            loss = sum([metrics["loss"] * num for num, metrics in eval_metrics]) / total_num
            log(INFO, f"PREDICTION MODE - Aggregated loss calculation: sum(loss*num)={sum([metrics['loss'] * num for num, metrics in eval_metrics])}, total_num={total_num}, result={loss}")
        else:
            loss = 0.0
            log(INFO, "PREDICTION MODE - Loss not available in all client metrics")
        # Create aggregated metrics dictionary
        aggregated_metrics = {
            "total_predictions": total_predictions,
            "malicious_predictions": malicious_predictions,
            "benign_predictions": benign_predictions,
            "prediction_mode": is_prediction_mode,
            "loss": loss
        }
        log(INFO, f"Aggregated prediction metrics: {aggregated_metrics}")
    else:
        # Aggregate evaluation metrics
        if all("precision" in metrics for _, metrics in eval_metrics):
            precision = sum([metrics["precision"] * num for num, metrics in eval_metrics]) / total_num
        else:
            precision = 0.0
        if all("recall" in metrics for _, metrics in eval_metrics):
            recall = sum([metrics["recall"] * num for num, metrics in eval_metrics]) / total_num
        else:
            recall = 0.0
        if all("f1" in metrics for _, metrics in eval_metrics):
            f1 = sum([metrics["f1"] * num for num, metrics in eval_metrics]) / total_num
        else:
            f1 = 0.0
        if all("loss" in metrics for _, metrics in eval_metrics):
            # Log individual client losses for analysis
            client_losses = [metrics["loss"] for _, metrics in eval_metrics]
            log(INFO, f"Individual client losses: {client_losses}")
            loss = sum([metrics["loss"] * num for num, metrics in eval_metrics]) / total_num
            log(INFO, f"EVALUATION MODE - Aggregated loss calculation: sum(loss*num)={sum([metrics['loss'] * num for num, metrics in eval_metrics])}, total_num={total_num}, result={loss}")
        else:
            loss = 0.0
            log(INFO, "EVALUATION MODE - Loss not available in all client metrics")
        # Aggregate confusion matrix elements
        tn = sum([metrics.get("true_negatives", 0) for _, metrics in eval_metrics])
        fp = sum([metrics.get("false_positives", 0) for _, metrics in eval_metrics])
        fn = sum([metrics.get("false_negatives", 0) for _, metrics in eval_metrics])
        tp = sum([metrics.get("true_positives", 0) for _, metrics in eval_metrics])
        # Create aggregated metrics dictionary
        aggregated_metrics = {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "true_negatives": tn,
            "false_positives": fp,
            "false_negatives": fn,
            "true_positives": tp,
            "prediction_mode": is_prediction_mode,
            "loss": loss
        }
        log(INFO, f"Aggregated evaluation metrics: {aggregated_metrics}")
    # Save aggregated results
    save_evaluation_results(aggregated_metrics, "aggregated")
    return loss, aggregated_metrics
def save_predictions_to_csv(data, predictions, round_num: int, output_dir: str = None, true_labels=None):
    """
    Save dataset with predictions to CSV in the specified directory.
    Args:
        data: Original data (not used in this simplified version)
        predictions: Prediction labels
        round_num (int): Round number
        output_dir (str, optional): Directory to save results to. If None, uses the default results directory.
        true_labels (array, optional): True labels if available
    Returns:
        str: Path to the saved CSV file
    """
    # Use default results directory if no output_dir is provided
    if output_dir is None:
        output_dir = "results"
    os.makedirs(output_dir, exist_ok=True)
    # Create predictions DataFrame
    predictions_dict = {
        'predicted_label': predictions,
        'prediction_type': ['malicious' if p == 1 else 'benign' for p in predictions]
    }
    # Add true labels if available and have the same length as predictions
    if true_labels is not None and len(true_labels) == len(predictions):
        log(INFO, "Including true labels in the output CSV (same length as predictions)")
        predictions_dict['true_label'] = true_labels
        predictions_dict['true_label_type'] = ['malicious' if t == 1 else 'benign' for t in true_labels]
    elif true_labels is not None:
        log(INFO, f"True labels available but length mismatch: predictions={len(predictions)}, true_labels={len(true_labels)}. Not including true labels in output.")
    predictions_df = pd.DataFrame(predictions_dict)
    # Save to CSV in the specified directory
    output_path = os.path.join(output_dir, f"predictions_round_{round_num}.csv")
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return output_path
def load_saved_model(model_path):
    """
    Load a saved XGBoost model from disk.
    Args:
        model_path (str): Path to the saved model file (.json or .bin)
    Returns:
        xgb.Booster: Loaded XGBoost model
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    log(INFO, "Loading model from: %s", model_path)
    try:
        # Create a new booster
        bst = xgb.Booster()
        # Try to load the model directly
        bst.load_model(model_path)
        log(INFO, "Model loaded successfully")
        return bst
    except Exception as e:
        log(INFO, "Error loading model directly: %s", str(e))
        # If direct loading fails, try alternative approaches
        try:
            # Try reading the file as bytes and loading
            with open(model_path, 'rb') as f:
                model_data = f.read()
            bst = xgb.Booster()
            bst.load_model(bytearray(model_data))
            log(INFO, "Model loaded successfully using bytearray")
            return bst
        except Exception as e2:
            log(INFO, "Error loading model using bytearray: %s", str(e2))
            # If that fails too, try with params
            try:
                from utils import BST_PARAMS
                bst = xgb.Booster(params=BST_PARAMS)
                bst.load_model(model_path)
                log(INFO, "Model loaded successfully with params")
                return bst
            except Exception as e3:
                log(INFO, "All loading attempts failed")
                raise ValueError(f"Failed to load model: {str(e)}, {str(e2)}, {str(e3)}")
def predict_with_saved_model(model_path, dmatrix, output_path):
    # Load the model
    model = load_saved_model(model_path)
    # Make predictions
    raw_predictions = model.predict(dmatrix)
    # Log raw predictions
    log(INFO, "Raw predictions: %s", raw_predictions)
    # Log distribution of scores
    log(INFO, "Prediction score distribution - Min: %.4f, Max: %.4f, Mean: %.4f", 
        np.min(raw_predictions), np.max(raw_predictions), np.mean(raw_predictions))
    # Convert raw predictions to probabilities if necessary
    # (Assuming a binary classification with a threshold of 0.5)
    probabilities = 1 / (1 + np.exp(-raw_predictions))  # Example for sigmoid transformation
    predicted_labels = (probabilities >= 0.5).astype(int)
    # Log predicted class distribution
    unique, counts = np.unique(predicted_labels, return_counts=True)
    log(INFO, "Predicted class distribution: %s", dict(zip(unique, counts)))
    # Save predictions to CSV
    predictions_df = pd.DataFrame({
        'predicted_label': predicted_labels,
        'prediction_type': ['benign' if label == 0 else 'malicious' for label in predicted_labels],
        'prediction_score': probabilities
    })
    predictions_df.to_csv(output_path, index=False)
    log(INFO, "Predictions saved to: %s", output_path)
    return predictions
def get_evaluate_fn(test_data):
    """Return a function for centralised evaluation."""
    def evaluate_fn(
        server_round: int, parameters: Parameters, config: Dict[str, Scalar]
    ):
        if server_round == 0:
            return 0, {}
        else:
            bst = xgb.Booster(params=BST_PARAMS)
            for para in parameters.tensors:
                para_b = bytearray(para)
            bst.load_model(para_b)
            # Predict on test data
            y_pred = bst.predict(test_data)
            y_pred_labels = y_pred.astype(int)
            # Get true labels
            y_true = test_data.get_label()
            # Save dataset with predictions to results directory
            output_path = save_predictions_to_csv(test_data, y_pred_labels, server_round, "results", y_true)
            # Compute metrics
            precision = precision_score(y_true, y_pred_labels, average='weighted')
            recall = recall_score(y_true, y_pred_labels, average='weighted')
            f1 = f1_score(y_true, y_pred_labels, average='weighted')
            # Generate confusion matrix
            conf_matrix = confusion_matrix(y_true, y_pred_labels)
            # Create metrics dictionary
            metrics = {
                "precision": float(precision),
                "recall": float(recall),
                "f1": float(f1),
                "true_negatives": int(conf_matrix[0][0]),
                "false_positives": int(conf_matrix[0][1]),
                "false_negatives": int(conf_matrix[1][0]),
                "true_positives": int(conf_matrix[1][1]),
                "predictions_file": output_path
            }
            log(INFO, f"Precision = {precision}, Recall = {recall}, F1 Score = {f1} at round {server_round}")
            log(INFO, f"Dataset with predictions saved to: {output_path}")
            return 0, metrics
    return evaluate_fn
class CyclicClientManager(SimpleClientManager):
    """Provides a cyclic client selection rule."""
    def sample(
        self,
        num_clients: int,
        min_num_clients: Optional[int] = None,
        criterion: Optional[Criterion] = None,
    ) -> List[ClientProxy]:
        """Sample a number of Flower ClientProxy instances."""
        # Block until at least num_clients are connected.
        if min_num_clients is None:
            min_num_clients = num_clients
        self.wait_for(min_num_clients)
        # Sample clients which meet the criterion
        available_cids = list(self.clients)
        if criterion is not None:
            available_cids = [
                cid for cid in available_cids if criterion.select(self.clients[cid])
            ]
        if num_clients > len(available_cids):
            log(
                INFO,
                "Sampling failed: number of available clients"
                " (%s) is less than number of requested clients (%s).",
                len(available_cids),
                num_clients,
            )
            return []
        # Return all available clients
        return [self.clients[cid] for cid in available_cids]
</file>

<file path="server.py">
import warnings
from logging import INFO
import os
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
import xgboost as xgb
from utils import server_args_parser, BST_PARAMS
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
    setup_output_directory,
    save_results_pickle,
)
from dataset import transform_dataset_to_dmatrix, load_csv_data
warnings.filterwarnings("ignore", category=UserWarning)
# Create output directory structure
output_dir = setup_output_directory()
# Parse arguments for experimental settings
args = server_args_parser()
train_method = args.train_method
pool_size = args.pool_size
num_rounds = args.num_rounds
num_clients_per_round = args.num_clients_per_round
num_evaluate_clients = args.num_evaluate_clients
centralised_eval = args.centralised_eval
# Load centralised test set
if centralised_eval:
    log(INFO, "Loading centralised test set...")
    test_set = load_csv_data("data/static_data.csv")["test"]
    test_set.set_format("pandas")
    test_dmatrix = transform_dataset_to_dmatrix(test_set)
# Define a custom config function that includes the output directory
def custom_eval_config(rnd: int):
    return eval_config(rnd, output_dir)
# Define strategy
if train_method == "bagging":
    # Bagging training
    strategy = FedXgbBagging(
        evaluate_function=get_evaluate_fn(test_dmatrix) if centralised_eval else None,
        fraction_fit=(float(num_clients_per_round) / pool_size),
        min_fit_clients=num_clients_per_round,
        min_available_clients=pool_size,
        min_evaluate_clients=num_evaluate_clients if not centralised_eval else 0,
        fraction_evaluate=1.0 if not centralised_eval else 0.0,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
        evaluate_metrics_aggregation_fn=(
            evaluate_metrics_aggregation if not centralised_eval else None
        ),
    )
    # Add a monkey patch to log the loss value before it's returned
    original_aggregate_evaluate = strategy.aggregate_evaluate
    def patched_aggregate_evaluate(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate
else:
    # Cyclic training
    strategy = FedXgbCyclic(
        fraction_fit=1.0,
        min_available_clients=pool_size,
        fraction_evaluate=1.0,
        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
        on_evaluate_config_fn=custom_eval_config,
        on_fit_config_fn=fit_config,
    )
    # Add a monkey patch to handle the new return format from evaluate_metrics_aggregation
    original_aggregate_evaluate_cyclic = strategy.aggregate_evaluate
    def patched_aggregate_evaluate_cyclic(server_round, eval_results, failures):
        log(INFO, "Aggregating evaluation results for round %s (cyclic)", server_round)
        # Call the original function
        aggregated_result = original_aggregate_evaluate_cyclic(server_round, eval_results, failures)
        # Check the format of the result
        if isinstance(aggregated_result, tuple) and len(aggregated_result) == 2:
            # The result is already in the correct format (loss, metrics)
            loss, metrics = aggregated_result
            log(INFO, "Aggregated loss for round %s: %s", server_round, loss)
            # Check if metrics is a dictionary before trying to access keys
            if isinstance(metrics, dict):
                log(INFO, "Metrics for round %s: %s", server_round, metrics.keys())
            else:
                log(INFO, "Metrics for round %s is not a dictionary: %s", server_round, type(metrics))
                # If metrics is not a dictionary, create a new dictionary
                if metrics is None:
                    metrics = {}
                elif not isinstance(metrics, dict):
                    # Try to convert to dictionary if possible
                    try:
                        metrics = dict(metrics)
                    except (TypeError, ValueError):
                        # If conversion fails, create a new dictionary with the original metrics as a value
                        metrics = {"original_metrics": metrics}
                log(INFO, "Created new metrics dictionary: %s", metrics)
            # Return the result in the correct format
            return loss, metrics
        # The result is not in the expected format
        log(INFO, "Unexpected format from original_aggregate_evaluate_cyclic: %s", type(aggregated_result))
        # Try to extract loss and metrics
        if isinstance(aggregated_result, (int, float)):
            # Only loss was returned
            loss = aggregated_result
            metrics = {}
        elif isinstance(aggregated_result, dict):
            # Only metrics were returned
            loss = aggregated_result.get("loss", 0.0)
            metrics = aggregated_result
        else:
            # Unknown format, use defaults
            loss = 0.0
            metrics = {}
        log(INFO, "Extracted loss: %s, metrics: %s", loss, metrics)
        # Return in the correct format
        return loss, metrics
    strategy.aggregate_evaluate = patched_aggregate_evaluate_cyclic
# Start Flower server
history = fl.server.start_server(
    server_address="0.0.0.0:8080",
    config=fl.server.ServerConfig(num_rounds=num_rounds),
    strategy=strategy,
    client_manager=CyclicClientManager() if train_method == "cyclic" else None,
)
# Save the results after training is complete
log(INFO, "Training complete. Saving results...")
# Create a dictionary to store the results
results = {}
# Add losses if available
if hasattr(history, 'losses_distributed') and history.losses_distributed:
    results["loss"] = history.losses_distributed
else:
    results["loss"] = []
    log(INFO, "No distributed losses found in history")
# Add metrics if available
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    results["metrics"] = history.metrics_distributed
else:
    results["metrics"] = {}
    log(INFO, "No distributed metrics found in history")
# Save the results
save_results_pickle(results, output_dir)
# Save the final trained model
log(INFO, "Saving the final trained model...")
if hasattr(strategy, 'global_model') and strategy.global_model is not None:
    # If the strategy has a global_model attribute, convert it to a Booster and save it
    try:
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Check if global_model is bytes or bytearray
        if isinstance(strategy.global_model, (bytes, bytearray)):
            # Load the bytes into the booster
            bst.load_model(bytearray(strategy.global_model))
        else:
            # If it's already a Booster, use it directly
            bst = strategy.global_model
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving global model: %s", str(e))
elif hasattr(history, 'parameters_aggregated') and history.parameters_aggregated:
    # If the strategy doesn't have a global_model attribute but history has parameters
    try:
        # Get the final parameters
        final_parameters = history.parameters_aggregated[-1]
        # Create a booster with the same parameters used in training
        bst = xgb.Booster(params=BST_PARAMS)
        # Load the parameters into the booster
        para_b = bytearray()
        for para in final_parameters.tensors:
            para_b.extend(para)
        bst.load_model(para_b)
        # Save the model to a file
        model_path = os.path.join(output_dir, "final_model.json")
        bst.save_model(model_path)
        # Also save in binary format for better compatibility
        bin_model_path = os.path.join(output_dir, "final_model.bin")
        bst.save_model(bin_model_path)
        log(INFO, "Final model saved to: %s and %s", model_path, bin_model_path)
    except Exception as e:
        log(INFO, "Error saving final model: %s", str(e))
else:
    log(INFO, "No final model parameters available to save")
# Also save the final evaluation results
if hasattr(history, 'metrics_distributed') and history.metrics_distributed:
    from server_utils import save_evaluation_results
    final_round = num_rounds
    # Check if metrics_distributed is a dictionary or a list
    if isinstance(history.metrics_distributed, dict):
        final_metrics = history.metrics_distributed
    elif isinstance(history.metrics_distributed, list) and len(history.metrics_distributed) > 0:
        final_metrics = history.metrics_distributed[-1][1]  # Get the metrics from the last round
    else:
        final_metrics = {}
        log(INFO, "No metrics available to save")
    save_evaluation_results(final_metrics, final_round, output_dir)
else:
    log(INFO, "No metrics available to save")
</file>

<file path="sim.py">
import warnings
import os
from logging import INFO
import xgboost as xgb
from tqdm import tqdm
import numpy as np
import pandas as pd
import flwr as fl
from flwr.common.logger import log
from flwr.server.strategy import FedXgbBagging, FedXgbCyclic
from dataset import (
    instantiate_partitioner,
    train_test_split,
    transform_dataset_to_dmatrix,
    separate_xy,
    resplit,
    load_csv_data,
)
from utils import (
    sim_args_parser,
    NUM_LOCAL_ROUND,
    BST_PARAMS,
)
from server_utils import (
    eval_config,
    fit_config,
    evaluate_metrics_aggregation,
    get_evaluate_fn,
    CyclicClientManager,
)
from client_utils import XgbClient
warnings.filterwarnings("ignore", category=UserWarning)
def get_latest_csv(directory: str) -> str:
    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
    latest_file = max(csv_files, key=lambda x: os.path.getctime(os.path.join(directory, x)))
    return os.path.join(directory, latest_file)
def get_client_fn(
    train_data_list, valid_data_list, train_method, params, num_local_round
):
    """Return a function to construct a client.
    The VirtualClientEngine will execute this function whenever a client is sampled by
    the strategy to participate.
    """
    def client_fn(cid: str) -> fl.client.Client:
        """Construct a FlowerClient with its own dataset partition."""
        x_train, y_train = train_data_list[int(cid)][0]
        x_valid, y_valid = valid_data_list[int(cid)][0]
        # Reformat data to DMatrix
        train_dmatrix = xgb.DMatrix(x_train, label=y_train)
        valid_dmatrix = xgb.DMatrix(x_valid, label=y_valid)
        # Fetch the number of examples
        num_train = train_data_list[int(cid)][1]
        num_val = valid_data_list[int(cid)][1]
        # Create and return client
        return XgbClient(
            train_dmatrix,
            valid_dmatrix,
            num_train,
            num_val,
            num_local_round,
            params,
            train_method,
        )
    return client_fn
def main():
    # Parse arguments for experimental settings
    args = sim_args_parser()
    # Load CSV dataset
    csv_file_path = "data/shuffled_merged.csv"
    #csv_file_path = get_latest_csv("/home/mohamed/Desktop/test_repo/data")
    dataset = load_csv_data(csv_file_path)
    # Conduct partitioning
    partitioner = instantiate_partitioner(
        partitioner_type=args.partitioner_type, num_partitions=args.pool_size
    )
    fds = dataset
    # Load centralised test set
    if args.centralised_eval or args.centralised_eval_client:
        log(INFO, "Loading centralised test set...")
        test_data = fds["test"]
        test_data.set_format("numpy")
        num_test = test_data.shape[0]
        test_dmatrix = transform_dataset_to_dmatrix(test_data)
    # Load partitions and reformat data to DMatrix for xgboost
    log(INFO, "Loading client local partitions...")
    train_data_list = []
    valid_data_list = []
    # Load and process all client partitions. This upfront cost is amortized soon
    # after the simulation begins since clients wont need to preprocess their partition.
    for partition_id in tqdm(range(args.pool_size), desc="Extracting client partition"):
        # Extract partition for client with partition_id
        partition = fds["train"]
        partition.set_format("numpy")
        if args.centralised_eval_client:
            # Use centralised test set for evaluation
            train_data = partition
            num_train = train_data.shape[0]
            x_test, y_test = separate_xy(test_data)
            valid_data_list.append(((x_test, y_test), num_test))
        else:
            # Train/test splitting
            train_data, valid_data, num_train, num_val = train_test_split(
                partition, test_fraction=args.test_fraction, seed=args.seed
            )
            x_valid, y_valid = separate_xy(valid_data)
            valid_data_list.append(((x_valid, y_valid), num_val))
        x_train, y_train = separate_xy(train_data)
        train_data_list.append(((x_train, y_train), num_train))
    # Define strategy
    if args.train_method == "bagging":
        # Bagging training
        strategy = FedXgbBagging(
            evaluate_function=(
                get_evaluate_fn(test_dmatrix) if args.centralised_eval else None
            ),
            fraction_fit=(float(args.num_clients_per_round) / args.pool_size),
            min_fit_clients=args.num_clients_per_round,
            min_available_clients=args.pool_size,
            min_evaluate_clients=(
                args.num_evaluate_clients if not args.centralised_eval else 0
            ),
            fraction_evaluate=1.0 if not args.centralised_eval else 0.0,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
            evaluate_metrics_aggregation_fn=(
                evaluate_metrics_aggregation if not args.centralised_eval else None
            ),
        )
    else:
        # Cyclic training
        strategy = FedXgbCyclic(
            fraction_fit=1.0,
            min_available_clients=args.pool_size,
            fraction_evaluate=1.0,
            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation,
            on_evaluate_config_fn=eval_config,
            on_fit_config_fn=fit_config,
        )
    # Resources to be assigned to each virtual client
    # In this example we use CPU by default
    client_resources = {
        "num_cpus": args.num_cpus_per_client,
        "num_gpus": 0.0,
    }
    # Hyper-parameters for xgboost training
    num_local_round = NUM_LOCAL_ROUND
    params = BST_PARAMS
    # Setup learning rate
    if args.train_method == "bagging" and args.scaled_lr:
        new_lr = params["eta"] / args.pool_size
        params.update({"eta": new_lr})
    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(
            train_data_list,
            valid_data_list,
            args.train_method,
            params,
            num_local_round,
        ),
        num_clients=args.pool_size,
        client_resources=client_resources,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy,
        client_manager=CyclicClientManager() if args.train_method == "cyclic" else None,
    )
if __name__ == "__main__":
    main()
</file>

<file path="use_saved_model.py">
#!/usr/bin/env python
"""
use_saved_model.py
This script demonstrates how to load and use a saved XGBoost model from the federated learning process
to make predictions on new data.
Usage:
    python use_saved_model.py --model_path <path_to_model> --data_path <path_to_data> --output_path <path_for_predictions>
Example:
    python use_saved_model.py --model_path outputs/2023-05-01/12-34-56/final_model.json --data_path data/test_data.csv --output_path predictions.csv
"""
import argparse
import os
from logging import INFO
import pandas as pd
import numpy as np
import xgboost as xgb
from flwr.common.logger import log
from server_utils import load_saved_model, predict_with_saved_model
from dataset import transform_dataset_to_dmatrix, load_csv_data
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Use a saved XGBoost model to make predictions")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="Path to the saved model file (.json or .bin)",
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="Path to the data file (.csv)",
    )
    parser.add_argument(
        "--output_path",
        type=str,
        default="predictions.csv",
        help="Path to save the predictions (default: predictions.csv)",
    )
    parser.add_argument(
        "--has_labels",
        action="store_true",
        help="Specify if the data file contains labels (for evaluation)",
    )
    parser.add_argument(
        "--info_only",
        action="store_true",
        help="Only display model information without making predictions",
    )
    return parser.parse_args()
def display_model_info(model):
    """Display information about the loaded model."""
    log(INFO, "Model Information:")
    # Get number of trees
    num_trees = len(model.get_dump())
    log(INFO, "Number of trees: %d", num_trees)
    # Get feature names if available
    try:
        feature_names = model.feature_names
        if feature_names:
            log(INFO, "Feature names: %s", feature_names)
    except AttributeError:
        log(INFO, "Feature names not available in the model")
    # Get feature importance if available
    try:
        importance = model.get_score(importance_type='weight')
        log(INFO, "Feature importance (top 10):")
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
        for feature, score in sorted_importance:
            log(INFO, "  %s: %.4f", feature, score)
    except Exception as e:
        log(INFO, "Could not get feature importance: %s", str(e))
    # Get model parameters
    try:
        params = model.get_params()
        log(INFO, "Model parameters: %s", params)
    except Exception as e:
        log(INFO, "Could not get model parameters: %s", str(e))
def clean_data_for_xgboost(df):
    """
    Clean data for XGBoost by handling infinity values and extremely large numbers.
    Args:
        df (pd.DataFrame): Input DataFrame
    Returns:
        pd.DataFrame: Cleaned DataFrame
    """
    # Create a copy to avoid modifying the original
    cleaned_df = df.copy()
    # Replace infinity values with NaN
    cleaned_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # Cap extremely large values (adjust threshold as needed)
    numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns
    for col in numeric_cols:
        # Get the 99th percentile as a reference
        threshold = cleaned_df[col].quantile(0.99) * 10
        # If threshold is too small, use a default large value
        if threshold < 1e6:
            threshold = 1e6
        # Cap values and log the changes
        mask = cleaned_df[col] > threshold
        if mask.sum() > 0:
            log(INFO, "Capping %d extreme values in column '%s'", mask.sum(), col)
            cleaned_df.loc[mask, col] = np.nan
    return cleaned_df
def save_detailed_predictions(predictions, output_path, threshold=0.5):
    """
    Save detailed prediction information to CSV.
    Args:
        predictions (np.ndarray): Raw prediction scores from the model
        output_path (str): Path to save the predictions
        threshold (float): Threshold for binary classification (default: 0.5)
    """
    # Create a DataFrame to store predictions
    results_df = pd.DataFrame()
    # Check if predictions are probabilities (between 0 and 1)
    is_probability = np.all((predictions >= 0) & (predictions <= 1))
    if is_probability:
        # For binary classification with probability outputs
        results_df['raw_score'] = predictions
        results_df['predicted_label'] = (predictions >= threshold).astype(int)
        # Add prediction type
        results_df['prediction_type'] = np.where(
            results_df['predicted_label'] == 0, 'benign', 'malicious'
        )
        # Add prediction score (probability of the predicted class)
        results_df['prediction_score'] = np.where(
            results_df['predicted_label'] == 0, 
            1 - predictions,  # Probability of class 0 (benign)
            predictions       # Probability of class 1 (malicious)
        )
    else:
        # For regression or multi-class outputs
        results_df['predicted_value'] = predictions
        # For binary classification with non-probability outputs
        if np.all(np.isin(np.unique(predictions.astype(int)), [0, 1])):
            results_df['predicted_label'] = predictions.astype(int)
            results_df['prediction_type'] = np.where(
                results_df['predicted_label'] == 0, 'benign', 'malicious'
            )
            results_df['prediction_score'] = 1.0  # Default confidence
    # Save to CSV
    results_df.to_csv(output_path, index=False)
    log(INFO, "Saved %d predictions to %s", len(results_df), output_path)
    # Log prediction statistics
    if 'predicted_label' in results_df.columns:
        label_counts = results_df['predicted_label'].value_counts()
        log(INFO, "Prediction counts: %s", dict(label_counts))
        if 'prediction_score' in results_df.columns:
            log(INFO, "Score statistics: min=%.6f, max=%.6f, mean=%.6f", 
                results_df['prediction_score'].min(),
                results_df['prediction_score'].max(),
                results_df['prediction_score'].mean())
    return results_df
def main():
    """Main function to load model and make predictions."""
    args = parse_args()
    # Check if model file exists
    if not os.path.exists(args.model_path):
        log(INFO, "Error: Model file not found: %s", args.model_path)
        return
    try:
        # Load the model
        log(INFO, "Loading model from: %s", args.model_path)
        model = load_saved_model(args.model_path)
        # Display model information
        display_model_info(model)
        # If info_only flag is set, exit after displaying model info
        if args.info_only:
            log(INFO, "Info only mode - exiting without making predictions")
            return
        # Check if data path is provided
        if args.data_path is None:
            log(INFO, "No data path provided. Use --data_path to specify data for predictions.")
            return
        # Check if data file exists
        if not os.path.exists(args.data_path):
            log(INFO, "Error: Data file not found: %s", args.data_path)
            return
        log(INFO, "Loading data from: %s", args.data_path)
        # Load data
        if args.has_labels:
            try:
                # Use the dataset loading function if data has labels
                dataset = load_csv_data(args.data_path)["test"]
                dataset.set_format("pandas")
                data = dataset.to_pandas()
                # Convert to DMatrix
                dmatrix = transform_dataset_to_dmatrix(dataset)
                # Get true labels for evaluation
                y_true = dmatrix.get_label()
                # Make predictions
                raw_predictions = model.predict(dmatrix)
                # Save detailed predictions
                save_detailed_predictions(raw_predictions, args.output_path)
                # Evaluate if data has labels
                y_pred_labels = (raw_predictions >= 0.5).astype(int)
                # Calculate accuracy
                accuracy = np.mean(y_pred_labels == y_true)
                log(INFO, "Accuracy: %.4f", accuracy)
                # Print confusion matrix
                from sklearn.metrics import confusion_matrix, classification_report
                cm = confusion_matrix(y_true, y_pred_labels)
                log(INFO, "Confusion Matrix:\n%s", cm)
                # Print classification report
                report = classification_report(y_true, y_pred_labels)
                log(INFO, "Classification Report:\n%s", report)
            except Exception as e:
                log(INFO, "Error processing labeled data: %s", str(e))
                log(INFO, "Falling back to unlabeled data processing")
                # Fall back to unlabeled data processing
                data = pd.read_csv(args.data_path)
                log(INFO, "Data columns: %s", data.columns.tolist())
                # Clean data for XGBoost
                data = clean_data_for_xgboost(data)
                # Convert to DMatrix (without label)
                dmatrix = xgb.DMatrix(data, missing=np.nan)
                # Make predictions directly with the model
                raw_predictions = model.predict(dmatrix)
                # Save detailed predictions
                save_detailed_predictions(raw_predictions, args.output_path)
        else:
            # If data doesn't have labels, load as pandas DataFrame
            data = pd.read_csv(args.data_path)
            # Check if data has expected features
            log(INFO, "Data columns: %s", data.columns.tolist())
            # Handle Timestamp column if present
            if 'Timestamp' in data.columns:
                log(INFO, "Dropping Timestamp column as it's not needed for prediction")
                data = data.drop(columns=['Timestamp'])
            # Clean data for XGBoost
            data = clean_data_for_xgboost(data)
            # Handle categorical columns
            categorical_cols = []
            for col in data.columns:
                if data[col].dtype == 'object' or data[col].dtype.name == 'category':
                    if col not in ['Label', 'Timestamp']:  # Skip label and timestamp
                        categorical_cols.append(col)
                        data[col] = data[col].astype('category').cat.codes
            if categorical_cols:
                log(INFO, "Converted categorical columns to codes: %s", categorical_cols)
            # Convert to DMatrix with enable_categorical=True if there are categorical features
            if categorical_cols:
                dmatrix = xgb.DMatrix(data, enable_categorical=True, missing=np.nan)
            else:
                dmatrix = xgb.DMatrix(data, missing=np.nan)
            # Make predictions directly with the model
            raw_predictions = model.predict(dmatrix)
            # Save detailed predictions
            save_detailed_predictions(raw_predictions, args.output_path)
        log(INFO, "Predictions saved to: %s", args.output_path)
    except (FileNotFoundError, ValueError, TypeError, xgb.core.XGBoostError) as e:
        log(INFO, "Error: %s", str(e))
    except Exception as e:
        log(INFO, "Unexpected error: %s", str(e))
        raise
if __name__ == "__main__":
    main()
</file>

<file path="utils.py">
import argparse
# Hyper-parameters for xgboost training
NUM_LOCAL_ROUND = 1
BST_PARAMS = {
    "objective": "binary:logistic",
    "eta": 0.1,
    "max_depth": 6,
    "min_child_weight": 5,
    "gamma": 0.5,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "nthread": 16,
    "tree_method": "hist",
    "eval_metric": ["error", "auc", "logloss"],
    "max_delta_step": 5,
    "reg_alpha": 0.1,
    "reg_lambda": 1.0,
    "base_score": 0.5  # Add this to start from a neutral point
}
def client_args_parser():
    """Parse arguments to define experimental settings on client side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--num-partitions", default=10, type=int, help="Number of partitions."
    )
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--partition-id",
        default=0,
        type=int,
        help="Partition ID used for the current client.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
def server_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    parser.add_argument(
        "--pool-size", default=2, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=5, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=2,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=2,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    args = parser.parse_args()
    return args
def sim_args_parser():
    """Parse arguments to define experimental settings on server side."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train-method",
        default="bagging",
        type=str,
        choices=["bagging", "cyclic"],
        help="Training methods selected from bagging aggregation or cyclic training.",
    )
    # Server side
    parser.add_argument(
        "--pool-size", default=5, type=int, help="Number of total clients."
    )
    parser.add_argument(
        "--num-rounds", default=30, type=int, help="Number of FL rounds."
    )
    parser.add_argument(
        "--num-clients-per-round",
        default=5,
        type=int,
        help="Number of clients participate in training each round.",
    )
    parser.add_argument(
        "--num-evaluate-clients",
        default=5,
        type=int,
        help="Number of clients selected for evaluation.",
    )
    parser.add_argument(
        "--centralised-eval",
        action="store_true",
        help="Conduct centralised evaluation (True), or client evaluation on hold-out data (False).",
    )
    parser.add_argument(
        "--num-cpus-per-client",
        default=2,
        type=int,
        help="Number of CPUs used for per client.",
    )
    # Client side
    parser.add_argument(
        "--partitioner-type",
        default="uniform",
        type=str,
        choices=["uniform", "linear", "square", "exponential"],
        help="Partitioner types.",
    )
    parser.add_argument(
        "--seed", default=42, type=int, help="Seed used for train/test splitting."
    )
    parser.add_argument(
        "--test-fraction",
        default=0.2,
        type=float,
        help="Test fraction for train/test splitting.",
    )
    parser.add_argument(
        "--centralised-eval-client",
        action="store_true",
        help="Conduct evaluation on centralised test set (True), or on hold-out data (False).",
    )
    parser.add_argument(
        "--scaled-lr",
        action="store_true",
        help="Perform scaled learning rate based on the number of clients (True).",
    )
    parser.add_argument(
    "--csv-file",
    type=str,
    help="Path to the CSV file for dataset."
)
    args = parser.parse_args()
    return args
</file>

</files>
