name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
        with:
          lfs: true
          
      # Ensure LFS objects are properly pulled
      - name: Pull LFS objects
        run: |
          git lfs install
          git lfs pull
          
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: 'pip'
      - uses: iterative/setup-cml@v1
      
      # Cache conda/mamba environments
      - name: Cache conda environment
        uses: actions/cache@v3
        with:
          path: |
            ~/.conda
            ~/miniconda3
            ./venv
          key: ${{ runner.os }}-conda-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-conda-
      
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          
          # Check if venv exists before creating
          if [ ! -d "venv" ]; then
            echo "Setting up virtual environment for the first time"
            pip install virtualenv
            virtualenv venv
          fi
          
          source venv/bin/activate
          
          # Check if mamba is installed
          if ! command -v mamba &> /dev/null; then
            echo "Installing mamba"
            pip install mamba
            mamba init
            source ~/.bashrc
            mamba create
            mamba activate
          fi
          
          # Install dependencies
          pip install -r requirements.txt
          
          # Check if main packages are installed to avoid reinstalling them
          if ! python -c "import xgboost" &> /dev/null; then
            pip install xgboost
          fi
          
          if ! python -c "import flwr" &> /dev/null; then
            pip install -U flwr["simulation"]
          fi
          
          if ! python -c "import ray" &> /dev/null; then
            pip install -U "ray[all]"
          fi
          
          if ! python -c "import torch" &> /dev/null; then
            pip install torch torchvision torchaudio
          fi
          
          if ! python -c "import hydra" &> /dev/null; then
            pip install hydra-core
          fi
          
          if ! python -c "import imblearn" &> /dev/null; then
            pip install imbalanced-learn
          fi
      
      - name: Install hyperopt
        run: |
          source venv/bin/activate
          pip install hyperopt

      - name: Run Ray Tune hyperparameter optimization
        run: |
          source venv/bin/activate
          # Check if data directory exists, create if not
          mkdir -p data/sample
          
          # Use the same dataset that federated learning will use for consistency
          FINAL_DATASET="data/received/final_dataset.csv"
          
          # Check if the final dataset exists
          if [ -f "$FINAL_DATASET" ]; then
            echo "Using final dataset for hyperparameter tuning: $FINAL_DATASET"
            
            # Create output directory for Ray Tune results
            mkdir -p tune_results
            
            # Run Ray Tune with the final dataset for consistency
            bash run_ray_tune.sh --data-file "$FINAL_DATASET" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            
          else
            echo "Final dataset not found at $FINAL_DATASET, checking for fallback data..."
            
            # Define train and test file paths for UNSW_NB15 as fallback
            TRAIN_FILE="data/received/UNSW_NB15_training-set.csv"
            TEST_FILE="data/received/UNSW_NB15_testing-set.csv"
            
            # Check if the UNSW_NB15 files exist, otherwise use sample data
            if [ -f "$TRAIN_FILE" ] && [ -f "$TEST_FILE" ]; then
              echo "Using UNSW_NB15 data files for hyperparameter tuning"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            else
              echo "No suitable data found, creating synthetic data for testing"
              
              # Try to find any CSV file to use as a fallback
              SAMPLE_DATA=""
              for csv_file in $(find data -name "*.csv" | head -n 1); do
                SAMPLE_DATA="$csv_file"
                break
              done
              
              if [ -z "$SAMPLE_DATA" ]; then
                echo "No CSV data found, creating a sample dataset for tuning"
                # Generate synthetic data for train and test (kept as fallback)
                python -c "import pandas as pd; import numpy as np; np.random.seed(42); n = 1000; df = pd.DataFrame({'feature1': np.random.normal(0, 1, n), 'feature2': np.random.normal(0, 1, n), 'feature3': np.random.normal(0, 1, n), 'feature4': np.random.normal(0, 1, n), 'feature5': np.random.normal(0, 1, n), 'label': np.random.choice([0, 1, 2], n)}); train = df.sample(frac=0.8, random_state=42); test = df.drop(train.index); train.to_csv('data/sample/synthetic_train.csv', index=False); test.to_csv('data/sample/synthetic_test.csv', index=False)"
                TRAIN_FILE="data/sample/synthetic_train.csv"
                TEST_FILE="data/sample/synthetic_test.csv"
              else
                # If we have a single file, create train/test split (kept as fallback)
                python -c "import pandas as pd; from sklearn.model_selection import train_test_split; df = pd.read_csv('$SAMPLE_DATA'); train, test = train_test_split(df, test_size=0.2, random_state=42); train.to_csv('data/sample/split_train.csv', index=False); test.to_csv('data/sample/split_test.csv', index=False)"
                TRAIN_FILE="data/sample/split_train.csv"
                TEST_FILE="data/sample/split_test.csv"
              fi
              
              echo "Using train file: $TRAIN_FILE"
              echo "Using test file: $TEST_FILE"
              
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              
              # Use a small number of samples in CI for speed
              bash run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples 5 --cpus-per-trial 2 --output-dir "./tune_results"
            fi
          fi
          
          # Apply the tuned parameters to the federated learning system
          if [ -f "tune_results/best_params.json" ]; then
            python use_tuned_params.py --params-file "./tune_results/best_params.json"
            echo "Successfully applied tuned parameters to federated learning"
          else
            echo "Warning: No tuned parameters found, will use default parameters"
          fi
      
      - name: Run federated learning pipeline
        run: |
          source venv/bin/activate
          # Run the federated learning with tuned parameters
          ./run_bagging.sh
      
      - name: Evaluate model performance
        run: |
          source venv/bin/activate
          # Compare model performance with and without tuned parameters
          echo "## XGBoost Model Performance" > model_performance.md
          echo "" >> model_performance.md
          
          # Extract metrics from the results directory
          if [ -d "results" ]; then
            echo "### Federated Learning Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "| Metric | Value |" >> model_performance.md
            echo "| ------ | ----- |" >> model_performance.md
            
            # Extract and add key metrics if available
            for metric_file in $(find results -name "*.txt" | grep -i "metrics\|accuracy\|f1\|precision\|recall"); do
              metric_name=$(basename "$metric_file" .txt)
              metric_value=$(cat "$metric_file" | head -n 1)
              echo "| $metric_name | $metric_value |" >> model_performance.md
            done
          fi
          
          # Add Ray Tune optimization results
          if [ -f "tune_results/best_params.json" ]; then
            echo "" >> model_performance.md
            echo "### Hyperparameter Optimization Results" >> model_performance.md
            echo "" >> model_performance.md
            echo "Best hyperparameters:" >> model_performance.md
            echo '```json' >> model_performance.md
            cat tune_results/best_params.json >> model_performance.md
            echo '```' >> model_performance.md
          fi

      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"

          git checkout multi-class-predictions
          # Add Ray Tune results
          git add tune_results/
          
          # Add aggegrated results files
          git add results/
          
          # Add model performance report
          git add model_performance.md
          
          # Add new files in outputs directory
          git add outputs/
          
          # Explicitly add any plot files 
          git add "outputs/**/*.png"
          
          # Commit all changes
          git commit -m "workflow with Ray Tune optimization in actionðŸš€"

      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true

      
