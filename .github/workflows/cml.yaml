name: federated-learning-flower
on:
  push:
  workflow_dispatch:
permissions:
     contents: write
     actions: write
jobs:
  run:
    runs-on: ubuntu-latest
    # optionally use a convenient Ubuntu LTS + DVC + CML image
    #container: ghcr.io/iterative/cml:0-dvc2-base1
    steps:
      - uses: actions/checkout@v3
        with:
          lfs: true
          
      # Ensure LFS objects are properly pulled
      - name: Pull LFS objects
        run: |
          git lfs install
          git lfs pull
          
      # may need to setup NodeJS & Python3 on e.g. self-hosted
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          cache: 'pip'
      - uses: iterative/setup-cml@v1
      
      # Cache conda/mamba environments
      - name: Cache conda environment
        uses: actions/cache@v3
        with:
          path: |
            ~/.conda
            ~/miniconda3
            ./venv
          key: ${{ runner.os }}-conda-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-conda-
      
      - name: Set up Python environment
        run: |
          python -m pip install --upgrade pip
          
          # Check if venv exists before creating
          if [ ! -d "venv" ]; then
            echo "Setting up virtual environment for the first time"
            pip install virtualenv
            virtualenv venv
          fi
          
          source venv/bin/activate
          
          # Check if mamba is installed
          if ! command -v mamba &> /dev/null; then
            echo "Installing mamba"
            pip install mamba
            mamba init
            source ~/.bashrc
            mamba create
            mamba activate
          fi
          
          # Install dependencies
          pip install -r requirements.txt
          
          # Check if main packages are installed to avoid reinstalling them
          if ! python -c "import xgboost" &> /dev/null; then
            pip install xgboost
          fi
          
          if ! python -c "import flwr" &> /dev/null; then
            pip install -U flwr["simulation"]
          fi
          
          if ! python -c "import ray" &> /dev/null; then
            pip install -U "ray[all]"
          fi
          
          if ! python -c "import torch" &> /dev/null; then
            pip install torch torchvision torchaudio
          fi
          
          if ! python -c "import hydra" &> /dev/null; then
            pip install hydra-core
          fi
          
          if ! python -c "import imblearn" &> /dev/null; then
            pip install imbalanced-learn
          fi
      
      - name: Install hyperopt
        run: |
          source venv/bin/activate
          pip install hyperopt

      # XGBoost Training Section
      - name: Run XGBoost Training
        run: |
          source venv/bin/activate
          # Check if data directory exists, create if not
          mkdir -p data/sample
          
          # Use the same dataset that federated learning will use for consistency
          FINAL_DATASET="data/received/final_dataset.csv"
          
          # Detect CI environment and adjust sample count accordingly
          if [ "$CI" = "true" ]; then
            SAMPLES=2  # Reduced from 100 to 5 for faster tuning during development
            echo "CI detected: Using $SAMPLES samples for quick optimization"
          else
            SAMPLES=2  # Reduced from 100 to 5 for faster tuning during development
            echo "Local run: Using $SAMPLES samples for quick optimization"
          fi
          
          # Check if the final dataset exists
          if [ -f "$FINAL_DATASET" ]; then
            echo "Using final dataset for hyperparameter tuning: $FINAL_DATASET"
            
            # Create output directory for Ray Tune results
            mkdir -p tune_results
            
            # Run Ray Tune with the final dataset for consistency (updated path)
            bash scripts/run_ray_tune.sh --data-file "$FINAL_DATASET" --num-samples $SAMPLES --cpus-per-trial 2 --output-dir "./tune_results"
            
          else
            echo "Final dataset not found at $FINAL_DATASET, checking for fallback data..."
            
            # Define train and test file paths for UNSW_NB15 as fallback
            TRAIN_FILE="data/received/UNSW_NB15_training-set.csv"
            TEST_FILE="data/received/UNSW_NB15_testing-set.csv"
            
            # Check if the UNSW_NB15 files exist, otherwise use sample data
            if [ -f "$TRAIN_FILE" ] && [ -f "$TEST_FILE" ]; then
              echo "Using UNSW_NB15 data files for hyperparameter tuning"
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              bash scripts/run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples $SAMPLES --cpus-per-trial 2 --output-dir "./tune_results"
            else
              echo "No suitable data found, creating synthetic data for testing"
              
              # Try to find any CSV file to use as a fallback
              SAMPLE_DATA=""
              for csv_file in $(find data -name "*.csv" | head -n 1); do
                SAMPLE_DATA="$csv_file"
                break
              done
              
              if [ -z "$SAMPLE_DATA" ]; then
                echo "No CSV data found, creating a sample dataset for tuning"
                # Generate synthetic data for train and test (kept as fallback)
                python -c "import pandas as pd; import numpy as np; np.random.seed(42); n = 1000; df = pd.DataFrame({'feature1': np.random.normal(0, 1, n), 'feature2': np.random.normal(0, 1, n), 'feature3': np.random.normal(0, 1, n), 'feature4': np.random.normal(0, 1, n), 'feature5': np.random.normal(0, 1, n), 'label': np.random.choice([0, 1, 2], n)}); train = df.sample(frac=0.8, random_state=42); test = df.drop(train.index); train.to_csv('data/sample/synthetic_train.csv', index=False); test.to_csv('data/sample/synthetic_test.csv', index=False)"
                TRAIN_FILE="data/sample/synthetic_train.csv"
                TEST_FILE="data/sample/synthetic_test.csv"
              else
                # If we have a single file, create train/test split (kept as fallback)
                python -c "import pandas as pd; from sklearn.model_selection import train_test_split; df = pd.read_csv('$SAMPLE_DATA'); train, test = train_test_split(df, test_size=0.2, random_state=42); train.to_csv('data/sample/split_train.csv', index=False); test.to_csv('data/sample/split_test.csv', index=False)"
                TRAIN_FILE="data/sample/split_train.csv"
                TEST_FILE="data/sample/split_test.csv"
              fi
              
              echo "Using train file: $TRAIN_FILE"
              echo "Using test file: $TEST_FILE"
              
              # Create output directory for Ray Tune results
              mkdir -p tune_results
              
              # Use the determined number of samples (updated path)
              bash scripts/run_ray_tune.sh --train-file "$TRAIN_FILE" --test-file "$TEST_FILE" --num-samples $SAMPLES --cpus-per-trial 2 --output-dir "./tune_results"
            fi
          fi
          
          # Apply the tuned parameters to the federated learning system (updated path)
          if [ -f "tune_results/best_params.json" ]; then
            python src/models/use_tuned_params.py --params-file "./tune_results/best_params.json"
            echo "Successfully applied tuned parameters to federated learning"
          else
            echo "Warning: No tuned parameters found, will use default parameters"
          fi
      
      # Commenting out XGBoost federated learning pipeline for now
      # - name: Run XGBoost federated learning pipeline
      #   run: |
      #     source venv/bin/activate
      #     
      #     # Create output directories to ensure they exist
      #     mkdir -p outputs results tune_results
      #     
      #     # Run the federated learning with tuned parameters, but disable hyperparameter tuning
      #     # since we already optimized the parameters in the previous step
      #     echo "Using Ray Tune optimized parameters from previous step..."
      #     echo "Disabling hyperparameter tuning in federated learning pipeline to avoid duplication..."
      #     
      #     # Override the tuning.enabled setting to false since we already have optimized parameters
      #     python run.py +experiment=bagging tuning.enabled=false
      #     
      #     # List what was created for debugging
      #     echo "Contents of outputs directory:"
      #     ls -la outputs/ || echo "outputs directory is empty or doesn't exist"
      #     
      #     echo "Contents of results directory:"
      #     ls -la results/ || echo "results directory is empty or doesn't exist"
      #     
      #     echo "Contents of tune_results directory:"
      #     ls -la tune_results/ || echo "tune_results directory is empty or doesn't exist"

      # Neural Network Training Section
      - name: Run Neural Network Training
        run: |
          source venv/bin/activate
          
          # Create output directories for neural network
          mkdir -p outputs/neural_network results/neural_network
          
          # Install neural network specific dependencies
          pip install -r src/neural_network/requirements.txt
          
          # Run neural network federated learning
          echo "Starting neural network federated learning..."
          python -m src.neural_network.run_federated
          
          # List neural network outputs
          echo "Contents of neural network outputs directory:"
          ls -la outputs/neural_network/ || echo "neural network outputs directory is empty or doesn't exist"
          
          echo "Contents of neural network results directory:"
          ls -la results/neural_network/ || echo "neural network results directory is empty or doesn't exist"
      
      - name: Debug git status before commit
        run: |
          echo "=== Current git status ==="
          git status
          
          echo "=== Files in outputs directory ==="
          find outputs -type f 2>/dev/null || echo "No files in outputs directory"
          
          echo "=== Files in results directory ==="
          find results -type f 2>/dev/null || echo "No files in results directory"
          
          echo "=== Files in tune_results directory ==="
          find tune_results -type f 2>/dev/null || echo "No files in tune_results directory"
          
          echo "=== Git check-ignore on key directories ==="
          git check-ignore outputs/ || echo "outputs/ is NOT ignored"
          git check-ignore results/ || echo "results/ is NOT ignored" 
          git check-ignore tune_results/ || echo "tune_results/ is NOT ignored"
          
      - name: Commit results file
        run: |
          git config --local user.email "abde8473@stthomas.edu"
          git config --local user.name "moh-a-abde"
          
          # Function to check file size and add if under 100MB
          check_and_add_file() {
            local file="$1"
            if [ -f "$file" ]; then
              # Get file size in MB
              size_mb=$(du -m "$file" | cut -f1)
              if [ "$size_mb" -le 100 ]; then
                git add -f "$file"
                echo "Added $file (${size_mb}MB)"
              else
                echo "Skipping $file (${size_mb}MB) - exceeds 100MB limit"
              fi
            fi
          }
          
          # Function to process directory recursively
          process_directory() {
            local dir="$1"
            if [ -d "$dir" ]; then
              # Process files in current directory
              for file in "$dir"/*; do
                if [ -f "$file" ]; then
                  check_and_add_file "$file"
                elif [ -d "$file" ]; then
                  process_directory "$file"
                fi
              done
            fi
          }
          
          # Process each directory
          for dir in tune_results results outputs; do
            if [ -d "$dir" ]; then
              echo "Processing directory: $dir"
              process_directory "$dir"
            fi
          done
          
          # Add any other important files that might have been created
          for file in *.json tuned_params.py; do
            if [ -f "$file" ]; then
              check_and_add_file "$file"
            fi
          done
          
          # Check what files are staged for commit
          echo "Files staged for commit:"
          git status --porcelain
          
          # Commit all changes (only if there are changes)
          if [ -n "$(git status --porcelain)" ]; then
            git commit -m "workflow with XGBoost and Neural Network training ðŸš€"
            echo "Changes committed successfully"
          else
            echo "No changes to commit"
          fi

      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          force: true

      
