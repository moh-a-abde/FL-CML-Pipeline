
# ğŸŒ¸ Federated Learning with Flower  


A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
- ğŸŒ¼ **Flower** - Federated Learning Framework  
- ğŸ”¥ **PyTorch** - Deep Learning Library  
- ğŸ”§ **Hydra** - Configuration Management  
- ğŸ“Š **CML** - Continuous Machine Learning  

## **ğŸ“š Table of Contents**
- [âœ¨ Features](#-features)  
- [ğŸ“‚ Project Structure](#-project-structure)  
- [ğŸš€ Getting Started](#-getting-started)  
- [âš™ï¸ Configuration](#configuration)  
- [ğŸ§ª Running Experiments](#-running-experiments)  
- [ğŸ“‚ Output Structure](#-output-structure)

---

## âœ¨ Features  
âœ… **Privacy-Preserving Training** - Federated learning implementation with data isolation  
âœ… **Flexible Configuration** - Hydra-powered experiment management  
âœ… **Reproducible Experiments** - Automatic output organization  
âœ… **CI/CD Integration** - GitHub Actions workflow with CML reporting  
âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **ğŸ“‚ Project Structure**
```bash
â”œâ”€â”€ conf/                 # Hydra configurations
â”‚   â””â”€â”€ base.yaml         # Main experiment settings
â”œâ”€â”€ GitHub/
â”‚   â””â”€â”€ workflows/        # CI/CD pipelines
â”‚       â””â”€â”€ cml.yaml      # ML workflow definition
â”œâ”€â”€ outputs/              # Experiment outputs
â”œâ”€â”€ client.py             # Flower client logic
â”œâ”€â”€ dataset.py            # Data loading/preprocessing
â”œâ”€â”€ main.py               # Entry point with Hydra
â”œâ”€â”€ model.py              # Neural network architecture
â”œâ”€â”€ server.py             # Flower server utilities
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md             # You are here ğŸ“
```

---

## **ğŸš€ Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment**
   ```bash
   python -m venv fl-env
   source fl-env/bin/activate  # Linux/MacOS
   # On Windows:
   # fl-env\Scripts\activate
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **âš™ï¸ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

### **ğŸ§ª Running Experiments**  

### Basic Execution  
To start a federated learning simulation with default settings:  
```python
python main.py
```
### Run with Custom Configuration

Override default parameters at runtime:
```python
python main.py num_rounds=5 num_clients=500 config_fit.lr=0.1 config_fit.local_epochs=2
```

---

## **ğŸ“‚ Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
â””â”€â”€ YYYY-MM-DD/                  # Run date
    â””â”€â”€ HH-MM-SS/                # Run time
        â”œâ”€â”€ .hydra/              # Config snapshots
        â”‚   â”œâ”€â”€ config.yaml
        â”‚   â””â”€â”€ hydra.yaml
        â””â”€â”€ results.pkl          # Training history
```










