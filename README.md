# Federated Learning with Flower  

A privacy-preserving machine learning implementation using federated learning with the Flower framework. This project demonstrates collaborative model training across multiple clients without sharing raw data.  

### **Key Technologies**  
-  **Flower** - Federated Learning Framework  
-  **PyTorch** - Deep Learning Library  
-  **Hydra** - Configuration Management  
-  **CML** - Continuous Machine Learning

---

## ğŸ› ï¸ Workflow Overview

```diff
+============================================[ DATA PIPELINE ]============================================+
!                                                                                                         !
!  1. Live Network Capture â†’ 2. Clean Capture and Convert to Dataset â†’ 3. Train/Test â†’ 4.ï¸Output Results   !
!                                                                                                         !
+=========================================================================================================+
```

---

## ğŸ—ºï¸ Architecture Overview

This library implements a federated learning system that:
1. Processes network traffic data
2. Trains an XGBoost model in a distributed manner
3. Detects network intrusions across multiple clients while preserving data privacy

The system consists of several key components:

1. Data Processing Pipeline

- `data/livepreprocessing_socket.py`: Processes live network traffic data from Kafka
- `data/receiving_data.py`: Receives and saves processed data
- `dataset.py`: Handles data loading, preprocessing, and partitioning

2. Federated Learning Core

- `server.py`: Central FL server implementation
- `client.py`: FL client implementation
- `client_utils.py`: Client-side helper functions and XGBoost client class
- `server_utils.py`: Server-side helper functions and client management

3. Training Methods

Two main training approaches:
- Bagging: Aggregates models from multiple clients
- Cyclic: Passes model sequentially through clients

4. Execution Scripts

- `run_bagging.sh`: Launches bagging-based training
- `run_cyclic.sh`: Launches cyclic training
- `run.py`: Orchestrates the entire training pipeline
- `sim.py`: Simulation environment for testing

---

## ğŸ¯ What is to be achieved?

1. Data Processing
- Real-time data ingestion from Kafka
- Automated preprocessing of network traffic data
- Support for multiple feature types (categorical and numerical)
- Dynamic data partitioning across clients

2. Model Training
- Distributed XGBoost training
- Support for both bagging and cyclic training methods
- Configurable local training rounds
- Centralized and decentralized evaluation options

3. Scalability & Configuration
- Configurable number of clients and rounds
- Adjustable learning rates and model parameters
- Support for CPU/GPU training
- Flexible client selection strategies

4. Evaluation & Metrics
- Support for multiple evaluation metrics:
  - Precision
  - Recall
  - F1 Score
- Centralized and distributed evaluation options
  
---

## **ğŸ“š Table of Contents**
- [âœ¨ Features](#-features)  
- [ğŸ“‚ Project Structure](#-project-structure)  
- [ğŸš€ Getting Started](#-getting-started)  
- [âš™ï¸ Configuration](#-configuration)
- [ğŸ“‚ Output Structure](#-output-structure)
- [ğŸ§ª Running Experiments](#-running-experiments)  
- [âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging](#-comparison-of-federated-xgboost-strategies:-cyclic-vs.-bagging)

---

## âœ¨ Features  
âœ… **Privacy-Preserving Training** - Federated learning implementation with data isolation  
âœ… **Flexible Configuration** - Hydra-powered experiment management  
âœ… **Reproducible Experiments** - âš ï¸Automatic output organization   
âœ… **CI/CD Integration** - GitHub Actions workflow with CML reporting  
âœ… **Custom Dataset Support** - CSV data loader with preprocessing pipeline  

---

## **ğŸ“‚ Project Structure**
```bash
â”œâ”€â”€ github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cml.yaml      # CI/CD workflow definition
â”œâ”€â”€ pyecache/             # Python cache directory
â”œâ”€â”€ data/                 # Dataset files, data capture script, and data cleaning script
â”œâ”€â”€ plot/                 # Visualization outputs - ğŸš§ under construction (implementation phase) ğŸš§
â”œâ”€â”€ client.py             # Flower client logic
â”œâ”€â”€ client_utils.py       # Client helper functions
â”œâ”€â”€ dataset.py            # Data loading/preprocessing
â”œâ”€â”€ poetry.lock           # Poetry dependency lockfile - ğŸ” exploring (research phase) ğŸ”
â”œâ”€â”€ pyproject.toml        # Poetry project configuration - ğŸ” exploring (research phase) ğŸ”
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ run.py                # runs FULL FULL & CML experiment; includes capturing data traffic and preprocessing - ğŸš§ under construction (implementation phase) ğŸš§
â”œâ”€â”€ run_bagging.sh        # Bagging experiment script - runs script.py + client.py
â”œâ”€â”€ run_cyclic.sh         # Cyclic experiment script - runs script.py + client.py
â”œâ”€â”€ server.py             # Flower server logic
â”œâ”€â”€ server_utils.py       # Server helper functions
â”œâ”€â”€ sim.py                # Start simulation - âš ï¸ deprecated soon âš ï¸
â”œâ”€â”€ utils.py              # Shared utilities
â””â”€â”€ README.md             # Project documentation
```

---

## **ğŸš€ Getting Started**

### **Prerequisites**  
Before running the project, ensure you have the following installed:  
- Python 3.8+  
- pip (Python package manager)  

### **Installation**  

1. **Clone the repository**  
   ```bash
   git clone https://github.com/moh-a-abde/FL-CML-Pipeline.git
   cd FL-CML-Pipeline
   ```
2. **Create and activate a virtual environment (Docker is being used to run CML locally to automate the workflow)**
   **After setting up the docker environment run the following:**
   ```bash
   sudo systemctl start docker
   sudo systemctl enable docker
   act -j run --container-architecture linux/amd64 -v
   ```
3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   
---

## **âš ï¸âš™ï¸ Configuration**

The experiment settings are managed using **Hydra** and are defined in `conf/base.yaml`.  
Modify these settings in conf/base.yaml or override them at runtime when executing experiments.

Here are the key parameters:  

```yaml
# Core Experiment Parameters
num_rounds: 10                   # Total training rounds
num_clients: 100                 # Total available clients
batch_size: 20                   # Local batch size
num_classes: 2                   # Output classes

# Client Sampling
num_clients_per_round_fit: 10    # Clients per training round
num_clients_per_round_eval: 25   # Clients per evaluation round

# Training Configuration
config_fit:
  lr: 0.01                       # Learning rate
  momentum: 0.9                  # SGD momentum
  local_epochs: 1                # Epochs per client update
```

---

## **âš ğŸ“‚ Output Structure**

Experiment outputs are automatically saved in the `outputs/` directory, organized by date and time. Each experiment run generates a unique folder with the following structure:  

```plaintext
outputs/
â””â”€â”€ YYYY-MM-DD/                  # Run date
    â””â”€â”€ HH-MM-SS/                # Run time
        â”œâ”€â”€ .hydra/              # âš ï¸Config snapshots
        â”‚   â”œâ”€â”€ config.yaml
        â”‚   â””â”€â”€ hydra.yaml
        â”œâ”€â”€ results.pkl          # Training history
        â”œâ”€â”€ predictions/         # Model predictions
            â”œâ”€â”€ predictions_round_X.csv  # Per-round predictions


```

All these files are automatically tracked by the CML workflow and included in result reports.

---

### **ğŸ§ª Running Experiments**  

### Basic Execution  
To start federated learning with default settings:  
```bash
./run_bagging.sh
```
or

```bash
./run_bagging.sh
```

---

# âš–ï¸ Comparison of Federated XGBoost Strategies: Cyclic vs. Bagging

A comparison of two federated learning strategies for XGBoost implementations using the Flower framework.

## ğŸ”„ **FedXgbCyclic**
**Documentation**: [flwr.server.strategy.FedXgbCyclic](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbCyclic.html)

### Key Characteristics:
- **Client Selection**: Sequential cycling through clients in fixed order
- **Training Pattern**: One client per round, sequential execution
- **Data Requirements**: Effective for non-IID data distributions
- **Tree Growth**: Builds trees sequentially across clients
- **Aggregation**: Maintains global model that cycles through clients
- **Use Case**: Client-ordered scenarios where data sequence matters

## ğŸ’ **FedXgbBagging**
**Documentation**: [flwr.server.strategy.FedXgbBagging](https://flower.ai/docs/framework/ref-api/flwr.server.strategy.FedXgbBagging.html)

### Key Characteristics:
- **Client Selection**: Random subset selection each round
- **Training Pattern**: Parallel client training (multiple clients per round)
- **Data Requirements**: Works best with IID data distributions
- **Tree Growth**: Builds multiple candidate trees in parallel
- **Aggregation**: Uses bootstrap aggregating (bagging) for ensemble effects
- **Use Case**: Traditional federated scenarios with independent data

## ğŸ“Š Key Differences

| Feature                | Cyclic                                  | Bagging                                |
|------------------------|-----------------------------------------|----------------------------------------|
| **Client Selection**   | Fixed order, sequential                 | Random subset, parallel                |
| **Round Execution**    | 1 client/round                          | Multiple clients/round                 |
| **Data Assumption**    | Tolerates non-IID                       | Prefers IID                            |
| **Tree Building**      | Sequential tree growth                  | Parallel tree candidates               |
| **Aggregation**        | Direct model cycling                    | Bootstrap aggregating                  |
| **Communication**      | Low bandwidth (1 client/round)          | Higher bandwidth                       |
| **Use Case**           | Ordered client sequences                | Traditional FL scenarios               |
| **Performance**        | Better for client-specific patterns     | Better for generalizable models        |

## When to Use Which

### Choose **Cyclic** When:
- Clients have ordered/sequential data relationships
- Data distribution is non-IID across clients
- You want explicit client participation order
- Bandwidth is constrained

### Choose **Bagging** When:
- Data is IID or approximately independent
- You want traditional federated averaging behavior
- Parallel client participation is preferred
- Ensemble effects are desirable

---

## Implementation Tips
1. **Cyclic** requires careful client ordering configuration
2. **Bagging** benefits from larger client subsets per round
3. Both support XGBoost's histogram-based training
4. Monitor client compute resources differently:
   - Cyclic: Manage sequential load
   - Bagging: Handle parallel compute demands
---

## Credits
This project uses code adapted from the [Flower XGBoost Comprehensive Example](https://github.com/adap/flower/tree/main/examples/xgboost-comprehensive) as the initial code skeleton.

---

<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ -->
<div align="center">

## ğŸ”¥ **R&D Led By** ğŸ”¥
### [ **`Mohamed Abdel-Hamid`** ]

![Static Badge](https://img.shields.io/badge/Phase-%F0%9F%94%A5_Innovation_Station-%23FF6B6B?style=for-the-badge)
<br>

```diff
+==================================================+
!  ğŸ§‘ğŸ’» Coded with 100% chaos-driven curiosity    !
!  â˜• Powered by midnight espresso & big dreams   !
+==================================================+
```
<sub>
ğŸ” Cyber Alchemy Brewing For ğŸ›ï¸ Indiana University of Pennsylvania's ARMZTA Project

ğŸ”— https://www.iup.edu/cybersecurity/grants/ncae-c-armzta/index.html</sub>

<sub>Grant: NCAE-C Program</sub>

</div> 
<!-- à¼¼ ã¤ â—•_â—• à¼½ã¤ R&D ZONE à¼¼ ã¤ â—•_â—• à¼½ã¤ --> 









